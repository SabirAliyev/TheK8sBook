apiVersion: v1
items:
- metadata:
    annotations:
      csi.volume.kubernetes.io/nodeid: '{"linodebs.csi.linode.com":"43589322"}'
      kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock
      lke.linode.com/wgip: 172.31.0.1
      lke.linode.com/wgpub: +wwdgLu/4Xwb740EiU0uXSao70A9+Rt+NAUMZdejM1A=
      node.alpha.kubernetes.io/ttl: "0"
      projectcalico.org/IPv4Address: 192.168.129.114/17
      projectcalico.org/IPv4IPIPTunnelAddr: 10.2.0.1
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2023-03-09T06:09:40Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/instance-type: g6-standard-1
      beta.kubernetes.io/os: linux
      failure-domain.beta.kubernetes.io/region: eu-west
      kubernetes.io/arch: amd64
      kubernetes.io/hostname: lke96996-146211-640977a3930b
      kubernetes.io/os: linux
      lke.linode.com/pool-id: "146211"
      node.kubernetes.io/instance-type: g6-standard-1
      topology.kubernetes.io/region: eu-west
      topology.linode.com/region: eu-west
    name: lke96996-146211-640977a3930b
    resourceVersion: "1306"
    uid: 3acd7392-09b7-4c8a-aa2b-738940017be5
  spec:
    podCIDR: 10.2.0.0/24
    podCIDRs:
    - 10.2.0.0/24
    providerID: linode://43589322
  status:
    addresses:
    - address: lke96996-146211-640977a3930b
      type: Hostname
    - address: 139.144.144.82
      type: ExternalIP
    - address: 192.168.129.114
      type: InternalIP
    allocatable:
      cpu: "1"
      ephemeral-storage: "47484676837"
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 1930768Ki
      pods: "110"
    capacity:
      cpu: "1"
      ephemeral-storage: 51524172Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 2033168Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2023-03-09T06:10:08Z"
      lastTransitionTime: "2023-03-09T06:10:08Z"
      message: Calico is running on this node
      reason: CalicoIsUp
      status: "False"
      type: NetworkUnavailable
    - lastHeartbeatTime: "2023-03-09T06:22:42Z"
      lastTransitionTime: "2023-03-09T06:09:39Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2023-03-09T06:22:42Z"
      lastTransitionTime: "2023-03-09T06:09:39Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2023-03-09T06:22:42Z"
      lastTransitionTime: "2023-03-09T06:09:39Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2023-03-09T06:22:42Z"
      lastTransitionTime: "2023-03-09T06:10:03Z"
      message: kubelet is posting ready status. AppArmor enabled
      reason: KubeletReady
      status: "True"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - docker.io/calico/cni@sha256:e282ea2914c806b5de2976330a17cfb5e6dcef47147bceb1432ca5c75fd46f50
      - docker.io/calico/cni:v3.24.5
      sizeBytes: 87458443
    - names:
      - docker.io/calico/node@sha256:5972ad2bcbdc668564d3e26960c9c513b2d7b05581c704747cf7c62ef3a405a6
      - docker.io/calico/node:v3.24.5
      sizeBytes: 81584866
    - names:
      - docker.io/bitnami/kubectl@sha256:c4a8d9c0cd9c5f903830ea64816c83adf307ff1d775bc3e5b77f1d49d3960205
      - docker.io/bitnami/kubectl:1.16.3-debian-10-r36
      sizeBytes: 65850129
    - names:
      - docker.io/calico/kube-controllers@sha256:2b6acd7f677f76ffe12ecf3ea7df92eb9b1bdb07336d1ac2a54c7631fb753f7e
      - docker.io/calico/kube-controllers:v3.24.5
      sizeBytes: 31137169
    - names:
      - docker.io/linode/csi-provisioner@sha256:bbae7cde811054f6a51060ba7a42d8bf2469b8c574abb50fec8b46c13e32541e
      - docker.io/linode/csi-provisioner:v3.0.0
      sizeBytes: 22727635
    - names:
      - docker.io/linode/csi-resizer@sha256:d2d2e429a0a87190ee73462698a02a08e555055246ad87ad979b464b999fedae
      - docker.io/linode/csi-resizer:v1.3.0
      sizeBytes: 21669981
    - names:
      - docker.io/linode/csi-attacher@sha256:221c1c6930fb1cb93b57762a74ccb59194c4c74a63c0fd49309d1158d4f8c72c
      - docker.io/linode/csi-attacher:v3.3.0
      sizeBytes: 21442902
    - names:
      - docker.io/linode/kube-proxy-amd64@sha256:569f1d31ded5da821cb2dc2c4c741881ab0c75967f48e8dab5d0286d22f2793e
      - docker.io/linode/kube-proxy-amd64:v1.25.6
      sizeBytes: 20274607
    - names:
      - docker.io/linode/coredns@sha256:bdb36ee882c13135669cfc2bb91c808a33926ad1a411fee07bd2dc344bb8f782
      - docker.io/linode/coredns:1.9.3
      sizeBytes: 14835873
    - names:
      - docker.io/linode/linode-blockstorage-csi-driver@sha256:6a1ecbfc761a5e3adabedb091d84fdb76d73d89e95af1592c0c97baad2042340
      - docker.io/linode/linode-blockstorage-csi-driver:v0.5.0
      sizeBytes: 11589636
    - names:
      - docker.io/linode/csi-node-driver-registrar@sha256:9622c6a6dac7499a055a382930f4de82905a3c5735c0753f7094115c9c871309
      - docker.io/linode/csi-node-driver-registrar:v1.3.0
      sizeBytes: 7717137
    - names:
      - docker.io/calico/pod2daemon-flexvol@sha256:392e392d0e11388bf55571155f915b1e8d080cb6824a7a09381537ad2f9b3c83
      - docker.io/calico/pod2daemon-flexvol:v3.24.5
      sizeBytes: 7067316
    - names:
      - k8s.gcr.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07
      - k8s.gcr.io/pause:3.5
      sizeBytes: 301416
    nodeInfo:
      architecture: amd64
      bootID: 2fefef42-97cc-4f32-87e6-ef4bee79e9a4
      containerRuntimeVersion: containerd://1.6.10
      kernelVersion: 5.10.0-19-cloud-amd64
      kubeProxyVersion: v1.25.4
      kubeletVersion: v1.25.4
      machineID: 28385c66b215419b9a576f4e66f2ce0d
      operatingSystem: linux
      osImage: Debian GNU/Linux 11 (bullseye)
      systemUUID: 28385c66b215419b9a576f4e66f2ce0d
- metadata:
    annotations:
      csi.volume.kubernetes.io/nodeid: '{"linodebs.csi.linode.com":"43589324"}'
      kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock
      lke.linode.com/wgip: 172.31.1.1
      lke.linode.com/wgpub: 1M6nzW2ggHeuz+Jm1OVt05e/0rP4uon/ZHHuw8C84HU=
      node.alpha.kubernetes.io/ttl: "0"
      projectcalico.org/IPv4Address: 192.168.173.115/17
      projectcalico.org/IPv4IPIPTunnelAddr: 10.2.1.1
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2023-03-09T06:10:09Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/instance-type: g6-standard-1
      beta.kubernetes.io/os: linux
      failure-domain.beta.kubernetes.io/region: eu-west
      kubernetes.io/arch: amd64
      kubernetes.io/hostname: lke96996-146211-640977a3e82e
      kubernetes.io/os: linux
      lke.linode.com/pool-id: "146211"
      node.kubernetes.io/instance-type: g6-standard-1
      topology.kubernetes.io/region: eu-west
      topology.linode.com/region: eu-west
    name: lke96996-146211-640977a3e82e
    resourceVersion: "1303"
    uid: c97e244d-76b3-4227-985e-e625f0e8fdcf
  spec:
    podCIDR: 10.2.1.0/24
    podCIDRs:
    - 10.2.1.0/24
    providerID: linode://43589324
  status:
    addresses:
    - address: lke96996-146211-640977a3e82e
      type: Hostname
    - address: 139.144.144.93
      type: ExternalIP
    - address: 192.168.173.115
      type: InternalIP
    allocatable:
      cpu: "1"
      ephemeral-storage: "47484676837"
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 1930768Ki
      pods: "110"
    capacity:
      cpu: "1"
      ephemeral-storage: 51524172Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 2033168Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2023-03-09T06:10:37Z"
      lastTransitionTime: "2023-03-09T06:10:37Z"
      message: Calico is running on this node
      reason: CalicoIsUp
      status: "False"
      type: NetworkUnavailable
    - lastHeartbeatTime: "2023-03-09T06:22:40Z"
      lastTransitionTime: "2023-03-09T06:10:09Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2023-03-09T06:22:40Z"
      lastTransitionTime: "2023-03-09T06:10:09Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2023-03-09T06:22:40Z"
      lastTransitionTime: "2023-03-09T06:10:09Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2023-03-09T06:22:40Z"
      lastTransitionTime: "2023-03-09T06:10:32Z"
      message: kubelet is posting ready status. AppArmor enabled
      reason: KubeletReady
      status: "True"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - docker.io/calico/cni@sha256:e282ea2914c806b5de2976330a17cfb5e6dcef47147bceb1432ca5c75fd46f50
      - docker.io/calico/cni:v3.24.5
      sizeBytes: 87458443
    - names:
      - docker.io/calico/node@sha256:5972ad2bcbdc668564d3e26960c9c513b2d7b05581c704747cf7c62ef3a405a6
      - docker.io/calico/node:v3.24.5
      sizeBytes: 81584866
    - names:
      - docker.io/bitnami/kubectl@sha256:c4a8d9c0cd9c5f903830ea64816c83adf307ff1d775bc3e5b77f1d49d3960205
      - docker.io/bitnami/kubectl:1.16.3-debian-10-r36
      sizeBytes: 65850129
    - names:
      - docker.io/linode/kube-proxy-amd64@sha256:569f1d31ded5da821cb2dc2c4c741881ab0c75967f48e8dab5d0286d22f2793e
      - docker.io/linode/kube-proxy-amd64:v1.25.6
      sizeBytes: 20274607
    - names:
      - docker.io/linode/linode-blockstorage-csi-driver@sha256:6a1ecbfc761a5e3adabedb091d84fdb76d73d89e95af1592c0c97baad2042340
      - docker.io/linode/linode-blockstorage-csi-driver:v0.5.0
      sizeBytes: 11589636
    - names:
      - docker.io/linode/csi-node-driver-registrar@sha256:9622c6a6dac7499a055a382930f4de82905a3c5735c0753f7094115c9c871309
      - docker.io/linode/csi-node-driver-registrar:v1.3.0
      sizeBytes: 7717137
    - names:
      - docker.io/calico/pod2daemon-flexvol@sha256:392e392d0e11388bf55571155f915b1e8d080cb6824a7a09381537ad2f9b3c83
      - docker.io/calico/pod2daemon-flexvol:v3.24.5
      sizeBytes: 7067316
    - names:
      - k8s.gcr.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07
      - k8s.gcr.io/pause:3.5
      sizeBytes: 301416
    nodeInfo:
      architecture: amd64
      bootID: acf16d9a-414b-4fbd-8c4d-2b5ceb68e684
      containerRuntimeVersion: containerd://1.6.10
      kernelVersion: 5.10.0-19-cloud-amd64
      kubeProxyVersion: v1.25.4
      kubeletVersion: v1.25.4
      machineID: 8ea1b07f0c6340a2a79a4a2ff1386c46
      operatingSystem: linux
      osImage: Debian GNU/Linux 11 (bullseye)
      systemUUID: 8ea1b07f0c6340a2a79a4a2ff1386c46
kind: NodeList
metadata:
  resourceVersion: "1307"
---
apiVersion: v1
items:
- action: Scheduling
  eventTime: "2023-03-09T06:09:25.201863Z"
  firstTimestamp: null
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: calico-kube-controllers-5dbfbbf9dc-wptvl
    namespace: kube-system
    resourceVersion: "486"
    uid: b68a79ad-5b8d-476a-a340-f1521ac6e199
  lastTimestamp: null
  message: no nodes available to schedule pods
  metadata:
    creationTimestamp: "2023-03-09T06:09:25Z"
    name: calico-kube-controllers-5dbfbbf9dc-wptvl.174aab4e986b1a2f
    namespace: kube-system
    resourceVersion: "511"
    uid: 9f427a41-9eac-4b26-ba58-a28d3037b1a8
  reason: FailedScheduling
  reportingComponent: default-scheduler
  reportingInstance: default-scheduler-kube-scheduler-5cf64855c7-q54kd
  source: {}
  type: Warning
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:32Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: calico-kube-controllers-5dbfbbf9dc-wptvl
    namespace: kube-system
    resourceVersion: "512"
    uid: b68a79ad-5b8d-476a-a340-f1521ac6e199
  lastTimestamp: "2023-03-09T06:09:32Z"
  message: 'pod didn''t trigger scale-up: '
  metadata:
    creationTimestamp: "2023-03-09T06:09:32Z"
    name: calico-kube-controllers-5dbfbbf9dc-wptvl.174aab504dd8bf8c
    namespace: kube-system
    resourceVersion: "523"
    uid: 1fd7671d-297a-4671-a23d-593a88feb882
  reason: NotTriggerScaleUp
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: cluster-autoscaler
  type: Normal
- action: Binding
  eventTime: "2023-03-09T06:09:40.603299Z"
  firstTimestamp: null
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: calico-kube-controllers-5dbfbbf9dc-wptvl
    namespace: kube-system
    resourceVersion: "512"
    uid: b68a79ad-5b8d-476a-a340-f1521ac6e199
  lastTimestamp: null
  message: Successfully assigned kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl
    to lke96996-146211-640977a3930b
  metadata:
    creationTimestamp: "2023-03-09T06:09:40Z"
    name: calico-kube-controllers-5dbfbbf9dc-wptvl.174aab522e6a3ab3
    namespace: kube-system
    resourceVersion: "539"
    uid: 19cd696f-bcc1-409d-9121-c447c411ea6d
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: default-scheduler-kube-scheduler-5cf64855c7-q54kd
  source: {}
  type: Normal
- count: 8
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:43Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: calico-kube-controllers-5dbfbbf9dc-wptvl
    namespace: kube-system
    resourceVersion: "532"
    uid: b68a79ad-5b8d-476a-a340-f1521ac6e199
  lastTimestamp: "2023-03-09T06:09:57Z"
  message: 'network is not ready: container runtime network not ready: NetworkReady=false
    reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin
    not initialized'
  metadata:
    creationTimestamp: "2023-03-09T06:09:44Z"
    name: calico-kube-controllers-5dbfbbf9dc-wptvl.174aab52e93e86a1
    namespace: kube-system
    resourceVersion: "648"
    uid: b9ba3f7b-7db4-4b2a-bf7e-97463049e7f5
  reason: NetworkNotReady
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Warning
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:00Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: calico-kube-controllers-5dbfbbf9dc-wptvl
    namespace: kube-system
    resourceVersion: "532"
    uid: b68a79ad-5b8d-476a-a340-f1521ac6e199
  lastTimestamp: "2023-03-09T06:10:00Z"
  message: 'Failed to create pod sandbox: rpc error: code = Unknown desc = failed
    to setup network for sandbox "fbe402cc330da895d8747a2cd3f7c7367f0861e4f1adfd970856e049e75df9a0":
    plugin type="calico" failed (add): stat /var/lib/calico/nodename: no such file
    or directory: check that the calico/node container is running and has mounted
    /var/lib/calico/'
  metadata:
    creationTimestamp: "2023-03-09T06:10:00Z"
    name: calico-kube-controllers-5dbfbbf9dc-wptvl.174aab56c4651962
    namespace: kube-system
    resourceVersion: "656"
    uid: 7a942c8e-8b85-4a98-9a1a-755e2a109bf8
  reason: FailedCreatePodSandBox
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Warning
- count: 2
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:01Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: calico-kube-controllers-5dbfbbf9dc-wptvl
    namespace: kube-system
    resourceVersion: "532"
    uid: b68a79ad-5b8d-476a-a340-f1521ac6e199
  lastTimestamp: "2023-03-09T06:10:12Z"
  message: Pod sandbox changed, it will be killed and re-created.
  metadata:
    creationTimestamp: "2023-03-09T06:10:01Z"
    name: calico-kube-controllers-5dbfbbf9dc-wptvl.174aab56eecac4d7
    namespace: kube-system
    resourceVersion: "725"
    uid: 95df6bbf-c505-43fb-8ce4-12fe88468af5
  reason: SandboxChanged
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:13Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{calico-kube-controllers}
    kind: Pod
    name: calico-kube-controllers-5dbfbbf9dc-wptvl
    namespace: kube-system
    resourceVersion: "532"
    uid: b68a79ad-5b8d-476a-a340-f1521ac6e199
  lastTimestamp: "2023-03-09T06:10:13Z"
  message: Pulling image "docker.io/calico/kube-controllers:v3.24.5"
  metadata:
    creationTimestamp: "2023-03-09T06:10:13Z"
    name: calico-kube-controllers-5dbfbbf9dc-wptvl.174aab59c0bc70f5
    namespace: kube-system
    resourceVersion: "728"
    uid: e275b74b-e1b0-42be-9d48-3715ba6da62c
  reason: Pulling
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:16Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{calico-kube-controllers}
    kind: Pod
    name: calico-kube-controllers-5dbfbbf9dc-wptvl
    namespace: kube-system
    resourceVersion: "532"
    uid: b68a79ad-5b8d-476a-a340-f1521ac6e199
  lastTimestamp: "2023-03-09T06:10:16Z"
  message: Successfully pulled image "docker.io/calico/kube-controllers:v3.24.5" in
    3.472245282s
  metadata:
    creationTimestamp: "2023-03-09T06:10:16Z"
    name: calico-kube-controllers-5dbfbbf9dc-wptvl.174aab5a8fb4173b
    namespace: kube-system
    resourceVersion: "746"
    uid: 8f60596d-8d11-460b-8d72-13ee4dbb2e04
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:16Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{calico-kube-controllers}
    kind: Pod
    name: calico-kube-controllers-5dbfbbf9dc-wptvl
    namespace: kube-system
    resourceVersion: "532"
    uid: b68a79ad-5b8d-476a-a340-f1521ac6e199
  lastTimestamp: "2023-03-09T06:10:16Z"
  message: Created container calico-kube-controllers
  metadata:
    creationTimestamp: "2023-03-09T06:10:16Z"
    name: calico-kube-controllers-5dbfbbf9dc-wptvl.174aab5a918273f9
    namespace: kube-system
    resourceVersion: "747"
    uid: 8a88c08c-6633-4df6-a82c-33b2b6162a9e
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:16Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{calico-kube-controllers}
    kind: Pod
    name: calico-kube-controllers-5dbfbbf9dc-wptvl
    namespace: kube-system
    resourceVersion: "532"
    uid: b68a79ad-5b8d-476a-a340-f1521ac6e199
  lastTimestamp: "2023-03-09T06:10:16Z"
  message: Started container calico-kube-controllers
  metadata:
    creationTimestamp: "2023-03-09T06:10:16Z"
    name: calico-kube-controllers-5dbfbbf9dc-wptvl.174aab5a99e4f5a9
    namespace: kube-system
    resourceVersion: "750"
    uid: d5d4bd64-5b5f-4e97-80c9-45d65ba72d81
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 4
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:17Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{calico-kube-controllers}
    kind: Pod
    name: calico-kube-controllers-5dbfbbf9dc-wptvl
    namespace: kube-system
    resourceVersion: "532"
    uid: b68a79ad-5b8d-476a-a340-f1521ac6e199
  lastTimestamp: "2023-03-09T06:10:19Z"
  message: |
    Readiness probe failed: initialized to false
  metadata:
    creationTimestamp: "2023-03-09T06:10:17Z"
    name: calico-kube-controllers-5dbfbbf9dc-wptvl.174aab5ab5c03413
    namespace: kube-system
    resourceVersion: "775"
    uid: 4c99ff94-7441-45f8-aab1-28193997ea05
  reason: Unhealthy
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Warning
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:11:19Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{calico-kube-controllers}
    kind: Pod
    name: calico-kube-controllers-5dbfbbf9dc-wptvl
    namespace: kube-system
    resourceVersion: "532"
    uid: b68a79ad-5b8d-476a-a340-f1521ac6e199
  lastTimestamp: "2023-03-09T06:11:19Z"
  message: |
    Liveness probe failed: Error verifying datastore: client rate limiter Wait returned an error: context deadline exceeded - error from a previous attempt: EOF
  metadata:
    creationTimestamp: "2023-03-09T06:11:37Z"
    name: calico-kube-controllers-5dbfbbf9dc-wptvl.174aab6949bb5b1b
    namespace: kube-system
    resourceVersion: "905"
    uid: 8c11e574-9857-42af-9aa3-5e38675a5eca
  reason: Unhealthy
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Warning
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:11:19Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{calico-kube-controllers}
    kind: Pod
    name: calico-kube-controllers-5dbfbbf9dc-wptvl
    namespace: kube-system
    resourceVersion: "532"
    uid: b68a79ad-5b8d-476a-a340-f1521ac6e199
  lastTimestamp: "2023-03-09T06:11:19Z"
  message: |
    Readiness probe failed: Error verifying datastore: client rate limiter Wait returned an error: context deadline exceeded - error from a previous attempt: EOF
  metadata:
    creationTimestamp: "2023-03-09T06:11:37Z"
    name: calico-kube-controllers-5dbfbbf9dc-wptvl.174aab694a2c89ab
    namespace: kube-system
    resourceVersion: "906"
    uid: 85f0391a-b78f-472a-ae68-ff7fe311391e
  reason: Unhealthy
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Warning
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: ReplicaSet
    name: calico-kube-controllers-5dbfbbf9dc
    namespace: kube-system
    resourceVersion: "457"
    uid: f35434b4-edd5-4b9a-9afe-3fe9a9d76b9d
  lastTimestamp: "2023-03-09T06:09:11Z"
  message: 'Created pod: calico-kube-controllers-5dbfbbf9dc-wptvl'
  metadata:
    creationTimestamp: "2023-03-09T06:09:11Z"
    name: calico-kube-controllers-5dbfbbf9dc.174aab4b56687f07
    namespace: kube-system
    resourceVersion: "489"
    uid: 1a1054f7-be22-4b3b-a988-9995911e807a
  reason: SuccessfulCreate
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: replicaset-controller
  type: Normal
- count: 2
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:09Z"
  involvedObject:
    apiVersion: policy/v1
    kind: PodDisruptionBudget
    name: calico-kube-controllers
    namespace: kube-system
    resourceVersion: "263"
    uid: 10ba2af2-f9a1-400d-b51d-fd851b68e0e3
  lastTimestamp: "2023-03-09T06:09:09Z"
  message: No matching pods found
  metadata:
    creationTimestamp: "2023-03-09T06:09:09Z"
    name: calico-kube-controllers.174aab4af08b0ddf
    namespace: kube-system
    resourceVersion: "470"
    uid: 4722772f-20db-4ff7-a791-0a417c691229
  reason: NoPods
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: controllermanager
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: calico-kube-controllers
    namespace: kube-system
    resourceVersion: "261"
    uid: d26504cd-f87d-43b4-be7d-7121ecc6bd6e
  lastTimestamp: "2023-03-09T06:09:09Z"
  message: Scaled up replica set calico-kube-controllers-5dbfbbf9dc to 1
  metadata:
    creationTimestamp: "2023-03-09T06:09:10Z"
    name: calico-kube-controllers.174aab4b02c6cada
    namespace: kube-system
    resourceVersion: "466"
    uid: ff77e021-27f2-4ca4-9a10-1da1bc1f5d1f
  reason: ScalingReplicaSet
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: deployment-controller
  type: Normal
- action: Binding
  eventTime: "2023-03-09T06:09:40.807912Z"
  firstTimestamp: null
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: calico-node-28hph
    namespace: kube-system
    resourceVersion: "536"
    uid: e4901bc7-42be-425b-9e93-0b022dfbb3e8
  lastTimestamp: null
  message: Successfully assigned kube-system/calico-node-28hph to lke96996-146211-640977a3930b
  metadata:
    creationTimestamp: "2023-03-09T06:09:40Z"
    name: calico-node-28hph.174aab523a9c6241
    namespace: kube-system
    resourceVersion: "548"
    uid: f8be4c14-4792-42af-a8fc-b1d6c80bc516
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: default-scheduler-kube-scheduler-5cf64855c7-q54kd
  source: {}
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:46Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{upgrade-ipam}
    kind: Pod
    name: calico-node-28hph
    namespace: kube-system
    resourceVersion: "545"
    uid: e4901bc7-42be-425b-9e93-0b022dfbb3e8
  lastTimestamp: "2023-03-09T06:09:46Z"
  message: Pulling image "docker.io/calico/cni:v3.24.5"
  metadata:
    creationTimestamp: "2023-03-09T06:09:46Z"
    name: calico-node-28hph.174aab537776636f
    namespace: kube-system
    resourceVersion: "592"
    uid: 060ab364-1ef2-4945-8f55-3870dbe81c1f
  reason: Pulling
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:51Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{upgrade-ipam}
    kind: Pod
    name: calico-node-28hph
    namespace: kube-system
    resourceVersion: "545"
    uid: e4901bc7-42be-425b-9e93-0b022dfbb3e8
  lastTimestamp: "2023-03-09T06:09:51Z"
  message: Successfully pulled image "docker.io/calico/cni:v3.24.5" in 5.089234955s
  metadata:
    creationTimestamp: "2023-03-09T06:09:51Z"
    name: calico-node-28hph.174aab54a6ce8215
    namespace: kube-system
    resourceVersion: "612"
    uid: 49a97c06-e2fa-4d38-a3d8-c82198b1b488
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:51Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{upgrade-ipam}
    kind: Pod
    name: calico-node-28hph
    namespace: kube-system
    resourceVersion: "545"
    uid: e4901bc7-42be-425b-9e93-0b022dfbb3e8
  lastTimestamp: "2023-03-09T06:09:51Z"
  message: Created container upgrade-ipam
  metadata:
    creationTimestamp: "2023-03-09T06:09:51Z"
    name: calico-node-28hph.174aab54a8424835
    namespace: kube-system
    resourceVersion: "613"
    uid: 11f3c91b-f5fa-4365-bcef-97124702848d
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:51Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{upgrade-ipam}
    kind: Pod
    name: calico-node-28hph
    namespace: kube-system
    resourceVersion: "545"
    uid: e4901bc7-42be-425b-9e93-0b022dfbb3e8
  lastTimestamp: "2023-03-09T06:09:51Z"
  message: Started container upgrade-ipam
  metadata:
    creationTimestamp: "2023-03-09T06:09:51Z"
    name: calico-node-28hph.174aab54ad379bab
    namespace: kube-system
    resourceVersion: "614"
    uid: dbfd21f9-e139-4e8c-aa83-73581f6c7912
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:51Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{install-cni}
    kind: Pod
    name: calico-node-28hph
    namespace: kube-system
    resourceVersion: "545"
    uid: e4901bc7-42be-425b-9e93-0b022dfbb3e8
  lastTimestamp: "2023-03-09T06:09:51Z"
  message: Container image "docker.io/calico/cni:v3.24.5" already present on machine
  metadata:
    creationTimestamp: "2023-03-09T06:09:51Z"
    name: calico-node-28hph.174aab54cefd0e1f
    namespace: kube-system
    resourceVersion: "621"
    uid: b30eaded-cd74-47b7-ba2f-35729047e5a9
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:51Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{install-cni}
    kind: Pod
    name: calico-node-28hph
    namespace: kube-system
    resourceVersion: "545"
    uid: e4901bc7-42be-425b-9e93-0b022dfbb3e8
  lastTimestamp: "2023-03-09T06:09:51Z"
  message: Created container install-cni
  metadata:
    creationTimestamp: "2023-03-09T06:09:51Z"
    name: calico-node-28hph.174aab54d06ed10b
    namespace: kube-system
    resourceVersion: "623"
    uid: 67a3740c-bc3e-4f65-9b4d-1d812aa99596
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:51Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{install-cni}
    kind: Pod
    name: calico-node-28hph
    namespace: kube-system
    resourceVersion: "545"
    uid: e4901bc7-42be-425b-9e93-0b022dfbb3e8
  lastTimestamp: "2023-03-09T06:09:51Z"
  message: Started container install-cni
  metadata:
    creationTimestamp: "2023-03-09T06:09:51Z"
    name: calico-node-28hph.174aab54d3acae24
    namespace: kube-system
    resourceVersion: "624"
    uid: 48703974-6f5b-4e0b-986d-4bd5f40f5c08
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:55Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{flexvol-driver}
    kind: Pod
    name: calico-node-28hph
    namespace: kube-system
    resourceVersion: "545"
    uid: e4901bc7-42be-425b-9e93-0b022dfbb3e8
  lastTimestamp: "2023-03-09T06:09:55Z"
  message: Pulling image "docker.io/calico/pod2daemon-flexvol:v3.24.5"
  metadata:
    creationTimestamp: "2023-03-09T06:09:55Z"
    name: calico-node-28hph.174aab55c2bb57fa
    namespace: kube-system
    resourceVersion: "637"
    uid: 8456c8f5-060e-499c-ae6c-7ce8ee0dce7e
  reason: Pulling
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:02Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{flexvol-driver}
    kind: Pod
    name: calico-node-28hph
    namespace: kube-system
    resourceVersion: "545"
    uid: e4901bc7-42be-425b-9e93-0b022dfbb3e8
  lastTimestamp: "2023-03-09T06:10:02Z"
  message: Successfully pulled image "docker.io/calico/pod2daemon-flexvol:v3.24.5"
    in 6.664376382s
  metadata:
    creationTimestamp: "2023-03-09T06:10:02Z"
    name: calico-node-28hph.174aab574ff6365e
    namespace: kube-system
    resourceVersion: "664"
    uid: cbafda8d-180e-44b3-81ba-7d814e1cf54d
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:02Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{flexvol-driver}
    kind: Pod
    name: calico-node-28hph
    namespace: kube-system
    resourceVersion: "545"
    uid: e4901bc7-42be-425b-9e93-0b022dfbb3e8
  lastTimestamp: "2023-03-09T06:10:02Z"
  message: Created container flexvol-driver
  metadata:
    creationTimestamp: "2023-03-09T06:10:02Z"
    name: calico-node-28hph.174aab5753cc9080
    namespace: kube-system
    resourceVersion: "665"
    uid: c34846aa-2af8-4e41-99de-9ee32fa172fd
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:02Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{flexvol-driver}
    kind: Pod
    name: calico-node-28hph
    namespace: kube-system
    resourceVersion: "545"
    uid: e4901bc7-42be-425b-9e93-0b022dfbb3e8
  lastTimestamp: "2023-03-09T06:10:02Z"
  message: Started container flexvol-driver
  metadata:
    creationTimestamp: "2023-03-09T06:10:02Z"
    name: calico-node-28hph.174aab57598f0f13
    namespace: kube-system
    resourceVersion: "666"
    uid: 36ea916b-f2ed-446d-9309-e9b767d06e36
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:03Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{calico-node}
    kind: Pod
    name: calico-node-28hph
    namespace: kube-system
    resourceVersion: "545"
    uid: e4901bc7-42be-425b-9e93-0b022dfbb3e8
  lastTimestamp: "2023-03-09T06:10:03Z"
  message: Pulling image "docker.io/calico/node:v3.24.5"
  metadata:
    creationTimestamp: "2023-03-09T06:10:03Z"
    name: calico-node-28hph.174aab5766631c84
    namespace: kube-system
    resourceVersion: "667"
    uid: 20a684db-e723-44bb-bd89-eaf02fdd7b0c
  reason: Pulling
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:08Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{calico-node}
    kind: Pod
    name: calico-node-28hph
    namespace: kube-system
    resourceVersion: "545"
    uid: e4901bc7-42be-425b-9e93-0b022dfbb3e8
  lastTimestamp: "2023-03-09T06:10:08Z"
  message: Successfully pulled image "docker.io/calico/node:v3.24.5" in 5.30196102s
  metadata:
    creationTimestamp: "2023-03-09T06:10:08Z"
    name: calico-node-28hph.174aab58a2692e03
    namespace: kube-system
    resourceVersion: "674"
    uid: 916b7f57-ccd6-45fb-90db-94f1ad4e49c9
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:08Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{calico-node}
    kind: Pod
    name: calico-node-28hph
    namespace: kube-system
    resourceVersion: "545"
    uid: e4901bc7-42be-425b-9e93-0b022dfbb3e8
  lastTimestamp: "2023-03-09T06:10:08Z"
  message: Created container calico-node
  metadata:
    creationTimestamp: "2023-03-09T06:10:08Z"
    name: calico-node-28hph.174aab58a427a262
    namespace: kube-system
    resourceVersion: "675"
    uid: cdc9077b-746c-425f-836f-1aae3b2f0a04
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:08Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{calico-node}
    kind: Pod
    name: calico-node-28hph
    namespace: kube-system
    resourceVersion: "545"
    uid: e4901bc7-42be-425b-9e93-0b022dfbb3e8
  lastTimestamp: "2023-03-09T06:10:08Z"
  message: Started container calico-node
  metadata:
    creationTimestamp: "2023-03-09T06:10:08Z"
    name: calico-node-28hph.174aab58a9d3fe12
    namespace: kube-system
    resourceVersion: "676"
    uid: 735339a1-9282-4d6b-8e21-0ad62348f8af
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:09Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{calico-node}
    kind: Pod
    name: calico-node-28hph
    namespace: kube-system
    resourceVersion: "545"
    uid: e4901bc7-42be-425b-9e93-0b022dfbb3e8
  lastTimestamp: "2023-03-09T06:10:09Z"
  message: |
    Readiness probe failed: calico/node is not ready: BIRD is not ready: Error querying BIRD: unable to connect to BIRDv4 socket: dial unix /var/run/bird/bird.ctl: connect: no such file or directory
  metadata:
    creationTimestamp: "2023-03-09T06:10:09Z"
    name: calico-node-28hph.174aab58d3456629
    namespace: kube-system
    resourceVersion: "683"
    uid: 45e307d0-e2f1-4eec-b19e-ad3307ee0ed4
  reason: Unhealthy
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Warning
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:10Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{calico-node}
    kind: Pod
    name: calico-node-28hph
    namespace: kube-system
    resourceVersion: "545"
    uid: e4901bc7-42be-425b-9e93-0b022dfbb3e8
  lastTimestamp: "2023-03-09T06:10:10Z"
  message: |
    Readiness probe failed: calico/node is not ready: BIRD is not ready: Error querying BIRD: unable to connect to BIRDv4 socket: dial unix /var/run/calico/bird.ctl: connect: connection refused
  metadata:
    creationTimestamp: "2023-03-09T06:10:10Z"
    name: calico-node-28hph.174aab5916f990da
    namespace: kube-system
    resourceVersion: "706"
    uid: e920383d-4158-40a8-a011-fac9faa7a27a
  reason: Unhealthy
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Warning
- action: Binding
  eventTime: "2023-03-09T06:10:09.823044Z"
  firstTimestamp: null
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: calico-node-hzjbq
    namespace: kube-system
    resourceVersion: "690"
    uid: 60639a77-396a-46f8-9f4a-e8fb338028c1
  lastTimestamp: null
  message: Successfully assigned kube-system/calico-node-hzjbq to lke96996-146211-640977a3e82e
  metadata:
    creationTimestamp: "2023-03-09T06:10:09Z"
    name: calico-node-hzjbq.174aab58fc0c32c8
    namespace: kube-system
    resourceVersion: "702"
    uid: c7b453ce-9511-446b-b56e-2216ac721080
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: default-scheduler-kube-scheduler-5cf64855c7-q54kd
  source: {}
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:14Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{upgrade-ipam}
    kind: Pod
    name: calico-node-hzjbq
    namespace: kube-system
    resourceVersion: "692"
    uid: 60639a77-396a-46f8-9f4a-e8fb338028c1
  lastTimestamp: "2023-03-09T06:10:14Z"
  message: Pulling image "docker.io/calico/cni:v3.24.5"
  metadata:
    creationTimestamp: "2023-03-09T06:10:15Z"
    name: calico-node-hzjbq.174aab5a1a41dadb
    namespace: kube-system
    resourceVersion: "738"
    uid: dd8292b4-8013-4bb3-9421-a1433042ccee
  reason: Pulling
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:19Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{upgrade-ipam}
    kind: Pod
    name: calico-node-hzjbq
    namespace: kube-system
    resourceVersion: "692"
    uid: 60639a77-396a-46f8-9f4a-e8fb338028c1
  lastTimestamp: "2023-03-09T06:10:19Z"
  message: Successfully pulled image "docker.io/calico/cni:v3.24.5" in 4.891316447s
  metadata:
    creationTimestamp: "2023-03-09T06:10:19Z"
    name: calico-node-hzjbq.174aab5b3dcdaea1
    namespace: kube-system
    resourceVersion: "768"
    uid: 3d1a6f84-0725-4573-8f40-5b3e1ac4431b
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:19Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{upgrade-ipam}
    kind: Pod
    name: calico-node-hzjbq
    namespace: kube-system
    resourceVersion: "692"
    uid: 60639a77-396a-46f8-9f4a-e8fb338028c1
  lastTimestamp: "2023-03-09T06:10:19Z"
  message: Created container upgrade-ipam
  metadata:
    creationTimestamp: "2023-03-09T06:10:19Z"
    name: calico-node-hzjbq.174aab5b3ffabd38
    namespace: kube-system
    resourceVersion: "771"
    uid: 99a74245-c353-459a-99b8-f99c417bd957
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:19Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{upgrade-ipam}
    kind: Pod
    name: calico-node-hzjbq
    namespace: kube-system
    resourceVersion: "692"
    uid: 60639a77-396a-46f8-9f4a-e8fb338028c1
  lastTimestamp: "2023-03-09T06:10:19Z"
  message: Started container upgrade-ipam
  metadata:
    creationTimestamp: "2023-03-09T06:10:19Z"
    name: calico-node-hzjbq.174aab5b440a664e
    namespace: kube-system
    resourceVersion: "773"
    uid: 7fe985c5-b54c-4015-a2e4-6ac6e5403454
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:20Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{install-cni}
    kind: Pod
    name: calico-node-hzjbq
    namespace: kube-system
    resourceVersion: "692"
    uid: 60639a77-396a-46f8-9f4a-e8fb338028c1
  lastTimestamp: "2023-03-09T06:10:20Z"
  message: Container image "docker.io/calico/cni:v3.24.5" already present on machine
  metadata:
    creationTimestamp: "2023-03-09T06:10:20Z"
    name: calico-node-hzjbq.174aab5b8490778b
    namespace: kube-system
    resourceVersion: "789"
    uid: 274b0d34-ed09-4d60-b369-7590e434ebdd
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:20Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{install-cni}
    kind: Pod
    name: calico-node-hzjbq
    namespace: kube-system
    resourceVersion: "692"
    uid: 60639a77-396a-46f8-9f4a-e8fb338028c1
  lastTimestamp: "2023-03-09T06:10:20Z"
  message: Created container install-cni
  metadata:
    creationTimestamp: "2023-03-09T06:10:20Z"
    name: calico-node-hzjbq.174aab5b85d21b30
    namespace: kube-system
    resourceVersion: "790"
    uid: b9a1ccfc-dcf7-4db9-aca8-f613ebdae89f
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:20Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{install-cni}
    kind: Pod
    name: calico-node-hzjbq
    namespace: kube-system
    resourceVersion: "692"
    uid: 60639a77-396a-46f8-9f4a-e8fb338028c1
  lastTimestamp: "2023-03-09T06:10:20Z"
  message: Started container install-cni
  metadata:
    creationTimestamp: "2023-03-09T06:10:20Z"
    name: calico-node-hzjbq.174aab5b893689c2
    namespace: kube-system
    resourceVersion: "792"
    uid: 4d8cb049-689f-4026-9982-87186f710995
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:22Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{flexvol-driver}
    kind: Pod
    name: calico-node-hzjbq
    namespace: kube-system
    resourceVersion: "692"
    uid: 60639a77-396a-46f8-9f4a-e8fb338028c1
  lastTimestamp: "2023-03-09T06:10:22Z"
  message: Pulling image "docker.io/calico/pod2daemon-flexvol:v3.24.5"
  metadata:
    creationTimestamp: "2023-03-09T06:10:22Z"
    name: calico-node-hzjbq.174aab5bfcf86adf
    namespace: kube-system
    resourceVersion: "807"
    uid: 911ac00f-aec0-4370-aad7-26177fafad6b
  reason: Pulling
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:30Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{flexvol-driver}
    kind: Pod
    name: calico-node-hzjbq
    namespace: kube-system
    resourceVersion: "692"
    uid: 60639a77-396a-46f8-9f4a-e8fb338028c1
  lastTimestamp: "2023-03-09T06:10:30Z"
  message: Successfully pulled image "docker.io/calico/pod2daemon-flexvol:v3.24.5"
    in 7.382742684s
  metadata:
    creationTimestamp: "2023-03-09T06:10:30Z"
    name: calico-node-hzjbq.174aab5db5045656
    namespace: kube-system
    resourceVersion: "832"
    uid: f73fc284-6e41-46f7-ba99-63fcc3c3b83c
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:30Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{flexvol-driver}
    kind: Pod
    name: calico-node-hzjbq
    namespace: kube-system
    resourceVersion: "692"
    uid: 60639a77-396a-46f8-9f4a-e8fb338028c1
  lastTimestamp: "2023-03-09T06:10:30Z"
  message: Created container flexvol-driver
  metadata:
    creationTimestamp: "2023-03-09T06:10:30Z"
    name: calico-node-hzjbq.174aab5db746c547
    namespace: kube-system
    resourceVersion: "833"
    uid: e79febc2-c545-4acc-8fa8-fc4e98c48f17
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:30Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{flexvol-driver}
    kind: Pod
    name: calico-node-hzjbq
    namespace: kube-system
    resourceVersion: "692"
    uid: 60639a77-396a-46f8-9f4a-e8fb338028c1
  lastTimestamp: "2023-03-09T06:10:30Z"
  message: Started container flexvol-driver
  metadata:
    creationTimestamp: "2023-03-09T06:10:30Z"
    name: calico-node-hzjbq.174aab5dc2f1c218
    namespace: kube-system
    resourceVersion: "836"
    uid: bf88dce4-b88d-4ca1-bf59-2d5a66c5b022
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:30Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{calico-node}
    kind: Pod
    name: calico-node-hzjbq
    namespace: kube-system
    resourceVersion: "692"
    uid: 60639a77-396a-46f8-9f4a-e8fb338028c1
  lastTimestamp: "2023-03-09T06:10:30Z"
  message: Pulling image "docker.io/calico/node:v3.24.5"
  metadata:
    creationTimestamp: "2023-03-09T06:10:30Z"
    name: calico-node-hzjbq.174aab5de399dad7
    namespace: kube-system
    resourceVersion: "837"
    uid: 9db7463e-180f-4423-a9f1-fb1f750534e7
  reason: Pulling
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:36Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{calico-node}
    kind: Pod
    name: calico-node-hzjbq
    namespace: kube-system
    resourceVersion: "692"
    uid: 60639a77-396a-46f8-9f4a-e8fb338028c1
  lastTimestamp: "2023-03-09T06:10:36Z"
  message: Successfully pulled image "docker.io/calico/node:v3.24.5" in 6.057741514s
  metadata:
    creationTimestamp: "2023-03-09T06:10:36Z"
    name: calico-node-hzjbq.174aab5f4cabcf01
    namespace: kube-system
    resourceVersion: "861"
    uid: fec07590-97df-4d44-982b-493c6259dd58
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:36Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{calico-node}
    kind: Pod
    name: calico-node-hzjbq
    namespace: kube-system
    resourceVersion: "692"
    uid: 60639a77-396a-46f8-9f4a-e8fb338028c1
  lastTimestamp: "2023-03-09T06:10:36Z"
  message: Created container calico-node
  metadata:
    creationTimestamp: "2023-03-09T06:10:36Z"
    name: calico-node-hzjbq.174aab5f4e699216
    namespace: kube-system
    resourceVersion: "862"
    uid: 6b0e6c3a-05d1-42d6-8694-eb6af115630e
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:37Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{calico-node}
    kind: Pod
    name: calico-node-hzjbq
    namespace: kube-system
    resourceVersion: "692"
    uid: 60639a77-396a-46f8-9f4a-e8fb338028c1
  lastTimestamp: "2023-03-09T06:10:37Z"
  message: Started container calico-node
  metadata:
    creationTimestamp: "2023-03-09T06:10:37Z"
    name: calico-node-hzjbq.174aab5f52cd4007
    namespace: kube-system
    resourceVersion: "863"
    uid: 5427912f-612c-4c54-81c5-969d0b7a771b
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:38Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{calico-node}
    kind: Pod
    name: calico-node-hzjbq
    namespace: kube-system
    resourceVersion: "692"
    uid: 60639a77-396a-46f8-9f4a-e8fb338028c1
  lastTimestamp: "2023-03-09T06:10:38Z"
  message: |
    Readiness probe failed: calico/node is not ready: BIRD is not ready: Error querying BIRD: unable to connect to BIRDv4 socket: dial unix /var/run/bird/bird.ctl: connect: no such file or directory
  metadata:
    creationTimestamp: "2023-03-09T06:10:38Z"
    name: calico-node-hzjbq.174aab5f8e0c1036
    namespace: kube-system
    resourceVersion: "868"
    uid: a95b58ed-eb55-4d66-9812-1541b1f57e3e
  reason: Unhealthy
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Warning
- count: 2
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:39Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{calico-node}
    kind: Pod
    name: calico-node-hzjbq
    namespace: kube-system
    resourceVersion: "692"
    uid: 60639a77-396a-46f8-9f4a-e8fb338028c1
  lastTimestamp: "2023-03-09T06:10:40Z"
  message: |
    Readiness probe failed: calico/node is not ready: BIRD is not ready: Error querying BIRD: unable to connect to BIRDv4 socket: dial unix /var/run/calico/bird.ctl: connect: connection refused
  metadata:
    creationTimestamp: "2023-03-09T06:10:39Z"
    name: calico-node-hzjbq.174aab5fd7572c2e
    namespace: kube-system
    resourceVersion: "875"
    uid: f6871c37-dc0f-44fd-8be0-09091e2a461a
  reason: Unhealthy
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Warning
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:40Z"
  involvedObject:
    apiVersion: apps/v1
    kind: DaemonSet
    name: calico-node
    namespace: kube-system
    resourceVersion: "477"
    uid: fc991107-9c7b-4a92-9345-fe92f48e066d
  lastTimestamp: "2023-03-09T06:09:40Z"
  message: 'Created pod: calico-node-28hph'
  metadata:
    creationTimestamp: "2023-03-09T06:09:40Z"
    name: calico-node.174aab522eb5a510
    namespace: kube-system
    resourceVersion: "546"
    uid: 109ad97d-87ee-46a7-b54c-7c29478ba16d
  reason: SuccessfulCreate
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: daemonset-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: DaemonSet
    name: calico-node
    namespace: kube-system
    resourceVersion: "553"
    uid: fc991107-9c7b-4a92-9345-fe92f48e066d
  lastTimestamp: "2023-03-09T06:10:09Z"
  message: 'Created pod: calico-node-hzjbq'
  metadata:
    creationTimestamp: "2023-03-09T06:10:09Z"
    name: calico-node.174aab58fe1182e9
    namespace: kube-system
    resourceVersion: "698"
    uid: 0fa6c653-7098-407d-b256-9bdbc86f3847
  reason: SuccessfulCreate
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: daemonset-controller
  type: Normal
- action: Scheduling
  eventTime: "2023-03-09T06:09:25.304327Z"
  firstTimestamp: null
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: coredns-5c64b647bf-gkmxq
    namespace: kube-system
    resourceVersion: "487"
    uid: 2fbf1e9f-e5fc-42bd-83a0-f0523cc4f760
  lastTimestamp: null
  message: no nodes available to schedule pods
  metadata:
    creationTimestamp: "2023-03-09T06:09:25Z"
    name: coredns-5c64b647bf-gkmxq.174aab4e9e866374
    namespace: kube-system
    resourceVersion: "513"
    uid: bff9c972-6b1c-4b8a-b59d-8183fd2131a9
  reason: FailedScheduling
  reportingComponent: default-scheduler
  reportingInstance: default-scheduler-kube-scheduler-5cf64855c7-q54kd
  source: {}
  type: Warning
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:32Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: coredns-5c64b647bf-gkmxq
    namespace: kube-system
    resourceVersion: "514"
    uid: 2fbf1e9f-e5fc-42bd-83a0-f0523cc4f760
  lastTimestamp: "2023-03-09T06:09:32Z"
  message: 'pod didn''t trigger scale-up: '
  metadata:
    creationTimestamp: "2023-03-09T06:09:32Z"
    name: coredns-5c64b647bf-gkmxq.174aab504dd8f39e
    namespace: kube-system
    resourceVersion: "525"
    uid: 15495f8a-a6d0-4dbe-b527-9d9cf3fc5c06
  reason: NotTriggerScaleUp
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: cluster-autoscaler
  type: Normal
- action: Binding
  eventTime: "2023-03-09T06:09:40.605092Z"
  firstTimestamp: null
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: coredns-5c64b647bf-gkmxq
    namespace: kube-system
    resourceVersion: "514"
    uid: 2fbf1e9f-e5fc-42bd-83a0-f0523cc4f760
  lastTimestamp: null
  message: Successfully assigned kube-system/coredns-5c64b647bf-gkmxq to lke96996-146211-640977a3930b
  metadata:
    creationTimestamp: "2023-03-09T06:09:40Z"
    name: coredns-5c64b647bf-gkmxq.174aab522e85878e
    namespace: kube-system
    resourceVersion: "540"
    uid: bf352a99-6d49-46f7-9fe5-ee3635f378d5
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: default-scheduler-kube-scheduler-5cf64855c7-q54kd
  source: {}
  type: Normal
- count: 8
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:43Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: coredns-5c64b647bf-gkmxq
    namespace: kube-system
    resourceVersion: "537"
    uid: 2fbf1e9f-e5fc-42bd-83a0-f0523cc4f760
  lastTimestamp: "2023-03-09T06:09:57Z"
  message: 'network is not ready: container runtime network not ready: NetworkReady=false
    reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin
    not initialized'
  metadata:
    creationTimestamp: "2023-03-09T06:09:44Z"
    name: coredns-5c64b647bf-gkmxq.174aab52e94878af
    namespace: kube-system
    resourceVersion: "646"
    uid: 3f2c3688-885f-43c7-adb0-ebfe6dd9633f
  reason: NetworkNotReady
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Warning
- count: 6
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:43Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: coredns-5c64b647bf-gkmxq
    namespace: kube-system
    resourceVersion: "537"
    uid: 2fbf1e9f-e5fc-42bd-83a0-f0523cc4f760
  lastTimestamp: "2023-03-09T06:09:59Z"
  message: 'MountVolume.SetUp failed for volume "config-volume" : object "kube-system"/"coredns"
    not registered'
  metadata:
    creationTimestamp: "2023-03-09T06:09:44Z"
    name: coredns-5c64b647bf-gkmxq.174aab52f21b1e79
    namespace: kube-system
    resourceVersion: "651"
    uid: 03001ec0-5f6a-4d64-a857-c025b6fcedaa
  reason: FailedMount
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Warning
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:16Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{coredns}
    kind: Pod
    name: coredns-5c64b647bf-gkmxq
    namespace: kube-system
    resourceVersion: "537"
    uid: 2fbf1e9f-e5fc-42bd-83a0-f0523cc4f760
  lastTimestamp: "2023-03-09T06:10:16Z"
  message: Pulling image "linode/coredns:1.9.3"
  metadata:
    creationTimestamp: "2023-03-09T06:10:16Z"
    name: coredns-5c64b647bf-gkmxq.174aab5a97e06086
    namespace: kube-system
    resourceVersion: "749"
    uid: 4027bdeb-759b-4d90-aab8-ad2a5129537c
  reason: Pulling
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:19Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{coredns}
    kind: Pod
    name: coredns-5c64b647bf-gkmxq
    namespace: kube-system
    resourceVersion: "537"
    uid: 2fbf1e9f-e5fc-42bd-83a0-f0523cc4f760
  lastTimestamp: "2023-03-09T06:10:19Z"
  message: Successfully pulled image "linode/coredns:1.9.3" in 2.804705s
  metadata:
    creationTimestamp: "2023-03-09T06:10:19Z"
    name: coredns-5c64b647bf-gkmxq.174aab5b3f0d22b8
    namespace: kube-system
    resourceVersion: "769"
    uid: 9bccf004-db53-4c1c-aab2-9d949192e28f
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:19Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{coredns}
    kind: Pod
    name: coredns-5c64b647bf-gkmxq
    namespace: kube-system
    resourceVersion: "537"
    uid: 2fbf1e9f-e5fc-42bd-83a0-f0523cc4f760
  lastTimestamp: "2023-03-09T06:10:19Z"
  message: Created container coredns
  metadata:
    creationTimestamp: "2023-03-09T06:10:19Z"
    name: coredns-5c64b647bf-gkmxq.174aab5b3ffd5e80
    namespace: kube-system
    resourceVersion: "770"
    uid: c0c79874-d726-4aba-a1e0-37e0f9150d82
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:19Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{coredns}
    kind: Pod
    name: coredns-5c64b647bf-gkmxq
    namespace: kube-system
    resourceVersion: "537"
    uid: 2fbf1e9f-e5fc-42bd-83a0-f0523cc4f760
  lastTimestamp: "2023-03-09T06:10:19Z"
  message: Started container coredns
  metadata:
    creationTimestamp: "2023-03-09T06:10:19Z"
    name: coredns-5c64b647bf-gkmxq.174aab5b42f2e0ec
    namespace: kube-system
    resourceVersion: "772"
    uid: 685f5e02-ae80-40a9-8a81-d5ae7bfc9f67
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- action: Scheduling
  eventTime: "2023-03-09T06:09:25.404448Z"
  firstTimestamp: null
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: coredns-5c64b647bf-kxh68
    namespace: kube-system
    resourceVersion: "488"
    uid: 339e0bc9-99d7-4ffb-a6ea-eafd28060908
  lastTimestamp: null
  message: no nodes available to schedule pods
  metadata:
    creationTimestamp: "2023-03-09T06:09:25Z"
    name: coredns-5c64b647bf-kxh68.174aab4ea47e2a74
    namespace: kube-system
    resourceVersion: "516"
    uid: 6d743e35-86b8-4bec-b6bf-236fc75aef8d
  reason: FailedScheduling
  reportingComponent: default-scheduler
  reportingInstance: default-scheduler-kube-scheduler-5cf64855c7-q54kd
  source: {}
  type: Warning
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:32Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: coredns-5c64b647bf-kxh68
    namespace: kube-system
    resourceVersion: "515"
    uid: 339e0bc9-99d7-4ffb-a6ea-eafd28060908
  lastTimestamp: "2023-03-09T06:09:32Z"
  message: 'pod didn''t trigger scale-up: '
  metadata:
    creationTimestamp: "2023-03-09T06:09:32Z"
    name: coredns-5c64b647bf-kxh68.174aab504dd8fcb8
    namespace: kube-system
    resourceVersion: "526"
    uid: f6e98f49-0176-4a92-9bda-906278a10d81
  reason: NotTriggerScaleUp
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: cluster-autoscaler
  type: Normal
- action: Binding
  eventTime: "2023-03-09T06:09:40.603880Z"
  firstTimestamp: null
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: coredns-5c64b647bf-kxh68
    namespace: kube-system
    resourceVersion: "515"
    uid: 339e0bc9-99d7-4ffb-a6ea-eafd28060908
  lastTimestamp: null
  message: Successfully assigned kube-system/coredns-5c64b647bf-kxh68 to lke96996-146211-640977a3930b
  metadata:
    creationTimestamp: "2023-03-09T06:09:40Z"
    name: coredns-5c64b647bf-kxh68.174aab522e730100
    namespace: kube-system
    resourceVersion: "541"
    uid: 951c95e9-2179-44e7-a89d-60944f77244d
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: default-scheduler-kube-scheduler-5cf64855c7-q54kd
  source: {}
  type: Normal
- count: 8
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:43Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: coredns-5c64b647bf-kxh68
    namespace: kube-system
    resourceVersion: "533"
    uid: 339e0bc9-99d7-4ffb-a6ea-eafd28060908
  lastTimestamp: "2023-03-09T06:09:57Z"
  message: 'network is not ready: container runtime network not ready: NetworkReady=false
    reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin
    not initialized'
  metadata:
    creationTimestamp: "2023-03-09T06:09:44Z"
    name: coredns-5c64b647bf-kxh68.174aab52e9424550
    namespace: kube-system
    resourceVersion: "645"
    uid: 8181e6ee-e01c-46a2-a0e0-8a5ba60d8e73
  reason: NetworkNotReady
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Warning
- count: 6
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:43Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: coredns-5c64b647bf-kxh68
    namespace: kube-system
    resourceVersion: "533"
    uid: 339e0bc9-99d7-4ffb-a6ea-eafd28060908
  lastTimestamp: "2023-03-09T06:09:59Z"
  message: 'MountVolume.SetUp failed for volume "config-volume" : object "kube-system"/"coredns"
    not registered'
  metadata:
    creationTimestamp: "2023-03-09T06:09:44Z"
    name: coredns-5c64b647bf-kxh68.174aab52f083ed6c
    namespace: kube-system
    resourceVersion: "652"
    uid: 6ba2e44a-9768-4771-8fd5-75abf73b9b84
  reason: FailedMount
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Warning
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:16Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{coredns}
    kind: Pod
    name: coredns-5c64b647bf-kxh68
    namespace: kube-system
    resourceVersion: "533"
    uid: 339e0bc9-99d7-4ffb-a6ea-eafd28060908
  lastTimestamp: "2023-03-09T06:10:16Z"
  message: Pulling image "linode/coredns:1.9.3"
  metadata:
    creationTimestamp: "2023-03-09T06:10:16Z"
    name: coredns-5c64b647bf-kxh68.174aab5a976cf81b
    namespace: kube-system
    resourceVersion: "748"
    uid: 23af44ad-3e41-4773-8ecb-838061d1ab83
  reason: Pulling
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:18Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{coredns}
    kind: Pod
    name: coredns-5c64b647bf-kxh68
    namespace: kube-system
    resourceVersion: "533"
    uid: 339e0bc9-99d7-4ffb-a6ea-eafd28060908
  lastTimestamp: "2023-03-09T06:10:18Z"
  message: Successfully pulled image "linode/coredns:1.9.3" in 2.19921074s
  metadata:
    creationTimestamp: "2023-03-09T06:10:18Z"
    name: coredns-5c64b647bf-kxh68.174aab5b1a8267b4
    namespace: kube-system
    resourceVersion: "760"
    uid: 04e0ada2-6ff4-4b5e-86b7-390014e84c8d
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:18Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{coredns}
    kind: Pod
    name: coredns-5c64b647bf-kxh68
    namespace: kube-system
    resourceVersion: "533"
    uid: 339e0bc9-99d7-4ffb-a6ea-eafd28060908
  lastTimestamp: "2023-03-09T06:10:18Z"
  message: Created container coredns
  metadata:
    creationTimestamp: "2023-03-09T06:10:18Z"
    name: coredns-5c64b647bf-kxh68.174aab5b1c69d81b
    namespace: kube-system
    resourceVersion: "761"
    uid: 03c78c31-70fb-4549-bac0-158ded8c33d8
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:19Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{coredns}
    kind: Pod
    name: coredns-5c64b647bf-kxh68
    namespace: kube-system
    resourceVersion: "533"
    uid: 339e0bc9-99d7-4ffb-a6ea-eafd28060908
  lastTimestamp: "2023-03-09T06:10:19Z"
  message: Started container coredns
  metadata:
    creationTimestamp: "2023-03-09T06:10:19Z"
    name: coredns-5c64b647bf-kxh68.174aab5b200986e1
    namespace: kube-system
    resourceVersion: "762"
    uid: b143de28-50b5-4b5b-a03f-a9599c6f3e39
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:19Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{coredns}
    kind: Pod
    name: coredns-5c64b647bf-kxh68
    namespace: kube-system
    resourceVersion: "533"
    uid: 339e0bc9-99d7-4ffb-a6ea-eafd28060908
  lastTimestamp: "2023-03-09T06:10:19Z"
  message: 'Readiness probe failed: HTTP probe failed with statuscode: 503'
  metadata:
    creationTimestamp: "2023-03-09T06:10:19Z"
    name: coredns-5c64b647bf-kxh68.174aab5b276239db
    namespace: kube-system
    resourceVersion: "763"
    uid: f766de25-f591-4192-b59b-3881fc8e6fad
  reason: Unhealthy
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Warning
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: ReplicaSet
    name: coredns-5c64b647bf
    namespace: kube-system
    resourceVersion: "460"
    uid: a61e81aa-9173-4c5e-861d-68f16dc7471c
  lastTimestamp: "2023-03-09T06:09:11Z"
  message: 'Created pod: coredns-5c64b647bf-gkmxq'
  metadata:
    creationTimestamp: "2023-03-09T06:09:11Z"
    name: coredns-5c64b647bf.174aab4b633d40ea
    namespace: kube-system
    resourceVersion: "493"
    uid: 9083af94-f62e-48dd-808b-71fdbaf29da8
  reason: SuccessfulCreate
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: replicaset-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: ReplicaSet
    name: coredns-5c64b647bf
    namespace: kube-system
    resourceVersion: "460"
    uid: a61e81aa-9173-4c5e-861d-68f16dc7471c
  lastTimestamp: "2023-03-09T06:09:11Z"
  message: 'Created pod: coredns-5c64b647bf-kxh68'
  metadata:
    creationTimestamp: "2023-03-09T06:09:12Z"
    name: coredns-5c64b647bf.174aab4b67ca87ee
    namespace: kube-system
    resourceVersion: "500"
    uid: 218cf3a4-ad76-4299-b75c-db8ab698ced3
  reason: SuccessfulCreate
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: replicaset-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: coredns
    namespace: kube-system
    resourceVersion: "291"
    uid: 5ea2a960-69ee-42d0-97b8-e093f6bd2fd9
  lastTimestamp: "2023-03-09T06:09:09Z"
  message: Scaled up replica set coredns-5c64b647bf to 2
  metadata:
    creationTimestamp: "2023-03-09T06:09:10Z"
    name: coredns.174aab4b08704cde
    namespace: kube-system
    resourceVersion: "474"
    uid: 891ddc80-53a8-4a3e-81b0-6d6e60544b27
  reason: ScalingReplicaSet
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: deployment-controller
  type: Normal
- action: Scheduling
  eventTime: "2023-03-09T06:09:25.701240Z"
  firstTimestamp: null
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: csi-linode-controller-0
    namespace: kube-system
    resourceVersion: "482"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  lastTimestamp: null
  message: no nodes available to schedule pods
  metadata:
    creationTimestamp: "2023-03-09T06:09:25Z"
    name: csi-linode-controller-0.174aab4eb62ee052
    namespace: kube-system
    resourceVersion: "517"
    uid: b3daa2c1-a789-41bb-ac3f-cc9a3d6eb8db
  reason: FailedScheduling
  reportingComponent: default-scheduler
  reportingInstance: default-scheduler-kube-scheduler-5cf64855c7-q54kd
  source: {}
  type: Warning
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:32Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: csi-linode-controller-0
    namespace: kube-system
    resourceVersion: "518"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  lastTimestamp: "2023-03-09T06:09:32Z"
  message: 'pod didn''t trigger scale-up: '
  metadata:
    creationTimestamp: "2023-03-09T06:09:32Z"
    name: csi-linode-controller-0.174aab504dd901ea
    namespace: kube-system
    resourceVersion: "527"
    uid: 35494fd0-de79-4d52-8f3a-1c10af6596cc
  reason: NotTriggerScaleUp
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: cluster-autoscaler
  type: Normal
- action: Binding
  eventTime: "2023-03-09T06:09:40.502437Z"
  firstTimestamp: null
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: csi-linode-controller-0
    namespace: kube-system
    resourceVersion: "518"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  lastTimestamp: null
  message: Successfully assigned kube-system/csi-linode-controller-0 to lke96996-146211-640977a3930b
  metadata:
    creationTimestamp: "2023-03-09T06:09:40Z"
    name: csi-linode-controller-0.174aab52286b8a98
    namespace: kube-system
    resourceVersion: "535"
    uid: 501681a3-f099-412b-b436-80c8c87520b0
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: default-scheduler-kube-scheduler-5cf64855c7-q54kd
  source: {}
  type: Normal
- count: 8
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:43Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: csi-linode-controller-0
    namespace: kube-system
    resourceVersion: "531"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  lastTimestamp: "2023-03-09T06:09:57Z"
  message: 'network is not ready: container runtime network not ready: NetworkReady=false
    reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin
    not initialized'
  metadata:
    creationTimestamp: "2023-03-09T06:09:44Z"
    name: csi-linode-controller-0.174aab52e937a016
    namespace: kube-system
    resourceVersion: "647"
    uid: aaa9c758-3c44-41a6-a49f-21048902c2f5
  reason: NetworkNotReady
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Warning
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:00Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: csi-linode-controller-0
    namespace: kube-system
    resourceVersion: "531"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  lastTimestamp: "2023-03-09T06:10:00Z"
  message: 'Failed to create pod sandbox: rpc error: code = Unknown desc = failed
    to setup network for sandbox "51295bee7c12b1099aabe3a1c0beb0bbb9a6849991260f03169c7dde5bdffe36":
    plugin type="calico" failed (add): stat /var/lib/calico/nodename: no such file
    or directory: check that the calico/node container is running and has mounted
    /var/lib/calico/'
  metadata:
    creationTimestamp: "2023-03-09T06:10:00Z"
    name: csi-linode-controller-0.174aab56c426ce8a
    namespace: kube-system
    resourceVersion: "655"
    uid: b117973a-6883-4f8f-9bfd-337f134534f9
  reason: FailedCreatePodSandBox
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Warning
- count: 2
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:01Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: csi-linode-controller-0
    namespace: kube-system
    resourceVersion: "531"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  lastTimestamp: "2023-03-09T06:10:15Z"
  message: Pod sandbox changed, it will be killed and re-created.
  metadata:
    creationTimestamp: "2023-03-09T06:10:01Z"
    name: csi-linode-controller-0.174aab56ee85e407
    namespace: kube-system
    resourceVersion: "741"
    uid: b2fb9682-0f83-486d-bbd4-410a7d94450a
  reason: SandboxChanged
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:16Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{init}
    kind: Pod
    name: csi-linode-controller-0
    namespace: kube-system
    resourceVersion: "531"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  lastTimestamp: "2023-03-09T06:10:16Z"
  message: Container image "bitnami/kubectl:1.16.3-debian-10-r36" already present
    on machine
  metadata:
    creationTimestamp: "2023-03-09T06:10:16Z"
    name: csi-linode-controller-0.174aab5a9e293af1
    namespace: kube-system
    resourceVersion: "751"
    uid: ebf2b9b0-ca53-4769-8c28-76f1b64200a5
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:16Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{init}
    kind: Pod
    name: csi-linode-controller-0
    namespace: kube-system
    resourceVersion: "531"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  lastTimestamp: "2023-03-09T06:10:16Z"
  message: Created container init
  metadata:
    creationTimestamp: "2023-03-09T06:10:16Z"
    name: csi-linode-controller-0.174aab5a9f1ddf93
    namespace: kube-system
    resourceVersion: "752"
    uid: c1bc1c8d-2582-45cf-a828-703df65f6b9f
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:16Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{init}
    kind: Pod
    name: csi-linode-controller-0
    namespace: kube-system
    resourceVersion: "531"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  lastTimestamp: "2023-03-09T06:10:16Z"
  message: Started container init
  metadata:
    creationTimestamp: "2023-03-09T06:10:16Z"
    name: csi-linode-controller-0.174aab5aa195984d
    namespace: kube-system
    resourceVersion: "753"
    uid: 9a52598f-738c-4635-8ca3-eb8429ec6a82
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:17Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-provisioner}
    kind: Pod
    name: csi-linode-controller-0
    namespace: kube-system
    resourceVersion: "531"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  lastTimestamp: "2023-03-09T06:10:17Z"
  message: Pulling image "linode/csi-provisioner:v3.0.0"
  metadata:
    creationTimestamp: "2023-03-09T06:10:17Z"
    name: csi-linode-controller-0.174aab5aaec9013d
    namespace: kube-system
    resourceVersion: "754"
    uid: 9d15f7e5-4aef-413a-bfe3-c942ca997053
  reason: Pulling
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:22Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-provisioner}
    kind: Pod
    name: csi-linode-controller-0
    namespace: kube-system
    resourceVersion: "531"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  lastTimestamp: "2023-03-09T06:10:22Z"
  message: Successfully pulled image "linode/csi-provisioner:v3.0.0" in 4.952704393s
  metadata:
    creationTimestamp: "2023-03-09T06:10:22Z"
    name: csi-linode-controller-0.174aab5bd5fdc100
    namespace: kube-system
    resourceVersion: "798"
    uid: fb2709be-a501-44f0-909d-55274b271a51
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:22Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-provisioner}
    kind: Pod
    name: csi-linode-controller-0
    namespace: kube-system
    resourceVersion: "531"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  lastTimestamp: "2023-03-09T06:10:22Z"
  message: Created container csi-provisioner
  metadata:
    creationTimestamp: "2023-03-09T06:10:22Z"
    name: csi-linode-controller-0.174aab5bd7d4f026
    namespace: kube-system
    resourceVersion: "799"
    uid: 9af07b80-f80e-42cc-a41b-6832765a48a2
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:22Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-provisioner}
    kind: Pod
    name: csi-linode-controller-0
    namespace: kube-system
    resourceVersion: "531"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  lastTimestamp: "2023-03-09T06:10:22Z"
  message: Started container csi-provisioner
  metadata:
    creationTimestamp: "2023-03-09T06:10:22Z"
    name: csi-linode-controller-0.174aab5bdb9a395c
    namespace: kube-system
    resourceVersion: "800"
    uid: d98c29ff-c04e-414e-93a2-bd2b8ccfcec0
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:22Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-attacher}
    kind: Pod
    name: csi-linode-controller-0
    namespace: kube-system
    resourceVersion: "531"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  lastTimestamp: "2023-03-09T06:10:22Z"
  message: Pulling image "linode/csi-attacher:v3.3.0"
  metadata:
    creationTimestamp: "2023-03-09T06:10:22Z"
    name: csi-linode-controller-0.174aab5bdbd37a2d
    namespace: kube-system
    resourceVersion: "801"
    uid: f0491aca-5830-410c-add6-a98dc3d92236
  reason: Pulling
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:24Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-attacher}
    kind: Pod
    name: csi-linode-controller-0
    namespace: kube-system
    resourceVersion: "531"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  lastTimestamp: "2023-03-09T06:10:24Z"
  message: Successfully pulled image "linode/csi-attacher:v3.3.0" in 2.321280491s
  metadata:
    creationTimestamp: "2023-03-09T06:10:24Z"
    name: csi-linode-controller-0.174aab5c66301af7
    namespace: kube-system
    resourceVersion: "813"
    uid: f737e3d1-4e3a-4ff2-abc2-89c2113c74ce
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:24Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-attacher}
    kind: Pod
    name: csi-linode-controller-0
    namespace: kube-system
    resourceVersion: "531"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  lastTimestamp: "2023-03-09T06:10:24Z"
  message: Created container csi-attacher
  metadata:
    creationTimestamp: "2023-03-09T06:10:24Z"
    name: csi-linode-controller-0.174aab5c683b6251
    namespace: kube-system
    resourceVersion: "814"
    uid: 667cf2a3-abea-4cd9-be5b-3a18a40da06b
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:24Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-attacher}
    kind: Pod
    name: csi-linode-controller-0
    namespace: kube-system
    resourceVersion: "531"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  lastTimestamp: "2023-03-09T06:10:24Z"
  message: Started container csi-attacher
  metadata:
    creationTimestamp: "2023-03-09T06:10:24Z"
    name: csi-linode-controller-0.174aab5c6d0410cc
    namespace: kube-system
    resourceVersion: "815"
    uid: f93c6bfa-2857-4e16-82f0-e2f501e11b2e
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:24Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-resizer}
    kind: Pod
    name: csi-linode-controller-0
    namespace: kube-system
    resourceVersion: "531"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  lastTimestamp: "2023-03-09T06:10:24Z"
  message: Pulling image "linode/csi-resizer:v1.3.0"
  metadata:
    creationTimestamp: "2023-03-09T06:10:24Z"
    name: csi-linode-controller-0.174aab5c6d196452
    namespace: kube-system
    resourceVersion: "816"
    uid: 7b1264c4-1954-4193-a9af-cc770f9201e7
  reason: Pulling
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:27Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-resizer}
    kind: Pod
    name: csi-linode-controller-0
    namespace: kube-system
    resourceVersion: "531"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  lastTimestamp: "2023-03-09T06:10:27Z"
  message: Successfully pulled image "linode/csi-resizer:v1.3.0" in 2.443438657s
  metadata:
    creationTimestamp: "2023-03-09T06:10:27Z"
    name: csi-linode-controller-0.174aab5cfebe51cc
    namespace: kube-system
    resourceVersion: "817"
    uid: d9a2b7cc-6498-4de9-a21e-864333a5d665
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:27Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-resizer}
    kind: Pod
    name: csi-linode-controller-0
    namespace: kube-system
    resourceVersion: "531"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  lastTimestamp: "2023-03-09T06:10:27Z"
  message: Created container csi-resizer
  metadata:
    creationTimestamp: "2023-03-09T06:10:27Z"
    name: csi-linode-controller-0.174aab5d00d2f479
    namespace: kube-system
    resourceVersion: "818"
    uid: c9a83eaf-5f5f-4eff-9ada-75a43a51a668
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: StatefulSet
    name: csi-linode-controller
    namespace: kube-system
    resourceVersion: "277"
    uid: 862d2eb3-2b1e-4145-b471-af89ddd2b4a9
  lastTimestamp: "2023-03-09T06:09:11Z"
  message: create Pod csi-linode-controller-0 in StatefulSet csi-linode-controller
    successful
  metadata:
    creationTimestamp: "2023-03-09T06:09:11Z"
    name: csi-linode-controller.174aab4b4a055978
    namespace: kube-system
    resourceVersion: "484"
    uid: 37896d0b-e107-4fed-8367-b02a4cc3c068
  reason: SuccessfulCreate
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: statefulset-controller
  type: Normal
- action: Binding
  eventTime: "2023-03-09T06:09:40.809073Z"
  firstTimestamp: null
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: csi-linode-node-bwnv6
    namespace: kube-system
    resourceVersion: "534"
    uid: 69708164-419b-46a4-8b6c-7aeb9eb17247
  lastTimestamp: null
  message: Successfully assigned kube-system/csi-linode-node-bwnv6 to lke96996-146211-640977a3930b
  metadata:
    creationTimestamp: "2023-03-09T06:09:40Z"
    name: csi-linode-node-bwnv6.174aab523aadf312
    namespace: kube-system
    resourceVersion: "549"
    uid: 63a780e1-c826-48e9-9efe-4b0cd9f57bc0
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: default-scheduler-kube-scheduler-5cf64855c7-q54kd
  source: {}
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:46Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{init}
    kind: Pod
    name: csi-linode-node-bwnv6
    namespace: kube-system
    resourceVersion: "547"
    uid: 69708164-419b-46a4-8b6c-7aeb9eb17247
  lastTimestamp: "2023-03-09T06:09:46Z"
  message: Pulling image "bitnami/kubectl:1.16.3-debian-10-r36"
  metadata:
    creationTimestamp: "2023-03-09T06:09:46Z"
    name: csi-linode-node-bwnv6.174aab537963e8cb
    namespace: kube-system
    resourceVersion: "594"
    uid: 4699fbaf-cf22-414b-98a1-47deea0f11ac
  reason: Pulling
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:56Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{init}
    kind: Pod
    name: csi-linode-node-bwnv6
    namespace: kube-system
    resourceVersion: "547"
    uid: 69708164-419b-46a4-8b6c-7aeb9eb17247
  lastTimestamp: "2023-03-09T06:09:56Z"
  message: Successfully pulled image "bitnami/kubectl:1.16.3-debian-10-r36" in 10.831070335s
  metadata:
    creationTimestamp: "2023-03-09T06:09:56Z"
    name: csi-linode-node-bwnv6.174aab55fef92337
    namespace: kube-system
    resourceVersion: "642"
    uid: cbea208f-6883-4b91-8f85-10bf3f4993a8
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:57Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{init}
    kind: Pod
    name: csi-linode-node-bwnv6
    namespace: kube-system
    resourceVersion: "547"
    uid: 69708164-419b-46a4-8b6c-7aeb9eb17247
  lastTimestamp: "2023-03-09T06:09:57Z"
  message: Created container init
  metadata:
    creationTimestamp: "2023-03-09T06:09:57Z"
    name: csi-linode-node-bwnv6.174aab5603204ef7
    namespace: kube-system
    resourceVersion: "643"
    uid: 6f5cc73d-2be8-42f6-b044-e03c6d2e64e5
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:57Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{init}
    kind: Pod
    name: csi-linode-node-bwnv6
    namespace: kube-system
    resourceVersion: "547"
    uid: 69708164-419b-46a4-8b6c-7aeb9eb17247
  lastTimestamp: "2023-03-09T06:09:57Z"
  message: Started container init
  metadata:
    creationTimestamp: "2023-03-09T06:09:57Z"
    name: csi-linode-node-bwnv6.174aab5606e90f6c
    namespace: kube-system
    resourceVersion: "644"
    uid: 923d9b89-c988-4370-8c85-0852361cfbb5
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:28Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-node-driver-registrar}
    kind: Pod
    name: csi-linode-node-bwnv6
    namespace: kube-system
    resourceVersion: "547"
    uid: 69708164-419b-46a4-8b6c-7aeb9eb17247
  lastTimestamp: "2023-03-09T06:10:28Z"
  message: Pulling image "linode/csi-node-driver-registrar:v1.3.0"
  metadata:
    creationTimestamp: "2023-03-09T06:10:28Z"
    name: csi-linode-node-bwnv6.174aab5d446a255c
    namespace: kube-system
    resourceVersion: "824"
    uid: 855199ee-2624-4203-8847-99e911f993e8
  reason: Pulling
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:31Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-node-driver-registrar}
    kind: Pod
    name: csi-linode-node-bwnv6
    namespace: kube-system
    resourceVersion: "547"
    uid: 69708164-419b-46a4-8b6c-7aeb9eb17247
  lastTimestamp: "2023-03-09T06:10:31Z"
  message: Successfully pulled image "linode/csi-node-driver-registrar:v1.3.0" in
    3.073766432s
  metadata:
    creationTimestamp: "2023-03-09T06:10:31Z"
    name: csi-linode-node-bwnv6.174aab5dfba06cce
    namespace: kube-system
    resourceVersion: "839"
    uid: 731d74a8-fa63-42d3-8f47-66ae13f423a5
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:31Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-node-driver-registrar}
    kind: Pod
    name: csi-linode-node-bwnv6
    namespace: kube-system
    resourceVersion: "547"
    uid: 69708164-419b-46a4-8b6c-7aeb9eb17247
  lastTimestamp: "2023-03-09T06:10:31Z"
  message: Created container csi-node-driver-registrar
  metadata:
    creationTimestamp: "2023-03-09T06:10:31Z"
    name: csi-linode-node-bwnv6.174aab5dfc91e280
    namespace: kube-system
    resourceVersion: "840"
    uid: 72a148cf-f4aa-4217-aefa-84a177e8d50c
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:31Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-node-driver-registrar}
    kind: Pod
    name: csi-linode-node-bwnv6
    namespace: kube-system
    resourceVersion: "547"
    uid: 69708164-419b-46a4-8b6c-7aeb9eb17247
  lastTimestamp: "2023-03-09T06:10:31Z"
  message: Started container csi-node-driver-registrar
  metadata:
    creationTimestamp: "2023-03-09T06:10:31Z"
    name: csi-linode-node-bwnv6.174aab5dff9ac5f6
    namespace: kube-system
    resourceVersion: "841"
    uid: ab4f7315-11da-4d07-954e-bce4660779f2
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:31Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-linode-plugin}
    kind: Pod
    name: csi-linode-node-bwnv6
    namespace: kube-system
    resourceVersion: "547"
    uid: 69708164-419b-46a4-8b6c-7aeb9eb17247
  lastTimestamp: "2023-03-09T06:10:31Z"
  message: Container image "linode/linode-blockstorage-csi-driver:v0.5.0" already
    present on machine
  metadata:
    creationTimestamp: "2023-03-09T06:10:31Z"
    name: csi-linode-node-bwnv6.174aab5dffad4496
    namespace: kube-system
    resourceVersion: "842"
    uid: 5d9037b1-a6ff-478c-bc3f-fbb0c812576b
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:31Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-linode-plugin}
    kind: Pod
    name: csi-linode-node-bwnv6
    namespace: kube-system
    resourceVersion: "547"
    uid: 69708164-419b-46a4-8b6c-7aeb9eb17247
  lastTimestamp: "2023-03-09T06:10:31Z"
  message: Created container csi-linode-plugin
  metadata:
    creationTimestamp: "2023-03-09T06:10:31Z"
    name: csi-linode-node-bwnv6.174aab5e00b4a86b
    namespace: kube-system
    resourceVersion: "843"
    uid: f7470cd7-a67f-4478-be16-f691cc92b429
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:31Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-linode-plugin}
    kind: Pod
    name: csi-linode-node-bwnv6
    namespace: kube-system
    resourceVersion: "547"
    uid: 69708164-419b-46a4-8b6c-7aeb9eb17247
  lastTimestamp: "2023-03-09T06:10:31Z"
  message: Started container csi-linode-plugin
  metadata:
    creationTimestamp: "2023-03-09T06:10:31Z"
    name: csi-linode-node-bwnv6.174aab5e033b379e
    namespace: kube-system
    resourceVersion: "844"
    uid: 08380eac-e585-4ec5-bcdd-e4a815e8aa8b
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- action: Binding
  eventTime: "2023-03-09T06:10:10.038900Z"
  firstTimestamp: null
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: csi-linode-node-pttdn
    namespace: kube-system
    resourceVersion: "697"
    uid: 8b9785df-ef7d-4aab-8eed-912a78bc3eef
  lastTimestamp: null
  message: Successfully assigned kube-system/csi-linode-node-pttdn to lke96996-146211-640977a3e82e
  metadata:
    creationTimestamp: "2023-03-09T06:10:10Z"
    name: csi-linode-node-pttdn.174aab5908e9d032
    namespace: kube-system
    resourceVersion: "704"
    uid: 8d338a4f-1b3f-455b-9be6-020e9f83eba1
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: default-scheduler-kube-scheduler-5cf64855c7-q54kd
  source: {}
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:14Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{init}
    kind: Pod
    name: csi-linode-node-pttdn
    namespace: kube-system
    resourceVersion: "701"
    uid: 8b9785df-ef7d-4aab-8eed-912a78bc3eef
  lastTimestamp: "2023-03-09T06:10:14Z"
  message: Pulling image "bitnami/kubectl:1.16.3-debian-10-r36"
  metadata:
    creationTimestamp: "2023-03-09T06:10:15Z"
    name: csi-linode-node-pttdn.174aab5a1cc058bd
    namespace: kube-system
    resourceVersion: "740"
    uid: d84eae3c-cc68-4a93-bcb4-1d3ea4f5ece8
  reason: Pulling
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:27Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{init}
    kind: Pod
    name: csi-linode-node-pttdn
    namespace: kube-system
    resourceVersion: "701"
    uid: 8b9785df-ef7d-4aab-8eed-912a78bc3eef
  lastTimestamp: "2023-03-09T06:10:27Z"
  message: Successfully pulled image "bitnami/kubectl:1.16.3-debian-10-r36" in 12.958099096s
  metadata:
    creationTimestamp: "2023-03-09T06:10:27Z"
    name: csi-linode-node-pttdn.174aab5d211d8bff
    namespace: kube-system
    resourceVersion: "819"
    uid: fb6f0476-b202-4cb5-bb7d-d3c3926b3bba
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:27Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{init}
    kind: Pod
    name: csi-linode-node-pttdn
    namespace: kube-system
    resourceVersion: "701"
    uid: 8b9785df-ef7d-4aab-8eed-912a78bc3eef
  lastTimestamp: "2023-03-09T06:10:27Z"
  message: Created container init
  metadata:
    creationTimestamp: "2023-03-09T06:10:27Z"
    name: csi-linode-node-pttdn.174aab5d24beb891
    namespace: kube-system
    resourceVersion: "820"
    uid: 2d9d6209-0537-4222-ae8f-afc9d97a36fb
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:27Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.initContainers{init}
    kind: Pod
    name: csi-linode-node-pttdn
    namespace: kube-system
    resourceVersion: "701"
    uid: 8b9785df-ef7d-4aab-8eed-912a78bc3eef
  lastTimestamp: "2023-03-09T06:10:27Z"
  message: Started container init
  metadata:
    creationTimestamp: "2023-03-09T06:10:27Z"
    name: csi-linode-node-pttdn.174aab5d2bf538c7
    namespace: kube-system
    resourceVersion: "821"
    uid: 08d2a537-7bb9-4a27-9607-02f452761a80
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:28Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-node-driver-registrar}
    kind: Pod
    name: csi-linode-node-pttdn
    namespace: kube-system
    resourceVersion: "701"
    uid: 8b9785df-ef7d-4aab-8eed-912a78bc3eef
  lastTimestamp: "2023-03-09T06:10:28Z"
  message: Pulling image "linode/csi-node-driver-registrar:v1.3.0"
  metadata:
    creationTimestamp: "2023-03-09T06:10:28Z"
    name: csi-linode-node-pttdn.174aab5d6c2624d5
    namespace: kube-system
    resourceVersion: "826"
    uid: 3e1f3c78-9a21-480b-b549-6a3dd2fd9a32
  reason: Pulling
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:32Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-node-driver-registrar}
    kind: Pod
    name: csi-linode-node-pttdn
    namespace: kube-system
    resourceVersion: "701"
    uid: 8b9785df-ef7d-4aab-8eed-912a78bc3eef
  lastTimestamp: "2023-03-09T06:10:32Z"
  message: Successfully pulled image "linode/csi-node-driver-registrar:v1.3.0" in
    3.405927889s
  metadata:
    creationTimestamp: "2023-03-09T06:10:32Z"
    name: csi-linode-node-pttdn.174aab5e3728a5be
    namespace: kube-system
    resourceVersion: "851"
    uid: ca67377a-bfdb-4ffc-9dd6-0cada6300714
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:32Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-node-driver-registrar}
    kind: Pod
    name: csi-linode-node-pttdn
    namespace: kube-system
    resourceVersion: "701"
    uid: 8b9785df-ef7d-4aab-8eed-912a78bc3eef
  lastTimestamp: "2023-03-09T06:10:32Z"
  message: Created container csi-node-driver-registrar
  metadata:
    creationTimestamp: "2023-03-09T06:10:32Z"
    name: csi-linode-node-pttdn.174aab5e3943fd18
    namespace: kube-system
    resourceVersion: "852"
    uid: d8a781ef-d909-4980-97a1-0409e9dd1f8a
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:32Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-node-driver-registrar}
    kind: Pod
    name: csi-linode-node-pttdn
    namespace: kube-system
    resourceVersion: "701"
    uid: 8b9785df-ef7d-4aab-8eed-912a78bc3eef
  lastTimestamp: "2023-03-09T06:10:32Z"
  message: Started container csi-node-driver-registrar
  metadata:
    creationTimestamp: "2023-03-09T06:10:32Z"
    name: csi-linode-node-pttdn.174aab5e3c9c4239
    namespace: kube-system
    resourceVersion: "853"
    uid: 162c6818-b1a8-42bd-9c39-5c7eba96f0cc
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:32Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-linode-plugin}
    kind: Pod
    name: csi-linode-node-pttdn
    namespace: kube-system
    resourceVersion: "701"
    uid: 8b9785df-ef7d-4aab-8eed-912a78bc3eef
  lastTimestamp: "2023-03-09T06:10:32Z"
  message: Pulling image "linode/linode-blockstorage-csi-driver:v0.5.0"
  metadata:
    creationTimestamp: "2023-03-09T06:10:32Z"
    name: csi-linode-node-pttdn.174aab5e3cad4cfe
    namespace: kube-system
    resourceVersion: "854"
    uid: c6a00a8b-cd6f-40a8-982a-6886a8e48d50
  reason: Pulling
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:39Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-linode-plugin}
    kind: Pod
    name: csi-linode-node-pttdn
    namespace: kube-system
    resourceVersion: "701"
    uid: 8b9785df-ef7d-4aab-8eed-912a78bc3eef
  lastTimestamp: "2023-03-09T06:10:39Z"
  message: Successfully pulled image "linode/linode-blockstorage-csi-driver:v0.5.0"
    in 6.778925398s
  metadata:
    creationTimestamp: "2023-03-09T06:10:39Z"
    name: csi-linode-node-pttdn.174aab5fd0bbb6ed
    namespace: kube-system
    resourceVersion: "869"
    uid: 2ec15ea3-6fe8-492e-ac9b-19c6caf7277c
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:39Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-linode-plugin}
    kind: Pod
    name: csi-linode-node-pttdn
    namespace: kube-system
    resourceVersion: "701"
    uid: 8b9785df-ef7d-4aab-8eed-912a78bc3eef
  lastTimestamp: "2023-03-09T06:10:39Z"
  message: Created container csi-linode-plugin
  metadata:
    creationTimestamp: "2023-03-09T06:10:39Z"
    name: csi-linode-node-pttdn.174aab5fd2b7e2fb
    namespace: kube-system
    resourceVersion: "870"
    uid: c07d978f-e6e9-4ac6-b001-9096658ae991
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:39Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{csi-linode-plugin}
    kind: Pod
    name: csi-linode-node-pttdn
    namespace: kube-system
    resourceVersion: "701"
    uid: 8b9785df-ef7d-4aab-8eed-912a78bc3eef
  lastTimestamp: "2023-03-09T06:10:39Z"
  message: Started container csi-linode-plugin
  metadata:
    creationTimestamp: "2023-03-09T06:10:39Z"
    name: csi-linode-node-pttdn.174aab5fdb0f40a6
    namespace: kube-system
    resourceVersion: "872"
    uid: d53b2602-eb97-4521-a927-b05893d0869a
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:40Z"
  involvedObject:
    apiVersion: apps/v1
    kind: DaemonSet
    name: csi-linode-node
    namespace: kube-system
    resourceVersion: "463"
    uid: 9e8b9ac5-0179-4287-9a6c-52d5dd82be12
  lastTimestamp: "2023-03-09T06:09:40Z"
  message: 'Created pod: csi-linode-node-bwnv6'
  metadata:
    creationTimestamp: "2023-03-09T06:09:40Z"
    name: csi-linode-node.174aab52343fdf8b
    namespace: kube-system
    resourceVersion: "550"
    uid: c499935e-2627-4cbf-bfb7-7c8563690b4d
  reason: SuccessfulCreate
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: daemonset-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: DaemonSet
    name: csi-linode-node
    namespace: kube-system
    resourceVersion: "544"
    uid: 9e8b9ac5-0179-4287-9a6c-52d5dd82be12
  lastTimestamp: "2023-03-09T06:10:09Z"
  message: 'Created pod: csi-linode-node-pttdn'
  metadata:
    creationTimestamp: "2023-03-09T06:10:09Z"
    name: csi-linode-node.174aab5904693f7f
    namespace: kube-system
    resourceVersion: "699"
    uid: 6854904f-8e09-4d2e-860c-f244df15534e
  reason: SuccessfulCreate
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: daemonset-controller
  type: Normal
- action: Binding
  eventTime: "2023-03-09T06:10:09.768728Z"
  firstTimestamp: null
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: kube-proxy-dz8bx
    namespace: kube-system
    resourceVersion: "687"
    uid: 77d6af8f-070b-44e9-a3ac-607d2e2208fd
  lastTimestamp: null
  message: Successfully assigned kube-system/kube-proxy-dz8bx to lke96996-146211-640977a3e82e
  metadata:
    creationTimestamp: "2023-03-09T06:10:09Z"
    name: kube-proxy-dz8bx.174aab58f8cf49ba
    namespace: kube-system
    resourceVersion: "691"
    uid: cec6873d-d0b9-42fe-92d5-0126d08502bc
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: default-scheduler-kube-scheduler-5cf64855c7-q54kd
  source: {}
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:14Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-proxy}
    kind: Pod
    name: kube-proxy-dz8bx
    namespace: kube-system
    resourceVersion: "689"
    uid: 77d6af8f-070b-44e9-a3ac-607d2e2208fd
  lastTimestamp: "2023-03-09T06:10:14Z"
  message: Pulling image "linode/kube-proxy-amd64:v1.25.6"
  metadata:
    creationTimestamp: "2023-03-09T06:10:15Z"
    name: kube-proxy-dz8bx.174aab5a1c92ebe0
    namespace: kube-system
    resourceVersion: "739"
    uid: 7e78681e-bad4-42f4-b1a6-480938b3bb85
  reason: Pulling
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:22Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-proxy}
    kind: Pod
    name: kube-proxy-dz8bx
    namespace: kube-system
    resourceVersion: "689"
    uid: 77d6af8f-070b-44e9-a3ac-607d2e2208fd
  lastTimestamp: "2023-03-09T06:10:22Z"
  message: Successfully pulled image "linode/kube-proxy-amd64:v1.25.6" in 7.881478641s
  metadata:
    creationTimestamp: "2023-03-09T06:10:22Z"
    name: kube-proxy-dz8bx.174aab5bf258e9c8
    namespace: kube-system
    resourceVersion: "802"
    uid: 019c87b9-3a1b-44bf-aa49-5e98cba103c5
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:22Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-proxy}
    kind: Pod
    name: kube-proxy-dz8bx
    namespace: kube-system
    resourceVersion: "689"
    uid: 77d6af8f-070b-44e9-a3ac-607d2e2208fd
  lastTimestamp: "2023-03-09T06:10:22Z"
  message: Created container kube-proxy
  metadata:
    creationTimestamp: "2023-03-09T06:10:22Z"
    name: kube-proxy-dz8bx.174aab5bf3d84b2a
    namespace: kube-system
    resourceVersion: "803"
    uid: b786f2bf-1a4d-4e38-91a0-d85d83ec97a1
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:22Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-proxy}
    kind: Pod
    name: kube-proxy-dz8bx
    namespace: kube-system
    resourceVersion: "689"
    uid: 77d6af8f-070b-44e9-a3ac-607d2e2208fd
  lastTimestamp: "2023-03-09T06:10:22Z"
  message: Started container kube-proxy
  metadata:
    creationTimestamp: "2023-03-09T06:10:22Z"
    name: kube-proxy-dz8bx.174aab5bf76ff2eb
    namespace: kube-system
    resourceVersion: "805"
    uid: 7c983f17-f6fb-4e08-b9a7-3c48c1169c22
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- action: Binding
  eventTime: "2023-03-09T06:09:41.000541Z"
  firstTimestamp: null
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: kube-proxy-n4rms
    namespace: kube-system
    resourceVersion: "552"
    uid: 74579826-daed-4442-b86e-aefe5744dda6
  lastTimestamp: null
  message: Successfully assigned kube-system/kube-proxy-n4rms to lke96996-146211-640977a3930b
  metadata:
    creationTimestamp: "2023-03-09T06:09:41Z"
    name: kube-proxy-n4rms.174aab5246185719
    namespace: kube-system
    resourceVersion: "557"
    uid: 2433b483-c2b7-4f75-b808-0ae0de75939e
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: default-scheduler-kube-scheduler-5cf64855c7-q54kd
  source: {}
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:46Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-proxy}
    kind: Pod
    name: kube-proxy-n4rms
    namespace: kube-system
    resourceVersion: "554"
    uid: 74579826-daed-4442-b86e-aefe5744dda6
  lastTimestamp: "2023-03-09T06:09:46Z"
  message: Pulling image "linode/kube-proxy-amd64:v1.25.6"
  metadata:
    creationTimestamp: "2023-03-09T06:09:46Z"
    name: kube-proxy-n4rms.174aab5379eb41ea
    namespace: kube-system
    resourceVersion: "595"
    uid: b2ee1c4c-9613-4eb9-bdaa-ac1f56300a5f
  reason: Pulling
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:00Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-proxy}
    kind: Pod
    name: kube-proxy-n4rms
    namespace: kube-system
    resourceVersion: "554"
    uid: 74579826-daed-4442-b86e-aefe5744dda6
  lastTimestamp: "2023-03-09T06:10:00Z"
  message: Successfully pulled image "linode/kube-proxy-amd64:v1.25.6" in 13.996304011s
  metadata:
    creationTimestamp: "2023-03-09T06:10:00Z"
    name: kube-proxy-n4rms.174aab56bc2a2bbb
    namespace: kube-system
    resourceVersion: "653"
    uid: 16563942-0238-4d6e-a347-b807bf673e15
  reason: Pulled
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:00Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-proxy}
    kind: Pod
    name: kube-proxy-n4rms
    namespace: kube-system
    resourceVersion: "554"
    uid: 74579826-daed-4442-b86e-aefe5744dda6
  lastTimestamp: "2023-03-09T06:10:00Z"
  message: Created container kube-proxy
  metadata:
    creationTimestamp: "2023-03-09T06:10:00Z"
    name: kube-proxy-n4rms.174aab56becdab7a
    namespace: kube-system
    resourceVersion: "654"
    uid: 694772de-2b96-4280-bdbf-129cd384f328
  reason: Created
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:00Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-proxy}
    kind: Pod
    name: kube-proxy-n4rms
    namespace: kube-system
    resourceVersion: "554"
    uid: 74579826-daed-4442-b86e-aefe5744dda6
  lastTimestamp: "2023-03-09T06:10:00Z"
  message: Started container kube-proxy
  metadata:
    creationTimestamp: "2023-03-09T06:10:00Z"
    name: kube-proxy-n4rms.174aab56c6765eab
    namespace: kube-system
    resourceVersion: "657"
    uid: eec6880a-fadd-49bc-a8ad-c9a9e1933f05
  reason: Started
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:40Z"
  involvedObject:
    apiVersion: apps/v1
    kind: DaemonSet
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "485"
    uid: 7317d7c0-81a6-4746-a30e-1c47bf5d88f7
  lastTimestamp: "2023-03-09T06:09:40Z"
  message: 'Created pod: kube-proxy-n4rms'
  metadata:
    creationTimestamp: "2023-03-09T06:09:40Z"
    name: kube-proxy.174aab5241620483
    namespace: kube-system
    resourceVersion: "555"
    uid: 485c2bf6-5182-4450-ae18-54aa035221ba
  reason: SuccessfulCreate
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: daemonset-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: DaemonSet
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "662"
    uid: 7317d7c0-81a6-4746-a30e-1c47bf5d88f7
  lastTimestamp: "2023-03-09T06:10:09Z"
  message: 'Created pod: kube-proxy-dz8bx'
  metadata:
    creationTimestamp: "2023-03-09T06:10:09Z"
    name: kube-proxy.174aab58f7852a7c
    namespace: kube-system
    resourceVersion: "694"
    uid: 0c2d0a5a-9cf5-4522-bf72-eb8f31aa2a68
  reason: SuccessfulCreate
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: daemonset-controller
  type: Normal
kind: EventList
metadata:
  resourceVersion: "1308"
---
apiVersion: v1
items: []
kind: ReplicationControllerList
metadata:
  resourceVersion: "1308"
---
apiVersion: v1
items:
- metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{"lke.linode.com/caplke-version":"v1.25.6-001","prometheus.io/port":"9153","prometheus.io/scrape":"true"},"labels":{"k8s-app":"kube-dns","kubernetes.io/cluster-service":"true","kubernetes.io/name":"KubeDNS"},"name":"kube-dns","namespace":"kube-system","resourceVersion":"0"},"spec":{"clusterIP":"10.128.0.10","ports":[{"name":"dns","port":53,"protocol":"UDP","targetPort":53},{"name":"dns-tcp","port":53,"protocol":"TCP","targetPort":53},{"name":"metrics","port":9153,"protocol":"TCP","targetPort":9153}],"selector":{"k8s-app":"kube-dns"}}}
      lke.linode.com/caplke-version: v1.25.6-001
      prometheus.io/port: "9153"
      prometheus.io/scrape: "true"
    creationTimestamp: "2023-03-09T06:08:43Z"
    labels:
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: KubeDNS
    name: kube-dns
    namespace: kube-system
    resourceVersion: "294"
    uid: 334d43c6-0f1b-4d31-bb55-f6e3949b9c3d
  spec:
    clusterIP: 10.128.0.10
    clusterIPs:
    - 10.128.0.10
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    - name: metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
kind: ServiceList
metadata:
  resourceVersion: "1308"
---
apiVersion: apps/v1
items:
- metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{"lke.linode.com/caplke-version":"v1.25.6-001"},"labels":{"k8s-app":"calico-node"},"name":"calico-node","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"calico-node"}},"template":{"metadata":{"labels":{"k8s-app":"calico-node"}},"spec":{"containers":[{"env":[{"name":"DATASTORE_TYPE","value":"kubernetes"},{"name":"WAIT_FOR_DATASTORE","value":"true"},{"name":"NODENAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"CALICO_NETWORKING_BACKEND","valueFrom":{"configMapKeyRef":{"key":"calico_backend","name":"calico-config"}}},{"name":"CLUSTER_TYPE","value":"k8s,bgp"},{"name":"IP","value":"autodetect"},{"name":"IP_AUTODETECTION_METHOD","value":"can-reach=192.168.128.1"},{"name":"CALICO_IPV4POOL_IPIP","value":"Always"},{"name":"CALICO_IPV4POOL_VXLAN","value":"Never"},{"name":"FELIX_IPINIPMTU","valueFrom":{"configMapKeyRef":{"key":"veth_mtu","name":"calico-config"}}},{"name":"FELIX_VXLANMTU","valueFrom":{"configMapKeyRef":{"key":"veth_mtu","name":"calico-config"}}},{"name":"FELIX_WIREGUARDMTU","valueFrom":{"configMapKeyRef":{"key":"veth_mtu","name":"calico-config"}}},{"name":"CALICO_IPV4POOL_CIDR","value":"10.2.0.0/16"},{"name":"USE_POD_CIDR","value":"true"},{"name":"CALICO_DISABLE_FILE_LOGGING","value":"true"},{"name":"FELIX_DEFAULTENDPOINTTOHOSTACTION","value":"ACCEPT"},{"name":"FELIX_IPV6SUPPORT","value":"false"},{"name":"FELIX_HEALTHENABLED","value":"true"},{"name":"KUBERNETES_SERVICE_HOST","value":"24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net"},{"name":"KUBERNETES_SERVICE_PORT","value":"443"}],"envFrom":[{"configMapRef":{"name":"kubernetes-services-endpoint","optional":true}}],"image":"docker.io/calico/node:v3.24.5","lifecycle":{"preStop":{"exec":{"command":["/bin/calico-node","-shutdown"]}}},"livenessProbe":{"exec":{"command":["/bin/calico-node","-felix-live","-bird-live"]},"failureThreshold":6,"initialDelaySeconds":10,"periodSeconds":10,"timeoutSeconds":10},"name":"calico-node","readinessProbe":{"exec":{"command":["/bin/calico-node","-felix-ready","-bird-ready"]},"periodSeconds":10,"timeoutSeconds":10},"resources":{"requests":{"cpu":"250m"}},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/host/etc/cni/net.d","name":"cni-net-dir","readOnly":false},{"mountPath":"/lib/modules","name":"lib-modules","readOnly":true},{"mountPath":"/run/xtables.lock","name":"xtables-lock","readOnly":false},{"mountPath":"/var/run/calico","name":"var-run-calico","readOnly":false},{"mountPath":"/var/lib/calico","name":"var-lib-calico","readOnly":false},{"mountPath":"/var/run/nodeagent","name":"policysync"},{"mountPath":"/sys/fs/","mountPropagation":"Bidirectional","name":"sysfs"},{"mountPath":"/var/log/calico/cni","name":"cni-log-dir","readOnly":true},{"mountPath":"/etc/service/enabled/birdmod/run","name":"birdmod-run","subPath":"run"},{"mountPath":"/etc/service/enabled/birdmod/log/run","name":"birdmod-log-run","subPath":"run"}]}],"hostNetwork":true,"initContainers":[{"command":["/opt/cni/bin/calico-ipam","-upgrade"],"env":[{"name":"KUBERNETES_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"CALICO_NETWORKING_BACKEND","valueFrom":{"configMapKeyRef":{"key":"calico_backend","name":"calico-config"}}},{"name":"KUBERNETES_SERVICE_HOST","value":"24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net"},{"name":"KUBERNETES_SERVICE_PORT","value":"443"}],"envFrom":[{"configMapRef":{"name":"kubernetes-services-endpoint","optional":true}}],"image":"docker.io/calico/cni:v3.24.5","name":"upgrade-ipam","securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/cni/networks","name":"host-local-net-dir"},{"mountPath":"/host/opt/cni/bin","name":"cni-bin-dir"}]},{"command":["/opt/cni/bin/install"],"env":[{"name":"CNI_CONF_NAME","value":"10-calico.conflist"},{"name":"CNI_NETWORK_CONFIG","valueFrom":{"configMapKeyRef":{"key":"cni_network_config","name":"calico-config"}}},{"name":"KUBERNETES_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"CNI_MTU","valueFrom":{"configMapKeyRef":{"key":"veth_mtu","name":"calico-config"}}},{"name":"SLEEP","value":"false"},{"name":"KUBERNETES_SERVICE_HOST","value":"24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net"},{"name":"KUBERNETES_SERVICE_PORT","value":"443"}],"envFrom":[{"configMapRef":{"name":"kubernetes-services-endpoint","optional":true}}],"image":"docker.io/calico/cni:v3.24.5","name":"install-cni","securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/host/opt/cni/bin","name":"cni-bin-dir"},{"mountPath":"/host/etc/cni/net.d","name":"cni-net-dir"}]},{"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net"},{"name":"KUBERNETES_SERVICE_PORT","value":"443"}],"image":"docker.io/calico/pod2daemon-flexvol:v3.24.5","name":"flexvol-driver","securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/host/driver","name":"flexvol-driver-host"}]}],"nodeSelector":{"kubernetes.io/os":"linux"},"priorityClassName":"system-node-critical","serviceAccountName":"calico-node","terminationGracePeriodSeconds":0,"tolerations":[{"effect":"NoSchedule","operator":"Exists"},{"key":"CriticalAddonsOnly","operator":"Exists"},{"effect":"NoExecute","operator":"Exists"}],"volumes":[{"hostPath":{"path":"/lib/modules"},"name":"lib-modules"},{"hostPath":{"path":"/var/run/calico"},"name":"var-run-calico"},{"hostPath":{"path":"/var/lib/calico"},"name":"var-lib-calico"},{"hostPath":{"path":"/run/xtables.lock","type":"FileOrCreate"},"name":"xtables-lock"},{"hostPath":{"path":"/sys/fs/","type":"DirectoryOrCreate"},"name":"sysfs"},{"hostPath":{"path":"/opt/cni/bin"},"name":"cni-bin-dir"},{"hostPath":{"path":"/etc/cni/net.d"},"name":"cni-net-dir"},{"hostPath":{"path":"/var/log/calico/cni"},"name":"cni-log-dir"},{"hostPath":{"path":"/var/lib/cni/networks"},"name":"host-local-net-dir"},{"hostPath":{"path":"/var/run/nodeagent","type":"DirectoryOrCreate"},"name":"policysync"},{"hostPath":{"path":"/usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds","type":"DirectoryOrCreate"},"name":"flexvol-driver-host"},{"configMap":{"defaultMode":493,"items":[{"key":"birdmod-run","path":"run"}],"name":"calico-birdmod"},"name":"birdmod-run"},{"configMap":{"defaultMode":493,"items":[{"key":"log-run","path":"run"}],"name":"calico-birdmod"},"name":"birdmod-log-run"}]}},"updateStrategy":{"rollingUpdate":{"maxUnavailable":1},"type":"RollingUpdate"}}}
      lke.linode.com/caplke-version: v1.25.6-001
    creationTimestamp: "2023-03-09T06:08:40Z"
    generation: 1
    labels:
      k8s-app: calico-node
    name: calico-node
    namespace: kube-system
    resourceVersion: "884"
    uid: fc991107-9c7b-4a92-9345-fe92f48e066d
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: calico-node
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: calico-node
      spec:
        containers:
        - env:
          - name: DATASTORE_TYPE
            value: kubernetes
          - name: WAIT_FOR_DATASTORE
            value: "true"
          - name: NODENAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: CALICO_NETWORKING_BACKEND
            valueFrom:
              configMapKeyRef:
                key: calico_backend
                name: calico-config
          - name: CLUSTER_TYPE
            value: k8s,bgp
          - name: IP
            value: autodetect
          - name: IP_AUTODETECTION_METHOD
            value: can-reach=192.168.128.1
          - name: CALICO_IPV4POOL_IPIP
            value: Always
          - name: CALICO_IPV4POOL_VXLAN
            value: Never
          - name: FELIX_IPINIPMTU
            valueFrom:
              configMapKeyRef:
                key: veth_mtu
                name: calico-config
          - name: FELIX_VXLANMTU
            valueFrom:
              configMapKeyRef:
                key: veth_mtu
                name: calico-config
          - name: FELIX_WIREGUARDMTU
            valueFrom:
              configMapKeyRef:
                key: veth_mtu
                name: calico-config
          - name: CALICO_IPV4POOL_CIDR
            value: 10.2.0.0/16
          - name: USE_POD_CIDR
            value: "true"
          - name: CALICO_DISABLE_FILE_LOGGING
            value: "true"
          - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
            value: ACCEPT
          - name: FELIX_IPV6SUPPORT
            value: "false"
          - name: FELIX_HEALTHENABLED
            value: "true"
          - name: KUBERNETES_SERVICE_HOST
            value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          envFrom:
          - configMapRef:
              name: kubernetes-services-endpoint
              optional: true
          image: docker.io/calico/node:v3.24.5
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/calico-node
                - -shutdown
          livenessProbe:
            exec:
              command:
              - /bin/calico-node
              - -felix-live
              - -bird-live
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: calico-node
          readinessProbe:
            exec:
              command:
              - /bin/calico-node
              - -felix-ready
              - -bird-ready
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources:
            requests:
              cpu: 250m
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/etc/cni/net.d
            name: cni-net-dir
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /var/run/calico
            name: var-run-calico
          - mountPath: /var/lib/calico
            name: var-lib-calico
          - mountPath: /var/run/nodeagent
            name: policysync
          - mountPath: /sys/fs/
            mountPropagation: Bidirectional
            name: sysfs
          - mountPath: /var/log/calico/cni
            name: cni-log-dir
            readOnly: true
          - mountPath: /etc/service/enabled/birdmod/run
            name: birdmod-run
            subPath: run
          - mountPath: /etc/service/enabled/birdmod/log/run
            name: birdmod-log-run
            subPath: run
        dnsPolicy: ClusterFirst
        hostNetwork: true
        initContainers:
        - command:
          - /opt/cni/bin/calico-ipam
          - -upgrade
          env:
          - name: KUBERNETES_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: CALICO_NETWORKING_BACKEND
            valueFrom:
              configMapKeyRef:
                key: calico_backend
                name: calico-config
          - name: KUBERNETES_SERVICE_HOST
            value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          envFrom:
          - configMapRef:
              name: kubernetes-services-endpoint
              optional: true
          image: docker.io/calico/cni:v3.24.5
          imagePullPolicy: IfNotPresent
          name: upgrade-ipam
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/cni/networks
            name: host-local-net-dir
          - mountPath: /host/opt/cni/bin
            name: cni-bin-dir
        - command:
          - /opt/cni/bin/install
          env:
          - name: CNI_CONF_NAME
            value: 10-calico.conflist
          - name: CNI_NETWORK_CONFIG
            valueFrom:
              configMapKeyRef:
                key: cni_network_config
                name: calico-config
          - name: KUBERNETES_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: CNI_MTU
            valueFrom:
              configMapKeyRef:
                key: veth_mtu
                name: calico-config
          - name: SLEEP
            value: "false"
          - name: KUBERNETES_SERVICE_HOST
            value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          envFrom:
          - configMapRef:
              name: kubernetes-services-endpoint
              optional: true
          image: docker.io/calico/cni:v3.24.5
          imagePullPolicy: IfNotPresent
          name: install-cni
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/opt/cni/bin
            name: cni-bin-dir
          - mountPath: /host/etc/cni/net.d
            name: cni-net-dir
        - env:
          - name: KUBERNETES_SERVICE_HOST
            value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          image: docker.io/calico/pod2daemon-flexvol:v3.24.5
          imagePullPolicy: IfNotPresent
          name: flexvol-driver
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/driver
            name: flexvol-driver-host
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-node
        serviceAccountName: calico-node
        terminationGracePeriodSeconds: 0
        tolerations:
        - effect: NoSchedule
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoExecute
          operator: Exists
        volumes:
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /var/run/calico
            type: ""
          name: var-run-calico
        - hostPath:
            path: /var/lib/calico
            type: ""
          name: var-lib-calico
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /sys/fs/
            type: DirectoryOrCreate
          name: sysfs
        - hostPath:
            path: /opt/cni/bin
            type: ""
          name: cni-bin-dir
        - hostPath:
            path: /etc/cni/net.d
            type: ""
          name: cni-net-dir
        - hostPath:
            path: /var/log/calico/cni
            type: ""
          name: cni-log-dir
        - hostPath:
            path: /var/lib/cni/networks
            type: ""
          name: host-local-net-dir
        - hostPath:
            path: /var/run/nodeagent
            type: DirectoryOrCreate
          name: policysync
        - hostPath:
            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
            type: DirectoryOrCreate
          name: flexvol-driver-host
        - configMap:
            defaultMode: 493
            items:
            - key: birdmod-run
              path: run
            name: calico-birdmod
          name: birdmod-run
        - configMap:
            defaultMode: 493
            items:
            - key: log-run
              path: run
            name: calico-birdmod
          name: birdmod-log-run
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{"lke.linode.com/caplke-version":"v1.25.6-001"},"labels":{"app":"csi-linode-node"},"name":"csi-linode-node","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"app":"csi-linode-node"}},"template":{"metadata":{"labels":{"app":"csi-linode-node","role":"csi-linode"}},"spec":{"containers":[{"args":["--v=2","--csi-address=$(ADDRESS)","--kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)"],"env":[{"name":"ADDRESS","value":"/csi/csi.sock"},{"name":"DRIVER_REG_SOCK_PATH","value":"/var/lib/kubelet/plugins/linodebs.csi.linode.com/csi.sock"},{"name":"KUBE_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"KUBERNETES_SERVICE_HOST","value":"24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net"},{"name":"KUBERNETES_SERVICE_PORT","value":"443"}],"image":"linode/csi-node-driver-registrar:v1.3.0","name":"csi-node-driver-registrar","volumeMounts":[{"mountPath":"/csi","name":"plugin-dir"},{"mountPath":"/registration","name":"registration-dir"}]},{"args":["--endpoint=$(CSI_ENDPOINT)","--token=$(LINODE_TOKEN)","--url=$(LINODE_API_URL)","--node=$(NODE_NAME)","--v=2"],"env":[{"name":"CSI_ENDPOINT","value":"unix:///csi/csi.sock"},{"name":"LINODE_API_URL","valueFrom":{"secretKeyRef":{"key":"apiurl","name":"linode"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"KUBERNETES_SERVICE_HOST","value":"24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net"},{"name":"KUBERNETES_SERVICE_PORT","value":"443"},{"name":"LINODE_URL","valueFrom":{"secretKeyRef":{"key":"apiurl","name":"linode"}}},{"name":"LINODE_TOKEN","valueFrom":{"secretKeyRef":{"key":"token","name":"linode"}}}],"image":"linode/linode-blockstorage-csi-driver:v0.5.0","imagePullPolicy":"IfNotPresent","name":"csi-linode-plugin","securityContext":{"allowPrivilegeEscalation":true,"capabilities":{"add":["SYS_ADMIN"]},"privileged":true},"volumeMounts":[{"mountPath":"/linode-info","name":"linode-info"},{"mountPath":"/scripts","name":"get-linode-id"},{"mountPath":"/csi","name":"plugin-dir"},{"mountPath":"/var/lib/kubelet","mountPropagation":"Bidirectional","name":"pods-mount-dir"},{"mountPath":"/dev","name":"device-dir"}]}],"hostNetwork":true,"initContainers":[{"command":["/scripts/get-linode-id.sh"],"env":[{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}}],"image":"bitnami/kubectl:1.16.3-debian-10-r36","name":"init","volumeMounts":[{"mountPath":"/linode-info","name":"linode-info"},{"mountPath":"/scripts","name":"get-linode-id"}]}],"priorityClassName":"system-node-critical","serviceAccount":"csi-node-sa","tolerations":[{"effect":"NoSchedule","operator":"Exists"},{"key":"CriticalAddonsOnly","operator":"Exists"},{"effect":"NoExecute","operator":"Exists"}],"volumes":[{"emptyDir":{},"name":"linode-info"},{"configMap":{"defaultMode":493,"name":"get-linode-id"},"name":"get-linode-id"},{"hostPath":{"path":"/var/lib/kubelet/plugins_registry/","type":"DirectoryOrCreate"},"name":"registration-dir"},{"hostPath":{"path":"/var/lib/kubelet","type":"Directory"},"name":"kubelet-dir"},{"hostPath":{"path":"/var/lib/kubelet/plugins/linodebs.csi.linode.com","type":"DirectoryOrCreate"},"name":"plugin-dir"},{"hostPath":{"path":"/var/lib/kubelet","type":"Directory"},"name":"pods-mount-dir"},{"hostPath":{"path":"/dev"},"name":"device-dir"},{"hostPath":{"path":"/etc/udev","type":"Directory"},"name":"udev-rules-etc"},{"hostPath":{"path":"/lib/udev","type":"Directory"},"name":"udev-rules-lib"},{"hostPath":{"path":"/run/udev","type":"Directory"},"name":"udev-socket"},{"hostPath":{"path":"/sys","type":"Directory"},"name":"sys"}]}}}}
      lke.linode.com/caplke-version: v1.25.6-001
    creationTimestamp: "2023-03-09T06:08:40Z"
    generation: 1
    labels:
      app: csi-linode-node
    name: csi-linode-node
    namespace: kube-system
    resourceVersion: "874"
    uid: 9e8b9ac5-0179-4287-9a6c-52d5dd82be12
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: csi-linode-node
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-linode-node
          role: csi-linode
      spec:
        containers:
        - args:
          - --v=2
          - --csi-address=$(ADDRESS)
          - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
          env:
          - name: ADDRESS
            value: /csi/csi.sock
          - name: DRIVER_REG_SOCK_PATH
            value: /var/lib/kubelet/plugins/linodebs.csi.linode.com/csi.sock
          - name: KUBE_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: KUBERNETES_SERVICE_HOST
            value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          image: linode/csi-node-driver-registrar:v1.3.0
          imagePullPolicy: IfNotPresent
          name: csi-node-driver-registrar
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: plugin-dir
          - mountPath: /registration
            name: registration-dir
        - args:
          - --endpoint=$(CSI_ENDPOINT)
          - --token=$(LINODE_TOKEN)
          - --url=$(LINODE_API_URL)
          - --node=$(NODE_NAME)
          - --v=2
          env:
          - name: CSI_ENDPOINT
            value: unix:///csi/csi.sock
          - name: LINODE_API_URL
            valueFrom:
              secretKeyRef:
                key: apiurl
                name: linode
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: KUBERNETES_SERVICE_HOST
            value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          - name: LINODE_URL
            valueFrom:
              secretKeyRef:
                key: apiurl
                name: linode
          - name: LINODE_TOKEN
            valueFrom:
              secretKeyRef:
                key: token
                name: linode
          image: linode/linode-blockstorage-csi-driver:v0.5.0
          imagePullPolicy: IfNotPresent
          name: csi-linode-plugin
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /linode-info
            name: linode-info
          - mountPath: /scripts
            name: get-linode-id
          - mountPath: /csi
            name: plugin-dir
          - mountPath: /var/lib/kubelet
            mountPropagation: Bidirectional
            name: pods-mount-dir
          - mountPath: /dev
            name: device-dir
        dnsPolicy: ClusterFirst
        hostNetwork: true
        initContainers:
        - command:
          - /scripts/get-linode-id.sh
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: bitnami/kubectl:1.16.3-debian-10-r36
          imagePullPolicy: IfNotPresent
          name: init
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /linode-info
            name: linode-info
          - mountPath: /scripts
            name: get-linode-id
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: csi-node-sa
        serviceAccountName: csi-node-sa
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoExecute
          operator: Exists
        volumes:
        - emptyDir: {}
          name: linode-info
        - configMap:
            defaultMode: 493
            name: get-linode-id
          name: get-linode-id
        - hostPath:
            path: /var/lib/kubelet/plugins_registry/
            type: DirectoryOrCreate
          name: registration-dir
        - hostPath:
            path: /var/lib/kubelet
            type: Directory
          name: kubelet-dir
        - hostPath:
            path: /var/lib/kubelet/plugins/linodebs.csi.linode.com
            type: DirectoryOrCreate
          name: plugin-dir
        - hostPath:
            path: /var/lib/kubelet
            type: Directory
          name: pods-mount-dir
        - hostPath:
            path: /dev
            type: ""
          name: device-dir
        - hostPath:
            path: /etc/udev
            type: Directory
          name: udev-rules-etc
        - hostPath:
            path: /lib/udev
            type: Directory
          name: udev-rules-lib
        - hostPath:
            path: /run/udev
            type: Directory
          name: udev-socket
        - hostPath:
            path: /sys
            type: Directory
          name: sys
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{"lke.linode.com/caplke-version":"v1.25.6-001"},"labels":{"k8s-app":"kube-proxy"},"name":"kube-proxy","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"kube-proxy"}},"template":{"metadata":{"labels":{"k8s-app":"kube-proxy"}},"spec":{"containers":[{"command":["/usr/local/bin/kube-proxy","--config=/var/lib/kube-proxy/config.conf","--hostname-override=$(NODE_NAME)"],"env":[{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}}],"image":"linode/kube-proxy-amd64:v1.25.6","imagePullPolicy":"IfNotPresent","name":"kube-proxy","securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/kube-proxy","name":"kube-proxy"},{"mountPath":"/run/xtables.lock","name":"xtables-lock","readOnly":false},{"mountPath":"/lib/modules","name":"lib-modules","readOnly":true}]}],"hostNetwork":true,"priorityClassName":"system-node-critical","serviceAccountName":"kube-proxy","tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"},{"operator":"Exists"}],"volumes":[{"configMap":{"defaultMode":420,"name":"kube-proxy"},"name":"kube-proxy"},{"hostPath":{"path":"/run/xtables.lock","type":"FileOrCreate"},"name":"xtables-lock"},{"hostPath":{"path":"/lib/modules"},"name":"lib-modules"}]}},"updateStrategy":{"type":"RollingUpdate"}}}
      lke.linode.com/caplke-version: v1.25.6-001
    creationTimestamp: "2023-03-09T06:08:43Z"
    generation: 1
    labels:
      k8s-app: kube-proxy
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "810"
    uid: 7317d7c0-81a6-4746-a30e-1c47bf5d88f7
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-proxy
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-proxy
      spec:
        containers:
        - command:
          - /usr/local/bin/kube-proxy
          - --config=/var/lib/kube-proxy/config.conf
          - --hostname-override=$(NODE_NAME)
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: linode/kube-proxy-amd64:v1.25.6
          imagePullPolicy: IfNotPresent
          name: kube-proxy
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/kube-proxy
            name: kube-proxy
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-proxy
        serviceAccountName: kube-proxy
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-proxy
          name: kube-proxy
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
kind: DaemonSetList
metadata:
  resourceVersion: "1308"
---
apiVersion: apps/v1
items:
- metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"lke.linode.com/caplke-version":"v1.25.6-001"},"labels":{"k8s-app":"calico-kube-controllers"},"name":"calico-kube-controllers","namespace":"kube-system"},"spec":{"replicas":1,"selector":{"matchLabels":{"k8s-app":"calico-kube-controllers"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"k8s-app":"calico-kube-controllers"},"name":"calico-kube-controllers","namespace":"kube-system"},"spec":{"containers":[{"env":[{"name":"ENABLED_CONTROLLERS","value":"node"},{"name":"DATASTORE_TYPE","value":"kubernetes"},{"name":"KUBERNETES_SERVICE_HOST","value":"24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net"},{"name":"KUBERNETES_SERVICE_PORT","value":"443"}],"image":"docker.io/calico/kube-controllers:v3.24.5","livenessProbe":{"exec":{"command":["/usr/bin/check-status","-l"]},"failureThreshold":6,"initialDelaySeconds":10,"periodSeconds":10,"timeoutSeconds":10},"name":"calico-kube-controllers","readinessProbe":{"exec":{"command":["/usr/bin/check-status","-r"]},"periodSeconds":10}}],"nodeSelector":{"kubernetes.io/os":"linux"},"priorityClassName":"system-cluster-critical","serviceAccountName":"calico-kube-controllers","tolerations":[{"effect":"NoSchedule","operator":"Exists"},{"key":"CriticalAddonsOnly","operator":"Exists"},{"effect":"NoExecute","operator":"Exists"}]}}}}
      lke.linode.com/caplke-version: v1.25.6-001
    creationTimestamp: "2023-03-09T06:08:40Z"
    generation: 1
    labels:
      k8s-app: calico-kube-controllers
    name: calico-kube-controllers
    namespace: kube-system
    resourceVersion: "831"
    uid: d26504cd-f87d-43b4-be7d-7121ecc6bd6e
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: calico-kube-controllers
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: calico-kube-controllers
        name: calico-kube-controllers
        namespace: kube-system
      spec:
        containers:
        - env:
          - name: ENABLED_CONTROLLERS
            value: node
          - name: DATASTORE_TYPE
            value: kubernetes
          - name: KUBERNETES_SERVICE_HOST
            value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          image: docker.io/calico/kube-controllers:v3.24.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/check-status
              - -l
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: calico-kube-controllers
          readinessProbe:
            exec:
              command:
              - /usr/bin/check-status
              - -r
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-kube-controllers
        serviceAccountName: calico-kube-controllers
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoExecute
          operator: Exists
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2023-03-09T06:10:29Z"
      lastUpdateTime: "2023-03-09T06:10:29Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2023-03-09T06:09:09Z"
      lastUpdateTime: "2023-03-09T06:10:29Z"
      message: ReplicaSet "calico-kube-controllers-5dbfbbf9dc" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"lke.linode.com/caplke-version":"v1.25.6-001"},"labels":{"k8s-app":"kube-dns"},"name":"coredns","namespace":"kube-system"},"spec":{"replicas":2,"selector":{"matchLabels":{"k8s-app":"kube-dns"}},"strategy":{"rollingUpdate":{"maxUnavailable":1},"type":"RollingUpdate"},"template":{"metadata":{"labels":{"k8s-app":"kube-dns"}},"spec":{"affinity":{"podAntiAffinity":{"preferredDuringSchedulingIgnoredDuringExecution":[{"podAffinityTerm":{"labelSelector":{"matchExpressions":[{"key":"k8s-app","operator":"In","values":["kube-dns"]}]},"topologyKey":"kubernetes.io/hostname"},"weight":100}]}},"containers":[{"args":["-conf","/etc/coredns/Corefile"],"image":"linode/coredns:1.9.3","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":5,"httpGet":{"path":"/health","port":8080,"scheme":"HTTP"},"initialDelaySeconds":60,"successThreshold":1,"timeoutSeconds":5},"name":"coredns","ports":[{"containerPort":53,"name":"dns","protocol":"UDP"},{"containerPort":53,"name":"dns-tcp","protocol":"TCP"},{"containerPort":9153,"name":"metrics","protocol":"TCP"}],"readinessProbe":{"httpGet":{"path":"/ready","port":8181,"scheme":"HTTP"}},"resources":{"limits":{"memory":"170Mi"},"requests":{"cpu":"100m","memory":"70Mi"}},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"add":["NET_BIND_SERVICE"],"drop":["all"]},"readOnlyRootFilesystem":true},"volumeMounts":[{"mountPath":"/etc/coredns","name":"config-volume","readOnly":true}]}],"dnsPolicy":"Default","nodeSelector":{"kubernetes.io/os":"linux"},"priorityClassName":"system-cluster-critical","serviceAccountName":"coredns","tolerations":[{"effect":"NoSchedule","operator":"Exists"},{"key":"CriticalAddonsOnly","operator":"Exists"},{"effect":"NoExecute","operator":"Exists"}],"volumes":[{"configMap":{"items":[{"key":"Corefile","path":"Corefile"}],"name":"coredns"},"name":"config-volume"}]}}}}
      lke.linode.com/caplke-version: v1.25.6-001
    creationTimestamp: "2023-03-09T06:08:43Z"
    generation: 1
    labels:
      k8s-app: kube-dns
    name: coredns
    namespace: kube-system
    resourceVersion: "788"
    uid: 5ea2a960-69ee-42d0-97b8-e093f6bd2fd9
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - kube-dns
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: linode/coredns:1.9.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoExecute
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2023-03-09T06:10:20Z"
      lastUpdateTime: "2023-03-09T06:10:20Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2023-03-09T06:09:09Z"
      lastUpdateTime: "2023-03-09T06:10:20Z"
      message: ReplicaSet "coredns-5c64b647bf" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
kind: DeploymentList
metadata:
  resourceVersion: "1308"
---
apiVersion: apps/v1
items:
- metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
      lke.linode.com/caplke-version: v1.25.6-001
    creationTimestamp: "2023-03-09T06:09:09Z"
    generation: 1
    labels:
      k8s-app: calico-kube-controllers
      pod-template-hash: 5dbfbbf9dc
    name: calico-kube-controllers-5dbfbbf9dc
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: calico-kube-controllers
      uid: d26504cd-f87d-43b4-be7d-7121ecc6bd6e
    resourceVersion: "830"
    uid: f35434b4-edd5-4b9a-9afe-3fe9a9d76b9d
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: calico-kube-controllers
        pod-template-hash: 5dbfbbf9dc
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: calico-kube-controllers
          pod-template-hash: 5dbfbbf9dc
        name: calico-kube-controllers
        namespace: kube-system
      spec:
        containers:
        - env:
          - name: ENABLED_CONTROLLERS
            value: node
          - name: DATASTORE_TYPE
            value: kubernetes
          - name: KUBERNETES_SERVICE_HOST
            value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          image: docker.io/calico/kube-controllers:v3.24.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/check-status
              - -l
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: calico-kube-controllers
          readinessProbe:
            exec:
              command:
              - /usr/bin/check-status
              - -r
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-kube-controllers
        serviceAccountName: calico-kube-controllers
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoExecute
          operator: Exists
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "1"
      lke.linode.com/caplke-version: v1.25.6-001
    creationTimestamp: "2023-03-09T06:09:09Z"
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: 5c64b647bf
    name: coredns-5c64b647bf
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: coredns
      uid: 5ea2a960-69ee-42d0-97b8-e093f6bd2fd9
    resourceVersion: "787"
    uid: a61e81aa-9173-4c5e-861d-68f16dc7471c
  spec:
    replicas: 2
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: 5c64b647bf
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: 5c64b647bf
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - kube-dns
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: linode/coredns:1.9.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoExecute
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
kind: ReplicaSetList
metadata:
  resourceVersion: "1308"
---
apiVersion: v1
items:
- metadata:
    annotations:
      cni.projectcalico.org/containerID: 1d19f74e9e63ae48cd686f28816ae16f81668aa6da9a83fe7c507c725b54cd7e
      cni.projectcalico.org/podIP: 10.2.0.2/32
      cni.projectcalico.org/podIPs: 10.2.0.2/32
    creationTimestamp: "2023-03-09T06:09:11Z"
    generateName: calico-kube-controllers-5dbfbbf9dc-
    labels:
      k8s-app: calico-kube-controllers
      pod-template-hash: 5dbfbbf9dc
    name: calico-kube-controllers-5dbfbbf9dc-wptvl
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: calico-kube-controllers-5dbfbbf9dc
      uid: f35434b4-edd5-4b9a-9afe-3fe9a9d76b9d
    resourceVersion: "828"
    uid: b68a79ad-5b8d-476a-a340-f1521ac6e199
  spec:
    containers:
    - env:
      - name: ENABLED_CONTROLLERS
        value: node
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: KUBERNETES_SERVICE_HOST
        value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/calico/kube-controllers:v3.24.5
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /usr/bin/check-status
          - -l
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      name: calico-kube-controllers
      readinessProbe:
        exec:
          command:
          - /usr/bin/check-status
          - -r
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-c6svl
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: lke96996-146211-640977a3930b
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-kube-controllers
    serviceAccountName: calico-kube-controllers
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      operator: Exists
    volumes:
    - name: kube-api-access-c6svl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:09:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:09:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d5c6d29cd639f51f4924f664bcda9d3562efc4e779be149c2aeb7a6531d9b8de
      image: docker.io/calico/kube-controllers:v3.24.5
      imageID: docker.io/calico/kube-controllers@sha256:2b6acd7f677f76ffe12ecf3ea7df92eb9b1bdb07336d1ac2a54c7631fb753f7e
      lastState: {}
      name: calico-kube-controllers
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-09T06:10:16Z"
    hostIP: 192.168.129.114
    phase: Running
    podIP: 10.2.0.2
    podIPs:
    - ip: 10.2.0.2
    qosClass: BestEffort
    startTime: "2023-03-09T06:09:43Z"
- metadata:
    creationTimestamp: "2023-03-09T06:09:40Z"
    generateName: calico-node-
    labels:
      controller-revision-hash: 5b565554fb
      k8s-app: calico-node
      pod-template-generation: "1"
    name: calico-node-28hph
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: calico-node
      uid: fc991107-9c7b-4a92-9345-fe92f48e066d
    resourceVersion: "732"
    uid: e4901bc7-42be-425b-9e93-0b022dfbb3e8
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - lke96996-146211-640977a3930b
    containers:
    - env:
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: WAIT_FOR_DATASTORE
        value: "true"
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CALICO_NETWORKING_BACKEND
        valueFrom:
          configMapKeyRef:
            key: calico_backend
            name: calico-config
      - name: CLUSTER_TYPE
        value: k8s,bgp
      - name: IP
        value: autodetect
      - name: IP_AUTODETECTION_METHOD
        value: can-reach=192.168.128.1
      - name: CALICO_IPV4POOL_IPIP
        value: Always
      - name: CALICO_IPV4POOL_VXLAN
        value: Never
      - name: FELIX_IPINIPMTU
        valueFrom:
          configMapKeyRef:
            key: veth_mtu
            name: calico-config
      - name: FELIX_VXLANMTU
        valueFrom:
          configMapKeyRef:
            key: veth_mtu
            name: calico-config
      - name: FELIX_WIREGUARDMTU
        valueFrom:
          configMapKeyRef:
            key: veth_mtu
            name: calico-config
      - name: CALICO_IPV4POOL_CIDR
        value: 10.2.0.0/16
      - name: USE_POD_CIDR
        value: "true"
      - name: CALICO_DISABLE_FILE_LOGGING
        value: "true"
      - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
        value: ACCEPT
      - name: FELIX_IPV6SUPPORT
        value: "false"
      - name: FELIX_HEALTHENABLED
        value: "true"
      - name: KUBERNETES_SERVICE_HOST
        value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      envFrom:
      - configMapRef:
          name: kubernetes-services-endpoint
          optional: true
      image: docker.io/calico/node:v3.24.5
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/calico-node
            - -shutdown
      livenessProbe:
        exec:
          command:
          - /bin/calico-node
          - -felix-live
          - -bird-live
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      name: calico-node
      readinessProbe:
        exec:
          command:
          - /bin/calico-node
          - -felix-ready
          - -bird-ready
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      resources:
        requests:
          cpu: 250m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/lib/calico
        name: var-lib-calico
      - mountPath: /var/run/nodeagent
        name: policysync
      - mountPath: /sys/fs/
        mountPropagation: Bidirectional
        name: sysfs
      - mountPath: /var/log/calico/cni
        name: cni-log-dir
        readOnly: true
      - mountPath: /etc/service/enabled/birdmod/run
        name: birdmod-run
        subPath: run
      - mountPath: /etc/service/enabled/birdmod/log/run
        name: birdmod-log-run
        subPath: run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-42w8p
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - /opt/cni/bin/calico-ipam
      - -upgrade
      env:
      - name: KUBERNETES_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CALICO_NETWORKING_BACKEND
        valueFrom:
          configMapKeyRef:
            key: calico_backend
            name: calico-config
      - name: KUBERNETES_SERVICE_HOST
        value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      envFrom:
      - configMapRef:
          name: kubernetes-services-endpoint
          optional: true
      image: docker.io/calico/cni:v3.24.5
      imagePullPolicy: IfNotPresent
      name: upgrade-ipam
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/cni/networks
        name: host-local-net-dir
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-42w8p
        readOnly: true
    - command:
      - /opt/cni/bin/install
      env:
      - name: CNI_CONF_NAME
        value: 10-calico.conflist
      - name: CNI_NETWORK_CONFIG
        valueFrom:
          configMapKeyRef:
            key: cni_network_config
            name: calico-config
      - name: KUBERNETES_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CNI_MTU
        valueFrom:
          configMapKeyRef:
            key: veth_mtu
            name: calico-config
      - name: SLEEP
        value: "false"
      - name: KUBERNETES_SERVICE_HOST
        value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      envFrom:
      - configMapRef:
          name: kubernetes-services-endpoint
          optional: true
      image: docker.io/calico/cni:v3.24.5
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-42w8p
        readOnly: true
    - env:
      - name: KUBERNETES_SERVICE_HOST
        value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/calico/pod2daemon-flexvol:v3.24.5
      imagePullPolicy: IfNotPresent
      name: flexvol-driver
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/driver
        name: flexvol-driver-host
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-42w8p
        readOnly: true
    nodeName: lke96996-146211-640977a3930b
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-node
    serviceAccountName: calico-node
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /var/run/calico
        type: ""
      name: var-run-calico
    - hostPath:
        path: /var/lib/calico
        type: ""
      name: var-lib-calico
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /sys/fs/
        type: DirectoryOrCreate
      name: sysfs
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-bin-dir
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-net-dir
    - hostPath:
        path: /var/log/calico/cni
        type: ""
      name: cni-log-dir
    - hostPath:
        path: /var/lib/cni/networks
        type: ""
      name: host-local-net-dir
    - hostPath:
        path: /var/run/nodeagent
        type: DirectoryOrCreate
      name: policysync
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
        type: DirectoryOrCreate
      name: flexvol-driver-host
    - configMap:
        defaultMode: 493
        items:
        - key: birdmod-run
          path: run
        name: calico-birdmod
      name: birdmod-run
    - configMap:
        defaultMode: 493
        items:
        - key: log-run
          path: run
        name: calico-birdmod
      name: birdmod-log-run
    - name: kube-api-access-42w8p
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:03Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:13Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:13Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:09:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://0f3e7cbf5e60e3cd85c9f95a73a796caae81f961c010bfec5ef2d3f17c8fbc3e
      image: docker.io/calico/node:v3.24.5
      imageID: docker.io/calico/node@sha256:5972ad2bcbdc668564d3e26960c9c513b2d7b05581c704747cf7c62ef3a405a6
      lastState: {}
      name: calico-node
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-09T06:10:08Z"
    hostIP: 192.168.129.114
    initContainerStatuses:
    - containerID: containerd://c3dd34b64fe6aba5b661acae3f835bbc825fcd4b4a28958f4a25a2c02accc782
      image: docker.io/calico/cni:v3.24.5
      imageID: docker.io/calico/cni@sha256:e282ea2914c806b5de2976330a17cfb5e6dcef47147bceb1432ca5c75fd46f50
      lastState: {}
      name: upgrade-ipam
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://c3dd34b64fe6aba5b661acae3f835bbc825fcd4b4a28958f4a25a2c02accc782
          exitCode: 0
          finishedAt: "2023-03-09T06:09:51Z"
          reason: Completed
          startedAt: "2023-03-09T06:09:51Z"
    - containerID: containerd://aac3c1decbca73a14f170edaeb30589e398499e4029a3946ed758a79343af9c2
      image: docker.io/calico/cni:v3.24.5
      imageID: docker.io/calico/cni@sha256:e282ea2914c806b5de2976330a17cfb5e6dcef47147bceb1432ca5c75fd46f50
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://aac3c1decbca73a14f170edaeb30589e398499e4029a3946ed758a79343af9c2
          exitCode: 0
          finishedAt: "2023-03-09T06:09:55Z"
          reason: Completed
          startedAt: "2023-03-09T06:09:51Z"
    - containerID: containerd://63ae5653444c3560a00add7e8369270e0937b63144e9edef1bb642c666a1c426
      image: docker.io/calico/pod2daemon-flexvol:v3.24.5
      imageID: docker.io/calico/pod2daemon-flexvol@sha256:392e392d0e11388bf55571155f915b1e8d080cb6824a7a09381537ad2f9b3c83
      lastState: {}
      name: flexvol-driver
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://63ae5653444c3560a00add7e8369270e0937b63144e9edef1bb642c666a1c426
          exitCode: 0
          finishedAt: "2023-03-09T06:10:02Z"
          reason: Completed
          startedAt: "2023-03-09T06:10:02Z"
    phase: Running
    podIP: 192.168.129.114
    podIPs:
    - ip: 192.168.129.114
    qosClass: Burstable
    startTime: "2023-03-09T06:09:43Z"
- metadata:
    creationTimestamp: "2023-03-09T06:10:09Z"
    generateName: calico-node-
    labels:
      controller-revision-hash: 5b565554fb
      k8s-app: calico-node
      pod-template-generation: "1"
    name: calico-node-hzjbq
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: calico-node
      uid: fc991107-9c7b-4a92-9345-fe92f48e066d
    resourceVersion: "883"
    uid: 60639a77-396a-46f8-9f4a-e8fb338028c1
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - lke96996-146211-640977a3e82e
    containers:
    - env:
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: WAIT_FOR_DATASTORE
        value: "true"
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CALICO_NETWORKING_BACKEND
        valueFrom:
          configMapKeyRef:
            key: calico_backend
            name: calico-config
      - name: CLUSTER_TYPE
        value: k8s,bgp
      - name: IP
        value: autodetect
      - name: IP_AUTODETECTION_METHOD
        value: can-reach=192.168.128.1
      - name: CALICO_IPV4POOL_IPIP
        value: Always
      - name: CALICO_IPV4POOL_VXLAN
        value: Never
      - name: FELIX_IPINIPMTU
        valueFrom:
          configMapKeyRef:
            key: veth_mtu
            name: calico-config
      - name: FELIX_VXLANMTU
        valueFrom:
          configMapKeyRef:
            key: veth_mtu
            name: calico-config
      - name: FELIX_WIREGUARDMTU
        valueFrom:
          configMapKeyRef:
            key: veth_mtu
            name: calico-config
      - name: CALICO_IPV4POOL_CIDR
        value: 10.2.0.0/16
      - name: USE_POD_CIDR
        value: "true"
      - name: CALICO_DISABLE_FILE_LOGGING
        value: "true"
      - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
        value: ACCEPT
      - name: FELIX_IPV6SUPPORT
        value: "false"
      - name: FELIX_HEALTHENABLED
        value: "true"
      - name: KUBERNETES_SERVICE_HOST
        value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      envFrom:
      - configMapRef:
          name: kubernetes-services-endpoint
          optional: true
      image: docker.io/calico/node:v3.24.5
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/calico-node
            - -shutdown
      livenessProbe:
        exec:
          command:
          - /bin/calico-node
          - -felix-live
          - -bird-live
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      name: calico-node
      readinessProbe:
        exec:
          command:
          - /bin/calico-node
          - -felix-ready
          - -bird-ready
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      resources:
        requests:
          cpu: 250m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/lib/calico
        name: var-lib-calico
      - mountPath: /var/run/nodeagent
        name: policysync
      - mountPath: /sys/fs/
        mountPropagation: Bidirectional
        name: sysfs
      - mountPath: /var/log/calico/cni
        name: cni-log-dir
        readOnly: true
      - mountPath: /etc/service/enabled/birdmod/run
        name: birdmod-run
        subPath: run
      - mountPath: /etc/service/enabled/birdmod/log/run
        name: birdmod-log-run
        subPath: run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dkb2h
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - /opt/cni/bin/calico-ipam
      - -upgrade
      env:
      - name: KUBERNETES_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CALICO_NETWORKING_BACKEND
        valueFrom:
          configMapKeyRef:
            key: calico_backend
            name: calico-config
      - name: KUBERNETES_SERVICE_HOST
        value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      envFrom:
      - configMapRef:
          name: kubernetes-services-endpoint
          optional: true
      image: docker.io/calico/cni:v3.24.5
      imagePullPolicy: IfNotPresent
      name: upgrade-ipam
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/cni/networks
        name: host-local-net-dir
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dkb2h
        readOnly: true
    - command:
      - /opt/cni/bin/install
      env:
      - name: CNI_CONF_NAME
        value: 10-calico.conflist
      - name: CNI_NETWORK_CONFIG
        valueFrom:
          configMapKeyRef:
            key: cni_network_config
            name: calico-config
      - name: KUBERNETES_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CNI_MTU
        valueFrom:
          configMapKeyRef:
            key: veth_mtu
            name: calico-config
      - name: SLEEP
        value: "false"
      - name: KUBERNETES_SERVICE_HOST
        value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      envFrom:
      - configMapRef:
          name: kubernetes-services-endpoint
          optional: true
      image: docker.io/calico/cni:v3.24.5
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dkb2h
        readOnly: true
    - env:
      - name: KUBERNETES_SERVICE_HOST
        value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: docker.io/calico/pod2daemon-flexvol:v3.24.5
      imagePullPolicy: IfNotPresent
      name: flexvol-driver
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/driver
        name: flexvol-driver-host
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dkb2h
        readOnly: true
    nodeName: lke96996-146211-640977a3e82e
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-node
    serviceAccountName: calico-node
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /var/run/calico
        type: ""
      name: var-run-calico
    - hostPath:
        path: /var/lib/calico
        type: ""
      name: var-lib-calico
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /sys/fs/
        type: DirectoryOrCreate
      name: sysfs
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-bin-dir
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-net-dir
    - hostPath:
        path: /var/log/calico/cni
        type: ""
      name: cni-log-dir
    - hostPath:
        path: /var/lib/cni/networks
        type: ""
      name: host-local-net-dir
    - hostPath:
        path: /var/run/nodeagent
        type: DirectoryOrCreate
      name: policysync
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
        type: DirectoryOrCreate
      name: flexvol-driver-host
    - configMap:
        defaultMode: 493
        items:
        - key: birdmod-run
          path: run
        name: calico-birdmod
      name: birdmod-run
    - configMap:
        defaultMode: 493
        items:
        - key: log-run
          path: run
        name: calico-birdmod
      name: birdmod-log-run
    - name: kube-api-access-dkb2h
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:47Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:47Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:09Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://74e83ed9b465adaa02bce1ccc877f2dc2236a6be52b17fd38483034cc66c4784
      image: docker.io/calico/node:v3.24.5
      imageID: docker.io/calico/node@sha256:5972ad2bcbdc668564d3e26960c9c513b2d7b05581c704747cf7c62ef3a405a6
      lastState: {}
      name: calico-node
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-09T06:10:37Z"
    hostIP: 192.168.173.115
    initContainerStatuses:
    - containerID: containerd://60f112bfed9a5e1c60bdc28e7a98b17f41668332298cc09340c87a0f2e63fcc5
      image: docker.io/calico/cni:v3.24.5
      imageID: docker.io/calico/cni@sha256:e282ea2914c806b5de2976330a17cfb5e6dcef47147bceb1432ca5c75fd46f50
      lastState: {}
      name: upgrade-ipam
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://60f112bfed9a5e1c60bdc28e7a98b17f41668332298cc09340c87a0f2e63fcc5
          exitCode: 0
          finishedAt: "2023-03-09T06:10:19Z"
          reason: Completed
          startedAt: "2023-03-09T06:10:19Z"
    - containerID: containerd://cb96b71935645c22039e122218258f12c05cff85a10d184c083cd1a2f79c799d
      image: docker.io/calico/cni:v3.24.5
      imageID: docker.io/calico/cni@sha256:e282ea2914c806b5de2976330a17cfb5e6dcef47147bceb1432ca5c75fd46f50
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://cb96b71935645c22039e122218258f12c05cff85a10d184c083cd1a2f79c799d
          exitCode: 0
          finishedAt: "2023-03-09T06:10:22Z"
          reason: Completed
          startedAt: "2023-03-09T06:10:20Z"
    - containerID: containerd://c6dd1976b82d94b2c6c97179359259f0d9de16cad58e980f99a14320ec195b30
      image: docker.io/calico/pod2daemon-flexvol:v3.24.5
      imageID: docker.io/calico/pod2daemon-flexvol@sha256:392e392d0e11388bf55571155f915b1e8d080cb6824a7a09381537ad2f9b3c83
      lastState: {}
      name: flexvol-driver
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://c6dd1976b82d94b2c6c97179359259f0d9de16cad58e980f99a14320ec195b30
          exitCode: 0
          finishedAt: "2023-03-09T06:10:30Z"
          reason: Completed
          startedAt: "2023-03-09T06:10:30Z"
    phase: Running
    podIP: 192.168.173.115
    podIPs:
    - ip: 192.168.173.115
    qosClass: Burstable
    startTime: "2023-03-09T06:10:12Z"
- metadata:
    annotations:
      cni.projectcalico.org/containerID: 0264467d12ee93b1fe9fc00096d6a6c16454aa22794b0029305be5fc0f7de896
      cni.projectcalico.org/podIP: 10.2.0.4/32
      cni.projectcalico.org/podIPs: 10.2.0.4/32
    creationTimestamp: "2023-03-09T06:09:11Z"
    generateName: coredns-5c64b647bf-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 5c64b647bf
    name: coredns-5c64b647bf-gkmxq
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-5c64b647bf
      uid: a61e81aa-9173-4c5e-861d-68f16dc7471c
    resourceVersion: "779"
    uid: 2fbf1e9f-e5fc-42bd-83a0-f0523cc4f760
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: linode/coredns:1.9.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bff8q
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: lke96996-146211-640977a3930b
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - name: kube-api-access-bff8q
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:09:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:20Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:20Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:09:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2f4fdd4dc9749e81797ec299117ac864fb47fa4c869bd8cd7a06275caa07c291
      image: docker.io/linode/coredns:1.9.3
      imageID: docker.io/linode/coredns@sha256:bdb36ee882c13135669cfc2bb91c808a33926ad1a411fee07bd2dc344bb8f782
      lastState: {}
      name: coredns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-09T06:10:19Z"
    hostIP: 192.168.129.114
    phase: Running
    podIP: 10.2.0.4
    podIPs:
    - ip: 10.2.0.4
    qosClass: Burstable
    startTime: "2023-03-09T06:09:43Z"
- metadata:
    annotations:
      cni.projectcalico.org/containerID: 7b32f3b9677606e7279c83ca6a4127fc2ffaf01769052de7e7c06b416643d4e0
      cni.projectcalico.org/podIP: 10.2.0.3/32
      cni.projectcalico.org/podIPs: 10.2.0.3/32
    creationTimestamp: "2023-03-09T06:09:11Z"
    generateName: coredns-5c64b647bf-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 5c64b647bf
    name: coredns-5c64b647bf-kxh68
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-5c64b647bf
      uid: a61e81aa-9173-4c5e-861d-68f16dc7471c
    resourceVersion: "783"
    uid: 339e0bc9-99d7-4ffb-a6ea-eafd28060908
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: linode/coredns:1.9.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7m4q6
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: lke96996-146211-640977a3930b
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - name: kube-api-access-7m4q6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:09:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:20Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:20Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:09:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9131e8ee18be360d7ee13e6c297b54c75aea6b4b73afb4d72448022a72221818
      image: docker.io/linode/coredns:1.9.3
      imageID: docker.io/linode/coredns@sha256:bdb36ee882c13135669cfc2bb91c808a33926ad1a411fee07bd2dc344bb8f782
      lastState: {}
      name: coredns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-09T06:10:19Z"
    hostIP: 192.168.129.114
    phase: Running
    podIP: 10.2.0.3
    podIPs:
    - ip: 10.2.0.3
    qosClass: Burstable
    startTime: "2023-03-09T06:09:43Z"
- metadata:
    annotations:
      cni.projectcalico.org/containerID: 128fb002bca33a3eabd41200312de40628f7d3be06c1889a13b143d0a5133d45
      cni.projectcalico.org/podIP: 10.2.0.5/32
      cni.projectcalico.org/podIPs: 10.2.0.5/32
    creationTimestamp: "2023-03-09T06:09:10Z"
    generateName: csi-linode-controller-
    labels:
      app: csi-linode-controller
      controller-revision-hash: csi-linode-controller-7dd6db964f
      role: csi-linode
      statefulset.kubernetes.io/pod-name: csi-linode-controller-0
    name: csi-linode-controller-0
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: csi-linode-controller
      uid: 862d2eb3-2b1e-4145-b471-af89ddd2b4a9
    resourceVersion: "834"
    uid: a87bb0d5-02ce-4e1b-8d70-b43d82afa98e
  spec:
    containers:
    - args:
      - --volume-name-prefix=pvc
      - --volume-name-uuid-length=16
      - --csi-address=$(ADDRESS)
      - --default-fstype=ext4
      - --v=2
      env:
      - name: ADDRESS
        value: /var/lib/csi/sockets/pluginproxy/csi.sock
      - name: KUBERNETES_SERVICE_HOST
        value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: linode/csi-provisioner:v3.0.0
      imagePullPolicy: IfNotPresent
      name: csi-provisioner
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/csi/sockets/pluginproxy/
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-g6pt8
        readOnly: true
    - args:
      - --v=2
      - --csi-address=$(ADDRESS)
      env:
      - name: ADDRESS
        value: /var/lib/csi/sockets/pluginproxy/csi.sock
      - name: KUBERNETES_SERVICE_HOST
        value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: linode/csi-attacher:v3.3.0
      imagePullPolicy: IfNotPresent
      name: csi-attacher
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/csi/sockets/pluginproxy/
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-g6pt8
        readOnly: true
    - args:
      - --v=2
      - --csi-address=$(ADDRESS)
      env:
      - name: ADDRESS
        value: /var/lib/csi/sockets/pluginproxy/csi.sock
      - name: KUBERNETES_SERVICE_HOST
        value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: linode/csi-resizer:v1.3.0
      imagePullPolicy: IfNotPresent
      name: csi-resizer
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/csi/sockets/pluginproxy/
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-g6pt8
        readOnly: true
    - args:
      - --endpoint=$(CSI_ENDPOINT)
      - --token=$(LINODE_TOKEN)
      - --url=$(LINODE_API_URL)
      - --node=$(NODE_NAME)
      - --bs-prefix=$(LINODE_BS_PREFIX)
      - --v=2
      env:
      - name: CSI_ENDPOINT
        value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock
      - name: LINODE_API_URL
        valueFrom:
          secretKeyRef:
            key: apiurl
            name: linode
      - name: LINODE_BS_PREFIX
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: LINODE_TOKEN
        valueFrom:
          secretKeyRef:
            key: token
            name: linode
      - name: KUBERNETES_SERVICE_HOST
        value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      - name: LINODE_URL
        valueFrom:
          secretKeyRef:
            key: apiurl
            name: linode
      image: linode/linode-blockstorage-csi-driver:v0.5.0
      imagePullPolicy: IfNotPresent
      name: linode-csi-plugin
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /linode-info
        name: linode-info
      - mountPath: /scripts
        name: get-linode-id
      - mountPath: /var/lib/csi/sockets/pluginproxy/
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-g6pt8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: csi-linode-controller-0
    initContainers:
    - command:
      - /scripts/get-linode-id.sh
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: bitnami/kubectl:1.16.3-debian-10-r36
      imagePullPolicy: IfNotPresent
      name: init
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /linode-info
        name: linode-info
      - mountPath: /scripts
        name: get-linode-id
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-g6pt8
        readOnly: true
    nodeName: lke96996-146211-640977a3930b
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: csi-controller-sa
    serviceAccountName: csi-controller-sa
    subdomain: csi-linode
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      operator: Exists
    volumes:
    - emptyDir: {}
      name: socket-dir
    - emptyDir: {}
      name: linode-info
    - configMap:
        defaultMode: 493
        name: get-linode-id
      name: get-linode-id
    - name: kube-api-access-g6pt8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:17Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:30Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:30Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:09:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4e9403a1642f5979299ab488c5f46dd8c2cbf7e3e309de8f56cd7b0b281983cb
      image: docker.io/linode/csi-attacher:v3.3.0
      imageID: docker.io/linode/csi-attacher@sha256:221c1c6930fb1cb93b57762a74ccb59194c4c74a63c0fd49309d1158d4f8c72c
      lastState: {}
      name: csi-attacher
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-09T06:10:24Z"
    - containerID: containerd://8352f81f05fe4e7a75da607a598d635ac933e1781b2f4df65054a7c4770460b4
      image: docker.io/linode/csi-provisioner:v3.0.0
      imageID: docker.io/linode/csi-provisioner@sha256:bbae7cde811054f6a51060ba7a42d8bf2469b8c574abb50fec8b46c13e32541e
      lastState: {}
      name: csi-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-09T06:10:22Z"
    - containerID: containerd://8df857373f1ce70064fc80914910cd64d97068053839f39e8ee32d4ecad7e404
      image: docker.io/linode/csi-resizer:v1.3.0
      imageID: docker.io/linode/csi-resizer@sha256:d2d2e429a0a87190ee73462698a02a08e555055246ad87ad979b464b999fedae
      lastState: {}
      name: csi-resizer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-09T06:10:27Z"
    - containerID: containerd://4dd7f2d9ab668524a1e14995dcffb2a107415e3ff1e5a33d31c65dce529994c6
      image: docker.io/linode/linode-blockstorage-csi-driver:v0.5.0
      imageID: docker.io/linode/linode-blockstorage-csi-driver@sha256:6a1ecbfc761a5e3adabedb091d84fdb76d73d89e95af1592c0c97baad2042340
      lastState: {}
      name: linode-csi-plugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-09T06:10:29Z"
    hostIP: 192.168.129.114
    initContainerStatuses:
    - containerID: containerd://d7218cc2381390c35d551e4e2a7679fbc9960bf0af15d72ece6325ebc31bda69
      image: docker.io/bitnami/kubectl:1.16.3-debian-10-r36
      imageID: docker.io/bitnami/kubectl@sha256:c4a8d9c0cd9c5f903830ea64816c83adf307ff1d775bc3e5b77f1d49d3960205
      lastState: {}
      name: init
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://d7218cc2381390c35d551e4e2a7679fbc9960bf0af15d72ece6325ebc31bda69
          exitCode: 0
          finishedAt: "2023-03-09T06:10:17Z"
          reason: Completed
          startedAt: "2023-03-09T06:10:16Z"
    phase: Running
    podIP: 10.2.0.5
    podIPs:
    - ip: 10.2.0.5
    qosClass: BestEffort
    startTime: "2023-03-09T06:09:43Z"
- metadata:
    creationTimestamp: "2023-03-09T06:09:40Z"
    generateName: csi-linode-node-
    labels:
      app: csi-linode-node
      controller-revision-hash: 5bdfdd8fc4
      pod-template-generation: "1"
      role: csi-linode
    name: csi-linode-node-bwnv6
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-linode-node
      uid: 9e8b9ac5-0179-4287-9a6c-52d5dd82be12
    resourceVersion: "849"
    uid: 69708164-419b-46a4-8b6c-7aeb9eb17247
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - lke96996-146211-640977a3930b
    containers:
    - args:
      - --v=2
      - --csi-address=$(ADDRESS)
      - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      - name: DRIVER_REG_SOCK_PATH
        value: /var/lib/kubelet/plugins/linodebs.csi.linode.com/csi.sock
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: KUBERNETES_SERVICE_HOST
        value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: linode/csi-node-driver-registrar:v1.3.0
      imagePullPolicy: IfNotPresent
      name: csi-node-driver-registrar
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-q2xjf
        readOnly: true
    - args:
      - --endpoint=$(CSI_ENDPOINT)
      - --token=$(LINODE_TOKEN)
      - --url=$(LINODE_API_URL)
      - --node=$(NODE_NAME)
      - --v=2
      env:
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      - name: LINODE_API_URL
        valueFrom:
          secretKeyRef:
            key: apiurl
            name: linode
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: KUBERNETES_SERVICE_HOST
        value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      - name: LINODE_URL
        valueFrom:
          secretKeyRef:
            key: apiurl
            name: linode
      - name: LINODE_TOKEN
        valueFrom:
          secretKeyRef:
            key: token
            name: linode
      image: linode/linode-blockstorage-csi-driver:v0.5.0
      imagePullPolicy: IfNotPresent
      name: csi-linode-plugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /linode-info
        name: linode-info
      - mountPath: /scripts
        name: get-linode-id
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /dev
        name: device-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-q2xjf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - /scripts/get-linode-id.sh
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: bitnami/kubectl:1.16.3-debian-10-r36
      imagePullPolicy: IfNotPresent
      name: init
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /linode-info
        name: linode-info
      - mountPath: /scripts
        name: get-linode-id
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-q2xjf
        readOnly: true
    nodeName: lke96996-146211-640977a3930b
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: csi-node-sa
    serviceAccountName: csi-node-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - emptyDir: {}
      name: linode-info
    - configMap:
        defaultMode: 493
        name: get-linode-id
      name: get-linode-id
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: DirectoryOrCreate
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: kubelet-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/linodebs.csi.linode.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: device-dir
    - hostPath:
        path: /etc/udev
        type: Directory
      name: udev-rules-etc
    - hostPath:
        path: /lib/udev
        type: Directory
      name: udev-rules-lib
    - hostPath:
        path: /run/udev
        type: Directory
      name: udev-socket
    - hostPath:
        path: /sys
        type: Directory
      name: sys
    - name: kube-api-access-q2xjf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:32Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:32Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:09:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9f8e72f633c231c27262541cb5137b34325d34edd3b84c556d3743b944b442ba
      image: docker.io/linode/linode-blockstorage-csi-driver:v0.5.0
      imageID: docker.io/linode/linode-blockstorage-csi-driver@sha256:6a1ecbfc761a5e3adabedb091d84fdb76d73d89e95af1592c0c97baad2042340
      lastState: {}
      name: csi-linode-plugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-09T06:10:31Z"
    - containerID: containerd://a2419f95ec59bbdc7e6c950dccc4e84d2b19e71981d019a9b5308b17d997cc0a
      image: docker.io/linode/csi-node-driver-registrar:v1.3.0
      imageID: docker.io/linode/csi-node-driver-registrar@sha256:9622c6a6dac7499a055a382930f4de82905a3c5735c0753f7094115c9c871309
      lastState: {}
      name: csi-node-driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-09T06:10:31Z"
    hostIP: 192.168.129.114
    initContainerStatuses:
    - containerID: containerd://31fe229dbcd75626669d351366f186ddb1ff1e526f63a9e461c38378e704a94e
      image: docker.io/bitnami/kubectl:1.16.3-debian-10-r36
      imageID: docker.io/bitnami/kubectl@sha256:c4a8d9c0cd9c5f903830ea64816c83adf307ff1d775bc3e5b77f1d49d3960205
      lastState: {}
      name: init
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://31fe229dbcd75626669d351366f186ddb1ff1e526f63a9e461c38378e704a94e
          exitCode: 0
          finishedAt: "2023-03-09T06:10:27Z"
          reason: Completed
          startedAt: "2023-03-09T06:09:57Z"
    phase: Running
    podIP: 192.168.129.114
    podIPs:
    - ip: 192.168.129.114
    qosClass: BestEffort
    startTime: "2023-03-09T06:09:43Z"
- metadata:
    creationTimestamp: "2023-03-09T06:10:09Z"
    generateName: csi-linode-node-
    labels:
      app: csi-linode-node
      controller-revision-hash: 5bdfdd8fc4
      pod-template-generation: "1"
      role: csi-linode
    name: csi-linode-node-pttdn
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-linode-node
      uid: 9e8b9ac5-0179-4287-9a6c-52d5dd82be12
    resourceVersion: "873"
    uid: 8b9785df-ef7d-4aab-8eed-912a78bc3eef
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - lke96996-146211-640977a3e82e
    containers:
    - args:
      - --v=2
      - --csi-address=$(ADDRESS)
      - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      - name: DRIVER_REG_SOCK_PATH
        value: /var/lib/kubelet/plugins/linodebs.csi.linode.com/csi.sock
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: KUBERNETES_SERVICE_HOST
        value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: linode/csi-node-driver-registrar:v1.3.0
      imagePullPolicy: IfNotPresent
      name: csi-node-driver-registrar
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zp4nf
        readOnly: true
    - args:
      - --endpoint=$(CSI_ENDPOINT)
      - --token=$(LINODE_TOKEN)
      - --url=$(LINODE_API_URL)
      - --node=$(NODE_NAME)
      - --v=2
      env:
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      - name: LINODE_API_URL
        valueFrom:
          secretKeyRef:
            key: apiurl
            name: linode
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: KUBERNETES_SERVICE_HOST
        value: 24c158da-629c-406d-8124-e364415bdd98.eu-west-1.linodelke.net
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      - name: LINODE_URL
        valueFrom:
          secretKeyRef:
            key: apiurl
            name: linode
      - name: LINODE_TOKEN
        valueFrom:
          secretKeyRef:
            key: token
            name: linode
      image: linode/linode-blockstorage-csi-driver:v0.5.0
      imagePullPolicy: IfNotPresent
      name: csi-linode-plugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /linode-info
        name: linode-info
      - mountPath: /scripts
        name: get-linode-id
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /dev
        name: device-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zp4nf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - /scripts/get-linode-id.sh
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: bitnami/kubectl:1.16.3-debian-10-r36
      imagePullPolicy: IfNotPresent
      name: init
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /linode-info
        name: linode-info
      - mountPath: /scripts
        name: get-linode-id
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zp4nf
        readOnly: true
    nodeName: lke96996-146211-640977a3e82e
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: csi-node-sa
    serviceAccountName: csi-node-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - emptyDir: {}
      name: linode-info
    - configMap:
        defaultMode: 493
        name: get-linode-id
      name: get-linode-id
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: DirectoryOrCreate
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: kubelet-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/linodebs.csi.linode.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: device-dir
    - hostPath:
        path: /etc/udev
        type: Directory
      name: udev-rules-etc
    - hostPath:
        path: /lib/udev
        type: Directory
      name: udev-rules-lib
    - hostPath:
        path: /run/udev
        type: Directory
      name: udev-socket
    - hostPath:
        path: /sys
        type: Directory
      name: sys
    - name: kube-api-access-zp4nf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:39Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:39Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:09Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://adb23b7f05a193be0591bca8c28d68efec252a9e2c993beb584585f96d99c245
      image: docker.io/linode/linode-blockstorage-csi-driver:v0.5.0
      imageID: docker.io/linode/linode-blockstorage-csi-driver@sha256:6a1ecbfc761a5e3adabedb091d84fdb76d73d89e95af1592c0c97baad2042340
      lastState: {}
      name: csi-linode-plugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-09T06:10:39Z"
    - containerID: containerd://1ef4e8eca1bfb75dce975b9e313e8bc380d5b42b7ee0d225bbf658569c7238e0
      image: docker.io/linode/csi-node-driver-registrar:v1.3.0
      imageID: docker.io/linode/csi-node-driver-registrar@sha256:9622c6a6dac7499a055a382930f4de82905a3c5735c0753f7094115c9c871309
      lastState: {}
      name: csi-node-driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-09T06:10:32Z"
    hostIP: 192.168.173.115
    initContainerStatuses:
    - containerID: containerd://65095cc0d34be7e9d310a8c7bc23561b18db8036972ea1feff70d1ffd67662e7
      image: docker.io/bitnami/kubectl:1.16.3-debian-10-r36
      imageID: docker.io/bitnami/kubectl@sha256:c4a8d9c0cd9c5f903830ea64816c83adf307ff1d775bc3e5b77f1d49d3960205
      lastState: {}
      name: init
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://65095cc0d34be7e9d310a8c7bc23561b18db8036972ea1feff70d1ffd67662e7
          exitCode: 0
          finishedAt: "2023-03-09T06:10:27Z"
          reason: Completed
          startedAt: "2023-03-09T06:10:27Z"
    phase: Running
    podIP: 192.168.173.115
    podIPs:
    - ip: 192.168.173.115
    qosClass: BestEffort
    startTime: "2023-03-09T06:10:12Z"
- metadata:
    creationTimestamp: "2023-03-09T06:10:09Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 6497b4f59b
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-dz8bx
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: 7317d7c0-81a6-4746-a30e-1c47bf5d88f7
    resourceVersion: "809"
    uid: 77d6af8f-070b-44e9-a3ac-607d2e2208fd
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - lke96996-146211-640977a3e82e
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: linode/kube-proxy-amd64:v1.25.6
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bmbd7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: lke96996-146211-640977a3e82e
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-bmbd7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:09Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://02fbed4d72e4cefa8ab35204a35a14d73e06a216d95e43773ef87f85e086fca1
      image: docker.io/linode/kube-proxy-amd64:v1.25.6
      imageID: docker.io/linode/kube-proxy-amd64@sha256:569f1d31ded5da821cb2dc2c4c741881ab0c75967f48e8dab5d0286d22f2793e
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-09T06:10:22Z"
    hostIP: 192.168.173.115
    phase: Running
    podIP: 192.168.173.115
    podIPs:
    - ip: 192.168.173.115
    qosClass: BestEffort
    startTime: "2023-03-09T06:10:12Z"
- metadata:
    creationTimestamp: "2023-03-09T06:09:40Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 6497b4f59b
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-n4rms
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: 7317d7c0-81a6-4746-a30e-1c47bf5d88f7
    resourceVersion: "661"
    uid: 74579826-daed-4442-b86e-aefe5744dda6
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - lke96996-146211-640977a3930b
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: linode/kube-proxy-amd64:v1.25.6
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8hkzn
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: lke96996-146211-640977a3930b
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-8hkzn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:09:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:01Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:10:01Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-09T06:09:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://115e6a373fc99efbec7694b3df4abe53f795d1594cfb0930cfca4be2f982c39a
      image: docker.io/linode/kube-proxy-amd64:v1.25.6
      imageID: docker.io/linode/kube-proxy-amd64@sha256:569f1d31ded5da821cb2dc2c4c741881ab0c75967f48e8dab5d0286d22f2793e
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-09T06:10:00Z"
    hostIP: 192.168.129.114
    phase: Running
    podIP: 192.168.129.114
    podIPs:
    - ip: 192.168.129.114
    qosClass: BestEffort
    startTime: "2023-03-09T06:09:43Z"
kind: PodList
metadata:
  resourceVersion: "1308"
==== START logs for container calico-kube-controllers of pod kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl ====
2023-03-09 06:10:16.841 [INFO][1] main.go 103: Loaded configuration from environment config=&config.Config{LogLevel:"info", WorkloadEndpointWorkers:1, ProfileWorkers:1, PolicyWorkers:1, NodeWorkers:1, Kubeconfig:"", DatastoreType:"kubernetes"}
W0309 06:10:16.849531       1 client_config.go:617] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
2023-03-09 06:10:16.849 [INFO][1] main.go 127: Ensuring Calico datastore is initialized
2023-03-09 06:10:21.940 [INFO][1] main.go 153: Calico datastore is initialized
2023-03-09 06:10:21.944 [INFO][1] main.go 190: Getting initial config snapshot from datastore
2023-03-09 06:10:21.986 [INFO][1] main.go 193: Got initial config snapshot
2023-03-09 06:10:21.987 [INFO][1] watchersyncer.go 89: Start called
2023-03-09 06:10:21.988 [INFO][1] main.go 207: Starting status report routine
2023-03-09 06:10:21.988 [INFO][1] main.go 216: Starting Prometheus metrics server on port 9094
2023-03-09 06:10:21.988 [INFO][1] main.go 495: Starting informer informer=&cache.sharedIndexInformer{indexer:(*cache.cache)(0xc0002e65a0), controller:cache.Controller(nil), processor:(*cache.sharedProcessor)(0xc00056c310), cacheMutationDetector:cache.dummyMutationDetector{}, listerWatcher:(*cache.ListWatch)(0xc0002e6588), objectType:(*v1.Pod)(0xc000302c00), resyncCheckPeriod:0, defaultEventHandlerResyncPeriod:0, clock:(*clock.RealClock)(0x3013200), started:false, stopped:false, startedLock:sync.Mutex{state:0, sema:0x0}, blockDeltas:sync.Mutex{state:0, sema:0x0}, watchErrorHandler:(cache.WatchErrorHandler)(nil), transform:(cache.TransformFunc)(nil)}
2023-03-09 06:10:21.988 [INFO][1] main.go 495: Starting informer informer=&cache.sharedIndexInformer{indexer:(*cache.cache)(0xc0002e7038), controller:cache.Controller(nil), processor:(*cache.sharedProcessor)(0xc00056c380), cacheMutationDetector:cache.dummyMutationDetector{}, listerWatcher:(*cache.ListWatch)(0xc0002e7020), objectType:(*v1.Node)(0xc000484300), resyncCheckPeriod:0, defaultEventHandlerResyncPeriod:0, clock:(*clock.RealClock)(0x3013200), started:false, stopped:false, startedLock:sync.Mutex{state:0, sema:0x0}, blockDeltas:sync.Mutex{state:0, sema:0x0}, watchErrorHandler:(cache.WatchErrorHandler)(nil), transform:(cache.TransformFunc)(nil)}
2023-03-09 06:10:21.988 [INFO][1] main.go 501: Starting controller ControllerType="Node"
2023-03-09 06:10:21.988 [INFO][1] controller.go 193: Starting Node controller
I0309 06:10:21.989141       1 shared_informer.go:255] Waiting for caches to sync for nodes
2023-03-09 06:10:21.989 [INFO][1] watchersyncer.go 130: Sending status update Status=wait-for-ready
2023-03-09 06:10:21.989 [INFO][1] syncer.go 86: Node controller syncer status updated: wait-for-ready
2023-03-09 06:10:21.990 [INFO][1] watchersyncer.go 149: Starting main event processing loop
2023-03-09 06:10:21.990 [INFO][1] watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/ippools"
2023-03-09 06:10:21.993 [INFO][1] watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/nodes"
2023-03-09 06:10:21.996 [INFO][1] watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/clusterinformations"
2023-03-09 06:10:21.998 [INFO][1] watchercache.go 181: Full resync is required ListRoot="/calico/ipam/v2/assignment/"
2023-03-09 06:10:22.004 [INFO][1] resources.go 350: Main client watcher loop
2023-03-09 06:10:22.006 [INFO][1] watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/ippools"
2023-03-09 06:10:22.009 [INFO][1] watchercache.go 294: Sending synced update ListRoot="/calico/ipam/v2/assignment/"
2023-03-09 06:10:22.011 [INFO][1] watchersyncer.go 130: Sending status update Status=resync
2023-03-09 06:10:22.012 [INFO][1] syncer.go 86: Node controller syncer status updated: resync
2023-03-09 06:10:22.012 [INFO][1] watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:22.012 [INFO][1] watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:22.030 [INFO][1] watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/nodes"
2023-03-09 06:10:22.032 [INFO][1] watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/clusterinformations"
2023-03-09 06:10:22.032 [INFO][1] watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:22.033 [INFO][1] watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:22.033 [INFO][1] watchersyncer.go 221: All watchers have sync'd data - sending data and final sync
2023-03-09 06:10:22.033 [INFO][1] watchersyncer.go 130: Sending status update Status=in-sync
2023-03-09 06:10:22.033 [INFO][1] syncer.go 86: Node controller syncer status updated: in-sync
2023-03-09 06:10:22.059 [INFO][1] hostendpoints.go 173: successfully synced all hostendpoints
I0309 06:10:22.090035       1 shared_informer.go:262] Caches are synced for nodes
I0309 06:10:22.090049       1 shared_informer.go:255] Waiting for caches to sync for pods
I0309 06:10:22.090057       1 shared_informer.go:262] Caches are synced for pods
2023-03-09 06:10:22.090 [INFO][1] ipam.go 253: Will run periodic IPAM sync every 7m30s
2023-03-09 06:10:22.090 [INFO][1] ipam.go 331: Syncer is InSync, kicking sync channel status=in-sync
2023-03-09 06:11:16.296 [ERROR][1] client.go 290: Error getting cluster information config ClusterInformation="default" error=client rate limiter Wait returned an error: context deadline exceeded - error from a previous attempt: EOF
2023-03-09 06:11:16.296 [ERROR][1] main.go 275: Failed to verify datastore error=client rate limiter Wait returned an error: context deadline exceeded - error from a previous attempt: EOF
2023-03-09 06:11:20.317 [ERROR][1] main.go 293: Received bad status code from apiserver error=client rate limiter Wait returned an error: context deadline exceeded - error from a previous attempt: EOF status=0
2023-03-09 06:11:20.318 [INFO][1] main.go 309: Health check is not ready, retrying in 2 seconds with new timeout: 8s
2023-03-09 06:11:36.502 [INFO][1] resources.go 378: Terminating main client watcher loop
2023-03-09 06:11:36.537 [INFO][1] watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/clusterinformations" error=too old resource version: 679 (902)
2023-03-09 06:11:37.002 [INFO][1] watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/clusterinformations"
2023-03-09 06:11:37.462 [INFO][1] watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/ippools" error=too old resource version: 678 (904)
2023-03-09 06:11:37.463 [INFO][1] watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/ippools"
2023-03-09 06:11:38.643 [INFO][1] resources.go 350: Main client watcher loop
==== END logs for container calico-kube-controllers of pod kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl ====
==== START logs for container upgrade-ipam of pod kube-system/calico-node-28hph ====
2023-03-09 06:09:51.341 [INFO][1] ipam_plugin.go 80: migrating from host-local to calico-ipam...
2023-03-09 06:09:51.342 [INFO][1] migrate.go 67: checking host-local IPAM data dir dir existence...
2023-03-09 06:09:51.342 [INFO][1] migrate.go 69: host-local IPAM data dir dir not found; no migration necessary, successfully exiting...
2023-03-09 06:09:51.343 [INFO][1] ipam_plugin.go 110: migration from host-local to calico-ipam complete node="lke96996-146211-640977a3930b"
==== END logs for container upgrade-ipam of pod kube-system/calico-node-28hph ====
==== START logs for container install-cni of pod kube-system/calico-node-28hph ====
time="2023-03-09T06:09:51Z" level=info msg="Running as a Kubernetes pod" source="install.go:145"
2023-03-09 06:09:52.733 [INFO][1] cni-installer/<nil> <nil>: Installed /host/opt/cni/bin/bandwidth
2023-03-09 06:09:53.374 [INFO][1] cni-installer/<nil> <nil>: Installed /host/opt/cni/bin/calico
2023-03-09 06:09:54.272 [INFO][1] cni-installer/<nil> <nil>: Installed /host/opt/cni/bin/calico-ipam
2023-03-09 06:09:54.316 [INFO][1] cni-installer/<nil> <nil>: Installed /host/opt/cni/bin/flannel
2023-03-09 06:09:54.338 [INFO][1] cni-installer/<nil> <nil>: Installed /host/opt/cni/bin/host-local
2023-03-09 06:09:55.213 [INFO][1] cni-installer/<nil> <nil>: Installed /host/opt/cni/bin/install
2023-03-09 06:09:55.287 [INFO][1] cni-installer/<nil> <nil>: Installed /host/opt/cni/bin/loopback
2023-03-09 06:09:55.331 [INFO][1] cni-installer/<nil> <nil>: Installed /host/opt/cni/bin/portmap
2023-03-09 06:09:55.426 [INFO][1] cni-installer/<nil> <nil>: Installed /host/opt/cni/bin/tuning
2023-03-09 06:09:55.426 [INFO][1] cni-installer/<nil> <nil>: Wrote Calico CNI binaries to /host/opt/cni/bin

2023-03-09 06:09:55.595 [INFO][1] cni-installer/<nil> <nil>: CNI plugin version: v3.24.5

2023-03-09 06:09:55.595 [INFO][1] cni-installer/<nil> <nil>: /host/secondary-bin-dir is not writeable, skipping
W0309 06:09:55.595707       1 client_config.go:617] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
time="2023-03-09T06:09:55Z" level=info msg="Using CNI config template from CNI_NETWORK_CONFIG environment variable." source="install.go:336"
2023-03-09 06:09:55.624 [INFO][1] cni-installer/<nil> <nil>: Created /host/etc/cni/net.d/10-calico.conflist
2023-03-09 06:09:55.624 [INFO][1] cni-installer/<nil> <nil>: Done configuring CNI.  Sleep= false
{
  "name": "k8s-pod-network",
  "cniVersion": "0.3.1",
  "plugins": [
    {
      "type": "calico",
      "log_level": "info",
      "log_file_path": "/var/log/calico/cni/cni.log",
      "datastore_type": "kubernetes",
      "nodename": "lke96996-146211-640977a3930b",
      "mtu": 0,
      "ipam": {
          "type": "host-local",
          "subnet": "usePodCidr"
      },
      "policy": {
          "type": "k8s"
      },
      "kubernetes": {
          "kubeconfig": "/etc/cni/net.d/calico-kubeconfig"
      }
    },
    {
      "type": "portmap",
      "snat": true,
      "capabilities": {"portMappings": true}
    },
    {
      "type": "bandwidth",
      "capabilities": {"bandwidth": true}
    }
  ]
}
==== END logs for container install-cni of pod kube-system/calico-node-28hph ====
==== START logs for container flexvol-driver of pod kube-system/calico-node-28hph ====
==== END logs for container flexvol-driver of pod kube-system/calico-node-28hph ====
==== START logs for container calico-node of pod kube-system/calico-node-28hph ====
2023-03-09 06:10:08.483 [INFO][9] startup/startup.go 427: Early log level set to info
2023-03-09 06:10:08.483 [INFO][9] startup/utils.go 127: Using NODENAME environment for node name lke96996-146211-640977a3930b
2023-03-09 06:10:08.483 [INFO][9] startup/utils.go 139: Determined node name: lke96996-146211-640977a3930b
2023-03-09 06:10:08.483 [INFO][9] startup/startup.go 94: Starting node lke96996-146211-640977a3930b with version v3.24.5
2023-03-09 06:10:08.484 [INFO][9] startup/startup.go 432: Checking datastore connection
2023-03-09 06:10:08.513 [INFO][9] startup/startup.go 456: Datastore connection verified
2023-03-09 06:10:08.513 [INFO][9] startup/startup.go 104: Datastore is ready
2023-03-09 06:10:08.519 [INFO][9] startup/customresource.go 102: Error getting resource Key=GlobalFelixConfig(name=CalicoVersion) Name="calicoversion" Resource="GlobalFelixConfigs" error=the server could not find the requested resource (get GlobalFelixConfigs.crd.projectcalico.org calicoversion)
2023-03-09 06:10:08.541 [INFO][9] startup/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.129.114
2023-03-09 06:10:08.541 [INFO][9] startup/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.129.114/17, detected by connecting to 192.168.128.1
2023-03-09 06:10:08.541 [INFO][9] startup/startup.go 561: Node IPv4 changed, will check for conflicts
2023-03-09 06:10:08.546 [INFO][9] startup/startup.go 701: No AS number configured on node resource, using global value
2023-03-09 06:10:08.563 [INFO][9] startup/startup.go 750: found v6= in the kubeadm config map
2023-03-09 06:10:08.567 [INFO][9] startup/startup.go 682: CALICO_IPV4POOL_NAT_OUTGOING is true (defaulted) through environment variable
2023-03-09 06:10:08.567 [INFO][9] startup/startup.go 682: CALICO_IPV4POOL_DISABLE_BGP_EXPORT is false (defaulted) through environment variable
2023-03-09 06:10:08.567 [INFO][9] startup/startup.go 908: Ensure default IPv4 pool is created. IPIP mode: Always, VXLAN mode: Never, DisableBGPExport: false
2023-03-09 06:10:08.571 [INFO][9] startup/k8s.go 670: Attempt to 'List' using kubernetes backend is not supported.
2023-03-09 06:10:08.579 [INFO][9] startup/startup.go 918: Created default IPv4 pool (10.2.0.0/16) with NAT outgoing true. IPIP mode: Always, VXLAN mode: Never, DisableBGPExport: false
2023-03-09 06:10:08.579 [INFO][9] startup/startup.go 676: FELIX_IPV6SUPPORT is false through environment variable
2023-03-09 06:10:08.609 [INFO][9] startup/startup.go 218: Using node name: lke96996-146211-640977a3930b
2023-03-09 06:10:08.609 [INFO][9] startup/utils.go 191: Setting NetworkUnavailable to false
2023-03-09 06:10:08.660 [INFO][15] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "wireguardmtu"="0"
2023-03-09 06:10:08.660 [INFO][15] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "vxlanmtu"="0"
2023-03-09 06:10:08.660 [INFO][15] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "healthenabled"="true"
2023-03-09 06:10:08.660 [INFO][15] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "defaultendpointtohostaction"="ACCEPT"
2023-03-09 06:10:08.660 [INFO][15] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "ipinipmtu"="0"
2023-03-09 06:10:08.660 [INFO][15] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "ipv6support"="false"
2023-03-09 06:10:08.661 [INFO][15] tunnel-ip-allocator/config_params.go 435: Merging in config from environment variable: map[defaultendpointtohostaction:ACCEPT healthenabled:true ipinipmtu:0 ipv6support:false vxlanmtu:0 wireguardmtu:0]
2023-03-09 06:10:08.661 [INFO][15] tunnel-ip-allocator/config_params.go 542: Parsing value for VXLANMTU: 0 (from environment variable)
2023-03-09 06:10:08.661 [INFO][15] tunnel-ip-allocator/config_params.go 578: Parsed value for VXLANMTU: 0 (from environment variable)
2023-03-09 06:10:08.661 [INFO][15] tunnel-ip-allocator/config_params.go 542: Parsing value for HealthEnabled: true (from environment variable)
2023-03-09 06:10:08.661 [INFO][15] tunnel-ip-allocator/config_params.go 578: Parsed value for HealthEnabled: true (from environment variable)
2023-03-09 06:10:08.661 [INFO][15] tunnel-ip-allocator/config_params.go 542: Parsing value for DefaultEndpointToHostAction: ACCEPT (from environment variable)
2023-03-09 06:10:08.661 [INFO][15] tunnel-ip-allocator/config_params.go 578: Parsed value for DefaultEndpointToHostAction: ACCEPT (from environment variable)
2023-03-09 06:10:08.661 [INFO][15] tunnel-ip-allocator/config_params.go 542: Parsing value for IpInIpMtu: 0 (from environment variable)
2023-03-09 06:10:08.661 [INFO][15] tunnel-ip-allocator/config_params.go 578: Parsed value for IpInIpMtu: 0 (from environment variable)
2023-03-09 06:10:08.661 [INFO][15] tunnel-ip-allocator/config_params.go 542: Parsing value for Ipv6Support: false (from environment variable)
2023-03-09 06:10:08.661 [INFO][15] tunnel-ip-allocator/config_params.go 578: Parsed value for Ipv6Support: false (from environment variable)
2023-03-09 06:10:08.661 [INFO][15] tunnel-ip-allocator/config_params.go 542: Parsing value for WireguardMTU: 0 (from environment variable)
2023-03-09 06:10:08.662 [INFO][15] tunnel-ip-allocator/config_params.go 578: Parsed value for WireguardMTU: 0 (from environment variable)
mkdir: cannot create directory '/etc/service/enabled': File exists
Calico node started successfully
bird: bird: Unable to open configuration file /etc/calico/confd/config/bird.cfg: No such file or directory
Unable to open configuration file /etc/calico/confd/config/bird6.cfg: No such file or directory
W0309 06:10:10.002823      60 client_config.go:617] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
2023-03-09 06:10:10.004 [INFO][66] confd/config.go 82: Skipping confd config file.
2023-03-09 06:10:10.009 [INFO][66] confd/run.go 18: Starting calico-confd
2023-03-09 06:10:10.030 [INFO][67] status-reporter/startup.go 427: Early log level set to info
2023-03-09 06:10:10.032 [INFO][67] status-reporter/watchersyncer.go 89: Start called
W0309 06:10:10.033607      66 client_config.go:617] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
2023-03-09 06:10:10.033 [INFO][66] confd/client.go 1419: Advertise global service ranges from this node
2023-03-09 06:10:10.034 [INFO][66] confd/client.go 1364: Updated with new cluster IP CIDRs: []
2023-03-09 06:10:10.034 [INFO][66] confd/client.go 1419: Advertise global service ranges from this node
2023-03-09 06:10:10.034 [INFO][66] confd/client.go 1355: Updated with new external IP CIDRs: []
2023-03-09 06:10:10.034 [INFO][66] confd/client.go 1419: Advertise global service ranges from this node
2023-03-09 06:10:10.034 [INFO][66] confd/client.go 1374: Updated with new Loadbalancer IP CIDRs: []
2023-03-09 06:10:10.034 [INFO][66] confd/watchersyncer.go 89: Start called
2023-03-09 06:10:10.034 [INFO][66] confd/client.go 422: Source SourceRouteGenerator readiness changed, ready=true
2023-03-09 06:10:10.034 [INFO][66] confd/watchersyncer.go 130: Sending status update Status=wait-for-ready
2023-03-09 06:10:10.035 [INFO][66] confd/watchersyncer.go 149: Starting main event processing loop
2023-03-09 06:10:10.035 [INFO][66] confd/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/bgppeers"
2023-03-09 06:10:10.036 [INFO][66] confd/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/ippools"
2023-03-09 06:10:10.036 [INFO][66] confd/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/bgpconfigurations"
2023-03-09 06:10:10.036 [INFO][66] confd/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/nodes"
2023-03-09 06:10:10.037 [INFO][67] status-reporter/watchersyncer.go 130: Sending status update Status=wait-for-ready
2023-03-09 06:10:10.037 [INFO][67] status-reporter/watchersyncer.go 149: Starting main event processing loop
2023-03-09 06:10:10.037 [INFO][67] status-reporter/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/caliconodestatuses"
2023-03-09 06:10:10.049 [INFO][66] confd/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/bgppeers"
2023-03-09 06:10:10.050 [INFO][66] confd/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/ippools"
2023-03-09 06:10:10.050 [INFO][66] confd/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/bgpconfigurations"
2023-03-09 06:10:10.050 [INFO][66] confd/watchersyncer.go 130: Sending status update Status=resync
2023-03-09 06:10:10.056 [INFO][66] confd/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:10.056 [INFO][66] confd/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:10.056 [INFO][66] confd/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:10.056 [INFO][66] confd/client.go 995: Recompute BGP peerings: HostBGPConfig(node=lke96996-146211-640977a3e82e; name=ip_addr_v4) updated; HostBGPConfig(node=lke96996-146211-640977a3e82e; name=ip_addr_v6) updated; HostBGPConfig(node=lke96996-146211-640977a3e82e; name=rr_cluster_id) updated; BlockAffinityKey(cidr=10.2.1.0/24, host=lke96996-146211-640977a3e82e) updated; lke96996-146211-640977a3e82e updated; HostBGPConfig(node=lke96996-146211-640977a3930b; name=ip_addr_v4) updated; HostBGPConfig(node=lke96996-146211-640977a3930b; name=ip_addr_v6) updated; HostBGPConfig(node=lke96996-146211-640977a3930b; name=network_v4) updated; HostBGPConfig(node=lke96996-146211-640977a3930b; name=rr_cluster_id) updated; BlockAffinityKey(cidr=10.2.0.0/24, host=lke96996-146211-640977a3930b) updated; lke96996-146211-640977a3930b updated
2023-03-09 06:10:10.050 [INFO][66] confd/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/nodes"
2023-03-09 06:10:10.057 [INFO][66] confd/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:10.057 [INFO][66] confd/watchersyncer.go 221: All watchers have sync'd data - sending data and final sync
2023-03-09 06:10:10.057 [INFO][66] confd/watchersyncer.go 130: Sending status update Status=in-sync
2023-03-09 06:10:10.057 [INFO][66] confd/client.go 422: Source SourceSyncer readiness changed, ready=true
2023-03-09 06:10:10.057 [INFO][66] confd/client.go 442: Data is now syncd, can start rendering templates
2023-03-09 06:10:10.071 [INFO][67] status-reporter/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/caliconodestatuses"
2023-03-09 06:10:10.072 [INFO][67] status-reporter/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:10.072 [INFO][67] status-reporter/watchersyncer.go 130: Sending status update Status=resync
2023-03-09 06:10:10.072 [INFO][67] status-reporter/watchersyncer.go 221: All watchers have sync'd data - sending data and final sync
2023-03-09 06:10:10.072 [INFO][67] status-reporter/watchersyncer.go 130: Sending status update Status=in-sync
2023-03-09 06:10:10.093 [INFO][65] felix/daemon.go 373: Successfully loaded configuration. GOMAXPROCS=1 builddate="2022-11-08T00:15:45+0000" config=&config.Config{UseInternalDataplaneDriver:true, DataplaneDriver:"calico-iptables-plugin", DataplaneWatchdogTimeout:90000000000, WireguardEnabled:false, WireguardEnabledV6:false, WireguardListeningPort:51820, WireguardListeningPortV6:51821, WireguardRoutingRulePriority:99, WireguardInterfaceName:"wireguard.cali", WireguardInterfaceNameV6:"wg-v6.cali", WireguardMTU:0, WireguardMTUV6:0, WireguardHostEncryptionEnabled:false, WireguardPersistentKeepAlive:0, BPFEnabled:false, BPFDisableUnprivileged:true, BPFLogLevel:"off", BPFDataIfacePattern:(*regexp.Regexp)(0xc0008daaa0), BPFConnectTimeLoadBalancingEnabled:true, BPFExternalServiceMode:"tunnel", BPFKubeProxyIptablesCleanupEnabled:true, BPFKubeProxyMinSyncPeriod:1000000000, BPFKubeProxyEndpointSlicesEnabled:true, BPFExtToServiceConnmark:0, BPFPSNATPorts:numorstring.Port{MinPort:0x4e20, MaxPort:0x752f, PortName:""}, BPFMapSizeNATFrontend:65536, BPFMapSizeNATBackend:262144, BPFMapSizeNATAffinity:65536, BPFMapSizeRoute:262144, BPFMapSizeConntrack:512000, BPFMapSizeIPSets:1048576, BPFMapSizeIfState:1000, BPFHostConntrackBypass:true, BPFEnforceRPF:"Strict", BPFPolicyDebugEnabled:true, DebugBPFCgroupV2:"", DebugBPFMapRepinEnabled:false, DatastoreType:"kubernetes", FelixHostname:"lke96996-146211-640977a3930b", EtcdAddr:"127.0.0.1:2379", EtcdScheme:"http", EtcdKeyFile:"", EtcdCertFile:"", EtcdCaFile:"", EtcdEndpoints:[]string(nil), TyphaAddr:"", TyphaK8sServiceName:"", TyphaK8sNamespace:"kube-system", TyphaReadTimeout:30000000000, TyphaWriteTimeout:10000000000, TyphaKeyFile:"", TyphaCertFile:"", TyphaCAFile:"", TyphaCN:"", TyphaURISAN:"", Ipv6Support:false, BpfIpv6Support:false, IptablesBackend:"auto", RouteRefreshInterval:90000000000, InterfaceRefreshInterval:90000000000, DeviceRouteSourceAddress:net.IP(nil), DeviceRouteSourceAddressIPv6:net.IP(nil), DeviceRouteProtocol:3, RemoveExternalRoutes:true, IptablesRefreshInterval:90000000000, IptablesPostWriteCheckIntervalSecs:1000000000, IptablesLockFilePath:"/run/xtables.lock", IptablesLockTimeoutSecs:0, IptablesLockProbeIntervalMillis:50000000, FeatureDetectOverride:map[string]string(nil), IpsetsRefreshInterval:10000000000, MaxIpsetSize:1048576, XDPRefreshInterval:90000000000, PolicySyncPathPrefix:"", NetlinkTimeoutSecs:10000000000, MetadataAddr:"", MetadataPort:8775, OpenstackRegion:"", InterfacePrefix:"cali", InterfaceExclude:[]*regexp.Regexp{(*regexp.Regexp)(0xc0008dabe0)}, ChainInsertMode:"insert", DefaultEndpointToHostAction:"ACCEPT", IptablesFilterAllowAction:"ACCEPT", IptablesMangleAllowAction:"ACCEPT", LogPrefix:"calico-packet", LogFilePath:"", LogSeverityFile:"", LogSeverityScreen:"INFO", LogSeveritySys:"", LogDebugFilenameRegex:(*regexp.Regexp)(nil), VXLANEnabled:(*bool)(nil), VXLANPort:4789, VXLANVNI:4096, VXLANMTU:0, VXLANMTUV6:0, IPv4VXLANTunnelAddr:net.IP(nil), IPv6VXLANTunnelAddr:net.IP(nil), VXLANTunnelMACAddr:"", VXLANTunnelMACAddrV6:"", IpInIpEnabled:(*bool)(nil), IpInIpMtu:0, IpInIpTunnelAddr:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xa, 0x2, 0x0, 0x1}, FloatingIPs:"Disabled", AllowVXLANPacketsFromWorkloads:false, AllowIPIPPacketsFromWorkloads:false, AWSSrcDstCheck:"DoNothing", ServiceLoopPrevention:"Drop", WorkloadSourceSpoofing:"Disabled", ReportingIntervalSecs:0, ReportingTTLSecs:90000000000, EndpointReportingEnabled:false, EndpointReportingDelaySecs:1000000000, IptablesMarkMask:0xffff0000, DisableConntrackInvalidCheck:false, HealthEnabled:true, HealthPort:9099, HealthHost:"localhost", PrometheusMetricsEnabled:false, PrometheusMetricsHost:"", PrometheusMetricsPort:9091, PrometheusGoMetricsEnabled:true, PrometheusProcessMetricsEnabled:true, PrometheusWireGuardMetricsEnabled:true, FailsafeInboundHostPorts:[]config.ProtoPort{config.ProtoPort{Net:"", Protocol:"tcp", Port:0x16}, config.ProtoPort{Net:"", Protocol:"udp", Port:0x44}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0xb3}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94c}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1561}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x192b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0a}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0b}}, FailsafeOutboundHostPorts:[]config.ProtoPort{config.ProtoPort{Net:"", Protocol:"udp", Port:0x35}, config.ProtoPort{Net:"", Protocol:"udp", Port:0x43}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0xb3}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94c}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1561}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x192b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0a}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0b}}, KubeNodePortRanges:[]numorstring.Port{numorstring.Port{MinPort:0x7530, MaxPort:0x7fff, PortName:""}}, NATPortRange:numorstring.Port{MinPort:0x0, MaxPort:0x0, PortName:""}, NATOutgoingAddress:net.IP(nil), UsageReportingEnabled:true, UsageReportingInitialDelaySecs:300000000000, UsageReportingIntervalSecs:86400000000000, ClusterGUID:"7197fc97702c482f9a5df8fb5c03809a", ClusterType:"k8s,bgp,kubeadm,kdd", CalicoVersion:"v3.24.5", ExternalNodesCIDRList:[]string(nil), DebugMemoryProfilePath:"", DebugCPUProfilePath:"/tmp/felix-cpu-<timestamp>.pprof", DebugDisableLogDropping:false, DebugSimulateCalcGraphHangAfter:0, DebugSimulateDataplaneHangAfter:0, DebugPanicAfter:0, DebugSimulateDataRace:false, RouteSource:"CalicoIPAM", RouteTableRange:idalloc.IndexRange{Min:0, Max:0}, RouteTableRanges:[]idalloc.IndexRange(nil), RouteSyncDisabled:false, IptablesNATOutgoingInterfaceFilter:"", SidecarAccelerationEnabled:false, XDPEnabled:true, GenericXDPEnabled:false, Variant:"Calico", MTUIfacePattern:(*regexp.Regexp)(0xc0008dae60), Encapsulation:config.Encapsulation{IPIPEnabled:true, VXLANEnabled:false, VXLANEnabledV6:false}, internalOverrides:map[string]string{}, sourceToRawConfig:map[config.Source]map[string]string{0x1:map[string]string{"CalicoVersion":"v3.24.5", "ClusterGUID":"7197fc97702c482f9a5df8fb5c03809a", "ClusterType":"k8s,bgp,kubeadm,kdd", "FloatingIPs":"Disabled", "LogSeverityScreen":"Info", "ReportingIntervalSecs":"0"}, 0x2:map[string]string{"IpInIpTunnelAddr":"10.2.0.1"}, 0x3:map[string]string{"LogFilePath":"None", "LogSeverityFile":"None", "LogSeveritySys":"None", "MetadataAddr":"None"}, 0x4:map[string]string{"datastoretype":"kubernetes", "defaultendpointtohostaction":"ACCEPT", "felixhostname":"lke96996-146211-640977a3930b", "healthenabled":"true", "ipinipmtu":"0", "ipv6support":"false", "vxlanmtu":"0", "wireguardmtu":"0"}}, rawValues:map[string]string{"CalicoVersion":"v3.24.5", "ClusterGUID":"7197fc97702c482f9a5df8fb5c03809a", "ClusterType":"k8s,bgp,kubeadm,kdd", "DatastoreType":"kubernetes", "DefaultEndpointToHostAction":"ACCEPT", "FelixHostname":"lke96996-146211-640977a3930b", "FloatingIPs":"Disabled", "HealthEnabled":"true", "IpInIpMtu":"0", "IpInIpTunnelAddr":"10.2.0.1", "Ipv6Support":"false", "LogFilePath":"None", "LogSeverityFile":"None", "LogSeverityScreen":"Info", "LogSeveritySys":"None", "MetadataAddr":"None", "ReportingIntervalSecs":"0", "VXLANMTU":"0", "WireguardMTU":"0"}, Err:error(nil), loadClientConfigFromEnvironment:(func() (*apiconfig.CalicoAPIConfig, error))(0x144f0c0), useNodeResourceUpdates:false} gitcommit="f1a1611acb98d9187f48bbbe2227301aa69f0499" version="v3.24.5"
2023-03-09 06:10:10.114 [INFO][65] felix/bootstrap.go 209: Wireguard is not enabled - ensure no wireguard config iface="wireguard.cali" ipVersion=0x4 nodeName="lke96996-146211-640977a3930b"
2023-03-09 06:10:10.118 [INFO][65] felix/bootstrap.go 624: Wireguard public key not set in datastore ipVersion=0x4 nodeName="lke96996-146211-640977a3930b"
2023-03-09 06:10:10.121 [INFO][66] confd/resource.go 278: Target config /etc/calico/confd/config/bird6_aggr.cfg has been updated
2023-03-09 06:10:10.118 [INFO][65] felix/bootstrap.go 209: Wireguard is not enabled - ensure no wireguard config iface="wg-v6.cali" ipVersion=0x6 nodeName="lke96996-146211-640977a3930b"
2023-03-09 06:10:10.129 [INFO][60] cni-config-monitor/token_watch.go 225: Update of CNI kubeconfig triggered based on elapsed time.
2023-03-09 06:10:10.130 [INFO][60] cni-config-monitor/token_watch.go 279: Wrote updated CNI kubeconfig file. path="/host/etc/cni/net.d/calico-kubeconfig"
2023-03-09 06:10:10.133 [INFO][66] confd/resource.go 278: Target config /etc/calico/confd/config/bird_aggr.cfg has been updated
2023-03-09 06:10:10.133 [INFO][66] confd/resource.go 278: Target config /etc/calico/confd/config/bird_ipam.cfg has been updated
2023-03-09 06:10:10.134 [ERROR][66] confd/resource.go 306: Error from checkcmd "bird6 -p -c /etc/calico/confd/config/.bird6.cfg3921715927": "bird: /etc/calico/confd/config/.bird6.cfg3921715927:7:1 Unable to open included file /etc/calico/confd/config/bird6_ipam.cfg: No such file or directory\n"
2023-03-09 06:10:10.134 [INFO][66] confd/resource.go 240: Check failed, but file does not yet exist - create anyway
2023-03-09 06:10:10.122 [INFO][65] felix/bootstrap.go 624: Wireguard public key not set in datastore ipVersion=0x6 nodeName="lke96996-146211-640977a3930b"
2023-03-09 06:10:10.123 [INFO][65] felix/driver.go 72: Using internal (linux) dataplane driver.
2023-03-09 06:10:10.123 [INFO][65] felix/driver.go 157: Calculated iptables mark bits acceptMark=0x10000 endpointMark=0xfff00000 endpointMarkNonCali=0x0 passMark=0x20000 scratch0Mark=0x40000 scratch1Mark=0x80000
2023-03-09 06:10:10.123 [INFO][65] felix/int_dataplane.go 336: Creating internal dataplane driver. config=intdataplane.Config{Hostname:"lke96996-146211-640977a3930b", IPv6Enabled:false, RuleRendererOverride:rules.RuleRenderer(nil), IPIPMTU:0, VXLANMTU:0, VXLANMTUV6:0, VXLANPort:4789, MaxIPSetSize:1048576, RouteSyncDisabled:false, IptablesBackend:"auto", IPSetsRefreshInterval:10000000000, RouteRefreshInterval:90000000000, DeviceRouteSourceAddress:net.IP(nil), DeviceRouteSourceAddressIPv6:net.IP(nil), DeviceRouteProtocol:3, RemoveExternalRoutes:true, IptablesRefreshInterval:90000000000, IptablesPostWriteCheckInterval:1000000000, IptablesInsertMode:"insert", IptablesLockFilePath:"/run/xtables.lock", IptablesLockTimeout:0, IptablesLockProbeInterval:50000000, XDPRefreshInterval:90000000000, FloatingIPsEnabled:false, Wireguard:wireguard.Config{Enabled:false, EnabledV6:false, ListeningPort:51820, ListeningPortV6:51821, FirewallMark:0, RoutingRulePriority:99, RoutingTableIndex:1, RoutingTableIndexV6:2, InterfaceName:"wireguard.cali", InterfaceNameV6:"wg-v6.cali", MTU:0, MTUV6:0, RouteSource:"CalicoIPAM", EncryptHostTraffic:false, PersistentKeepAlive:0, RouteSyncDisabled:false}, NetlinkTimeout:10000000000, RulesConfig:rules.Config{IPSetConfigV4:(*ipsets.IPVersionConfig)(0xc000912e60), IPSetConfigV6:(*ipsets.IPVersionConfig)(0xc000912f50), WorkloadIfacePrefixes:[]string{"cali"}, IptablesMarkAccept:0x10000, IptablesMarkPass:0x20000, IptablesMarkScratch0:0x40000, IptablesMarkScratch1:0x80000, IptablesMarkEndpoint:0xfff00000, IptablesMarkNonCaliEndpoint:0x0, KubeNodePortRanges:[]numorstring.Port{numorstring.Port{MinPort:0x7530, MaxPort:0x7fff, PortName:""}}, KubeIPVSSupportEnabled:false, OpenStackMetadataIP:net.IP(nil), OpenStackMetadataPort:0x2247, OpenStackSpecialCasesEnabled:false, VXLANEnabled:false, VXLANEnabledV6:false, VXLANPort:4789, VXLANVNI:4096, IPIPEnabled:true, FelixConfigIPIPEnabled:(*bool)(nil), IPIPTunnelAddress:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xa, 0x2, 0x0, 0x1}, VXLANTunnelAddress:net.IP(nil), VXLANTunnelAddressV6:net.IP(nil), AllowVXLANPacketsFromWorkloads:false, AllowIPIPPacketsFromWorkloads:false, WireguardEnabled:false, WireguardEnabledV6:false, WireguardInterfaceName:"wireguard.cali", WireguardInterfaceNameV6:"wg-v6.cali", WireguardIptablesMark:0x0, WireguardListeningPort:51820, WireguardListeningPortV6:51821, WireguardEncryptHostTraffic:false, RouteSource:"CalicoIPAM", IptablesLogPrefix:"calico-packet", EndpointToHostAction:"ACCEPT", IptablesFilterAllowAction:"ACCEPT", IptablesMangleAllowAction:"ACCEPT", FailsafeInboundHostPorts:[]config.ProtoPort{config.ProtoPort{Net:"", Protocol:"tcp", Port:0x16}, config.ProtoPort{Net:"", Protocol:"udp", Port:0x44}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0xb3}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94c}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1561}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x192b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0a}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0b}}, FailsafeOutboundHostPorts:[]config.ProtoPort{config.ProtoPort{Net:"", Protocol:"udp", Port:0x35}, config.ProtoPort{Net:"", Protocol:"udp", Port:0x43}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0xb3}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94c}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1561}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x192b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0a}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0b}}, DisableConntrackInvalid:false, NATPortRange:numorstring.Port{MinPort:0x0, MaxPort:0x0, PortName:""}, IptablesNATOutgoingInterfaceFilter:"", NATOutgoingAddress:net.IP(nil), BPFEnabled:false, ServiceLoopPrevention:"Drop"}, IfaceMonitorConfig:ifacemonitor.Config{InterfaceExcludes:[]*regexp.Regexp{(*regexp.Regexp)(0xc0008dabe0)}, ResyncInterval:90000000000}, StatusReportingInterval:0, ConfigChangedRestartCallback:(func())(0x259e3a0), FatalErrorRestartCallback:(func(error))(0x259e280), PostInSyncCallback:(func())(0x258bf40), HealthAggregator:(*health.HealthAggregator)(0xc0008f4300), WatchdogTimeout:90000000000, RouteTableManager:(*idalloc.IndexAllocator)(0xc0003b6360), DebugSimulateDataplaneHangAfter:0, ExternalNodesCidrs:[]string(nil), BPFEnabled:false, BPFPolicyDebugEnabled:true, BPFDisableUnprivileged:true, BPFKubeProxyIptablesCleanupEnabled:true, BPFLogLevel:"off", BPFExtToServiceConnmark:0, BPFDataIfacePattern:(*regexp.Regexp)(0xc0008daaa0), XDPEnabled:true, XDPAllowGeneric:false, BPFConntrackTimeouts:conntrack.Timeouts{CreationGracePeriod:10000000000, TCPPreEstablished:20000000000, TCPEstablished:3600000000000, TCPFinsSeen:30000000000, TCPResetSeen:40000000000, UDPLastSeen:60000000000, GenericIPLastSeen:600000000000, ICMPLastSeen:5000000000}, BPFCgroupV2:"", BPFConnTimeLBEnabled:true, BPFMapRepin:false, BPFNodePortDSREnabled:false, BPFPSNATPorts:numorstring.Port{MinPort:0x4e20, MaxPort:0x752f, PortName:""}, BPFMapSizeRoute:262144, BPFMapSizeConntrack:512000, BPFMapSizeNATFrontend:65536, BPFMapSizeNATBackend:262144, BPFMapSizeNATAffinity:65536, BPFMapSizeIPSets:1048576, BPFMapSizeIfState:1000, BPFIpv6Enabled:false, BPFHostConntrackBypass:true, BPFEnforceRPF:"Strict", KubeProxyMinSyncPeriod:1000000000, SidecarAccelerationEnabled:false, LookPathOverride:(func(string) (string, error))(nil), KubeClientSet:(*kubernetes.Clientset)(0xc0000e5080), FeatureDetectOverrides:map[string]string(nil), hostMTU:0, MTUIfacePattern:(*regexp.Regexp)(0xc0008dae60), RouteSource:"CalicoIPAM", KubernetesProvider:0x0}
2023-03-09 06:10:10.123 [INFO][65] felix/rule_defs.go 373: Creating rule renderer. config=rules.Config{IPSetConfigV4:(*ipsets.IPVersionConfig)(0xc000912e60), IPSetConfigV6:(*ipsets.IPVersionConfig)(0xc000912f50), WorkloadIfacePrefixes:[]string{"cali"}, IptablesMarkAccept:0x10000, IptablesMarkPass:0x20000, IptablesMarkScratch0:0x40000, IptablesMarkScratch1:0x80000, IptablesMarkEndpoint:0xfff00000, IptablesMarkNonCaliEndpoint:0x0, KubeNodePortRanges:[]numorstring.Port{numorstring.Port{MinPort:0x7530, MaxPort:0x7fff, PortName:""}}, KubeIPVSSupportEnabled:false, OpenStackMetadataIP:net.IP(nil), OpenStackMetadataPort:0x2247, OpenStackSpecialCasesEnabled:false, VXLANEnabled:false, VXLANEnabledV6:false, VXLANPort:4789, VXLANVNI:4096, IPIPEnabled:true, FelixConfigIPIPEnabled:(*bool)(nil), IPIPTunnelAddress:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xa, 0x2, 0x0, 0x1}, VXLANTunnelAddress:net.IP(nil), VXLANTunnelAddressV6:net.IP(nil), AllowVXLANPacketsFromWorkloads:false, AllowIPIPPacketsFromWorkloads:false, WireguardEnabled:false, WireguardEnabledV6:false, WireguardInterfaceName:"wireguard.cali", WireguardInterfaceNameV6:"wg-v6.cali", WireguardIptablesMark:0x0, WireguardListeningPort:51820, WireguardListeningPortV6:51821, WireguardEncryptHostTraffic:false, RouteSource:"CalicoIPAM", IptablesLogPrefix:"calico-packet", EndpointToHostAction:"ACCEPT", IptablesFilterAllowAction:"ACCEPT", IptablesMangleAllowAction:"ACCEPT", FailsafeInboundHostPorts:[]config.ProtoPort{config.ProtoPort{Net:"", Protocol:"tcp", Port:0x16}, config.ProtoPort{Net:"", Protocol:"udp", Port:0x44}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0xb3}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94c}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1561}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x192b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0a}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0b}}, FailsafeOutboundHostPorts:[]config.ProtoPort{config.ProtoPort{Net:"", Protocol:"udp", Port:0x35}, config.ProtoPort{Net:"", Protocol:"udp", Port:0x43}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0xb3}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94c}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1561}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x192b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0a}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0b}}, DisableConntrackInvalid:false, NATPortRange:numorstring.Port{MinPort:0x0, MaxPort:0x0, PortName:""}, IptablesNATOutgoingInterfaceFilter:"", NATOutgoingAddress:net.IP(nil), BPFEnabled:false, ServiceLoopPrevention:"Drop"}
2023-03-09 06:10:10.127 [INFO][65] felix/rule_defs.go 383: Workload to host packets will be accepted.
2023-03-09 06:10:10.130 [INFO][65] felix/rule_defs.go 397: filter table allowed packets will be accepted immediately.
2023-03-09 06:10:10.131 [INFO][65] felix/rule_defs.go 405: mangle table allowed packets will be accepted immediately.
2023-03-09 06:10:10.132 [INFO][65] felix/rule_defs.go 413: Packets to unknown service IPs will be dropped
2023-03-09 06:10:10.132 [INFO][65] felix/int_dataplane.go 1020: Determined pod MTU mtu=1480
2023-03-09 06:10:10.132 [INFO][65] felix/iface_monitor.go 84: configured to periodically rescan interfaces. interval=1m30s
2023-03-09 06:10:10.132 [INFO][65] felix/feature_detect.go 354: Looked up iptables command backendMode="legacy" candidates=[]string{"ip6tables-legacy-save", "ip6tables-save"} command="ip6tables-legacy-save" ipVersion=0x6 saveOrRestore="save"
2023-03-09 06:10:10.132 [INFO][65] felix/feature_detect.go 354: Looked up iptables command backendMode="legacy" candidates=[]string{"iptables-legacy-save", "iptables-save"} command="iptables-legacy-save" ipVersion=0x4 saveOrRestore="save"
2023-03-09 06:10:10.142 [INFO][66] confd/resource.go 278: Target config /etc/calico/confd/config/bird6_ipam.cfg has been updated
2023-03-09 06:10:10.151 [INFO][66] confd/resource.go 278: Target config /etc/calico/confd/config/bird.cfg has been updated
2023-03-09 06:10:10.152 [INFO][66] confd/resource.go 278: Target config /etc/calico/confd/config/bird6.cfg has been updated
2023-03-09 06:10:10.162 [INFO][59] monitor-addresses/startup.go 427: Early log level set to info
2023-03-09 06:10:10.164 [INFO][59] monitor-addresses/utils.go 127: Using NODENAME environment for node name lke96996-146211-640977a3930b
2023-03-09 06:10:10.154 [INFO][65] felix/feature_detect.go 354: Looked up iptables command backendMode="nft" candidates=[]string{"ip6tables-nft-save", "ip6tables-save"} command="ip6tables-nft-save" ipVersion=0x6 saveOrRestore="save"
2023-03-09 06:10:10.154 [INFO][65] felix/feature_detect.go 354: Looked up iptables command backendMode="nft" candidates=[]string{"iptables-nft-save", "iptables-save"} command="iptables-nft-save" ipVersion=0x4 saveOrRestore="save"
2023-03-09 06:10:10.167 [INFO][59] monitor-addresses/utils.go 139: Determined node name: lke96996-146211-640977a3930b
2023-03-09 06:10:10.184 [INFO][63] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "wireguardmtu"="0"
2023-03-09 06:10:10.184 [INFO][63] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "vxlanmtu"="0"
2023-03-09 06:10:10.184 [INFO][63] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "healthenabled"="true"
2023-03-09 06:10:10.184 [INFO][63] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "defaultendpointtohostaction"="ACCEPT"
2023-03-09 06:10:10.184 [INFO][63] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "ipinipmtu"="0"
2023-03-09 06:10:10.185 [INFO][63] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "ipv6support"="false"
2023-03-09 06:10:10.185 [INFO][63] tunnel-ip-allocator/config_params.go 435: Merging in config from environment variable: map[defaultendpointtohostaction:ACCEPT healthenabled:true ipinipmtu:0 ipv6support:false vxlanmtu:0 wireguardmtu:0]
2023-03-09 06:10:10.186 [INFO][63] tunnel-ip-allocator/config_params.go 542: Parsing value for WireguardMTU: 0 (from environment variable)
2023-03-09 06:10:10.186 [INFO][63] tunnel-ip-allocator/config_params.go 578: Parsed value for WireguardMTU: 0 (from environment variable)
2023-03-09 06:10:10.186 [INFO][63] tunnel-ip-allocator/config_params.go 542: Parsing value for VXLANMTU: 0 (from environment variable)
2023-03-09 06:10:10.186 [INFO][63] tunnel-ip-allocator/config_params.go 578: Parsed value for VXLANMTU: 0 (from environment variable)
2023-03-09 06:10:10.187 [INFO][63] tunnel-ip-allocator/config_params.go 542: Parsing value for HealthEnabled: true (from environment variable)
2023-03-09 06:10:10.187 [INFO][63] tunnel-ip-allocator/config_params.go 578: Parsed value for HealthEnabled: true (from environment variable)
2023-03-09 06:10:10.187 [INFO][63] tunnel-ip-allocator/config_params.go 542: Parsing value for DefaultEndpointToHostAction: ACCEPT (from environment variable)
2023-03-09 06:10:10.190 [INFO][63] tunnel-ip-allocator/config_params.go 578: Parsed value for DefaultEndpointToHostAction: ACCEPT (from environment variable)
2023-03-09 06:10:10.190 [INFO][63] tunnel-ip-allocator/config_params.go 542: Parsing value for IpInIpMtu: 0 (from environment variable)
2023-03-09 06:10:10.190 [INFO][63] tunnel-ip-allocator/config_params.go 578: Parsed value for IpInIpMtu: 0 (from environment variable)
2023-03-09 06:10:10.191 [INFO][63] tunnel-ip-allocator/config_params.go 542: Parsing value for Ipv6Support: false (from environment variable)
2023-03-09 06:10:10.191 [INFO][63] tunnel-ip-allocator/config_params.go 578: Parsed value for Ipv6Support: false (from environment variable)
2023-03-09 06:10:10.220 [INFO][65] felix/feature_detect.go 163: Updating detected iptables features features=environment.Features{SNATFullyRandom:true, MASQFullyRandom:true, RestoreSupportsLock:true, ChecksumOffloadBroken:false, IPIPDeviceIsL3:false} iptablesVersion=1.8.4 kernelVersion=5.10.0-19
2023-03-09 06:10:10.220 [INFO][65] felix/table.go 336: Calculated old-insert detection regex. pattern="(?:-j|--jump) cali-|(?:-j|--jump) califw-|(?:-j|--jump) calitw-|(?:-j|--jump) califh-|(?:-j|--jump) calith-|(?:-j|--jump) calipi-|(?:-j|--jump) calipo-|(?:-j|--jump) felix-"
2023-03-09 06:10:10.220 [INFO][65] felix/table.go 449: Enabling iptables-in-nftables-mode workarounds.
2023-03-09 06:10:10.220 [INFO][65] felix/feature_detect.go 354: Looked up iptables command backendMode="nft" candidates=[]string{"iptables-nft-restore", "iptables-restore"} command="iptables-nft-restore" ipVersion=0x4 saveOrRestore="restore"
2023-03-09 06:10:10.220 [INFO][65] felix/feature_detect.go 354: Looked up iptables command backendMode="nft" candidates=[]string{"iptables-nft-save", "iptables-save"} command="iptables-nft-save" ipVersion=0x4 saveOrRestore="save"
2023-03-09 06:10:10.220 [INFO][65] felix/table.go 336: Calculated old-insert detection regex. pattern="(?:-j|--jump) cali-|(?:-j|--jump) califw-|(?:-j|--jump) calitw-|(?:-j|--jump) califh-|(?:-j|--jump) calith-|(?:-j|--jump) calipi-|(?:-j|--jump) calipo-|(?:-j|--jump) felix-|-A POSTROUTING .* felix-masq-ipam-pools .*|-A POSTROUTING -o tunl0 -m addrtype ! --src-type LOCAL --limit-iface-out -m addrtype --src-type LOCAL -j MASQUERADE"
2023-03-09 06:10:10.220 [INFO][65] felix/table.go 449: Enabling iptables-in-nftables-mode workarounds.
2023-03-09 06:10:10.220 [INFO][65] felix/feature_detect.go 354: Looked up iptables command backendMode="nft" candidates=[]string{"iptables-nft-restore", "iptables-restore"} command="iptables-nft-restore" ipVersion=0x4 saveOrRestore="restore"
2023-03-09 06:10:10.220 [INFO][65] felix/feature_detect.go 354: Looked up iptables command backendMode="nft" candidates=[]string{"iptables-nft-save", "iptables-save"} command="iptables-nft-save" ipVersion=0x4 saveOrRestore="save"
2023-03-09 06:10:10.220 [INFO][65] felix/table.go 336: Calculated old-insert detection regex. pattern="(?:-j|--jump) cali-|(?:-j|--jump) califw-|(?:-j|--jump) calitw-|(?:-j|--jump) califh-|(?:-j|--jump) calith-|(?:-j|--jump) calipi-|(?:-j|--jump) calipo-|(?:-j|--jump) felix-"
2023-03-09 06:10:10.221 [INFO][65] felix/table.go 449: Enabling iptables-in-nftables-mode workarounds.
2023-03-09 06:10:10.221 [INFO][65] felix/feature_detect.go 354: Looked up iptables command backendMode="nft" candidates=[]string{"iptables-nft-restore", "iptables-restore"} command="iptables-nft-restore" ipVersion=0x4 saveOrRestore="restore"
2023-03-09 06:10:10.221 [INFO][65] felix/feature_detect.go 354: Looked up iptables command backendMode="nft" candidates=[]string{"iptables-nft-save", "iptables-save"} command="iptables-nft-save" ipVersion=0x4 saveOrRestore="save"
2023-03-09 06:10:10.222 [INFO][65] felix/table.go 336: Calculated old-insert detection regex. pattern="(?:-j|--jump) cali-|(?:-j|--jump) califw-|(?:-j|--jump) calitw-|(?:-j|--jump) califh-|(?:-j|--jump) calith-|(?:-j|--jump) calipi-|(?:-j|--jump) calipo-|(?:-j|--jump) felix-"
2023-03-09 06:10:10.222 [INFO][65] felix/table.go 449: Enabling iptables-in-nftables-mode workarounds.
2023-03-09 06:10:10.222 [INFO][65] felix/feature_detect.go 354: Looked up iptables command backendMode="nft" candidates=[]string{"iptables-nft-restore", "iptables-restore"} command="iptables-nft-restore" ipVersion=0x4 saveOrRestore="restore"
2023-03-09 06:10:10.223 [INFO][65] felix/feature_detect.go 354: Looked up iptables command backendMode="nft" candidates=[]string{"iptables-nft-save", "iptables-save"} command="iptables-nft-save" ipVersion=0x4 saveOrRestore="save"
2023-03-09 06:10:10.227 [INFO][65] felix/int_dataplane.go 514: XDP acceleration enabled.
2023-03-09 06:10:10.246 [INFO][65] felix/connecttime.go 54: Running bpftool to look up programs attached to cgroup args=[]string{"bpftool", "-j", "-p", "cgroup", "show", "/run/calico/cgroup"}
2023-03-09 06:10:10.273 [INFO][65] felix/route_table.go 317: Calculated interface name regexp ifaceRegex="^cali.*" ipVersion=0x4 tableIndex=0
2023-03-09 06:10:10.273 [INFO][65] felix/ipsets.go 130: Queueing IP set for creation family="inet" setID="all-ipam-pools" setType="hash:net"
2023-03-09 06:10:10.273 [INFO][65] felix/ipsets.go 130: Queueing IP set for creation family="inet" setID="masq-ipam-pools" setType="hash:net"
2023-03-09 06:10:10.273 [INFO][65] felix/route_table.go 317: Calculated interface name regexp ifaceRegex="^wireguard.cali$" ipVersion=0x4 tableIndex=1
2023-03-09 06:10:10.273 [INFO][65] felix/int_dataplane.go 918: Registering to report health.
2023-03-09 06:10:10.277 [INFO][65] felix/int_dataplane.go 1856: attempted to modprobe nf_conntrack_proto_sctp error=exit status 1 output=""
2023-03-09 06:10:10.277 [INFO][65] felix/int_dataplane.go 1858: Making sure IPv4 forwarding is enabled.
2023-03-09 06:10:10.277 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-failsafe-in" ipVersion=0x4 table="raw"
2023-03-09 06:10:10.277 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-failsafe-out" ipVersion=0x4 table="raw"
2023-03-09 06:10:10.277 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-PREROUTING" ipVersion=0x4 table="raw"
2023-03-09 06:10:10.277 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-rpf-skip"
2023-03-09 06:10:10.277 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-from-host-endpoint"
2023-03-09 06:10:10.277 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-wireguard-incoming-mark" ipVersion=0x4 table="raw"
2023-03-09 06:10:10.277 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-OUTPUT" ipVersion=0x4 table="raw"
2023-03-09 06:10:10.277 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-to-host-endpoint"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-PREROUTING"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-OUTPUT"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-FORWARD" ipVersion=0x4 table="filter"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-from-hep-forward"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-from-wl-dispatch"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-to-wl-dispatch"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-to-hep-forward"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-cidr-block"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-INPUT" ipVersion=0x4 table="filter"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-wl-to-host"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-from-host-endpoint"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-wl-to-host" ipVersion=0x4 table="filter"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-failsafe-in" ipVersion=0x4 table="filter"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-OUTPUT" ipVersion=0x4 table="filter"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-to-host-endpoint"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-failsafe-out" ipVersion=0x4 table="filter"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-FORWARD"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-INPUT"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-OUTPUT"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-PREROUTING" ipVersion=0x4 table="nat"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-fip-dnat"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-POSTROUTING" ipVersion=0x4 table="nat"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-fip-snat"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-nat-outgoing"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-OUTPUT" ipVersion=0x4 table="nat"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-PREROUTING"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-POSTROUTING"
2023-03-09 06:10:10.278 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-OUTPUT"
2023-03-09 06:10:10.279 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-failsafe-in" ipVersion=0x4 table="mangle"
2023-03-09 06:10:10.279 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-failsafe-out" ipVersion=0x4 table="mangle"
2023-03-09 06:10:10.279 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-PREROUTING" ipVersion=0x4 table="mangle"
2023-03-09 06:10:10.279 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-from-host-endpoint"
2023-03-09 06:10:10.279 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-POSTROUTING" ipVersion=0x4 table="mangle"
2023-03-09 06:10:10.279 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-to-host-endpoint"
2023-03-09 06:10:10.279 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-PREROUTING"
2023-03-09 06:10:10.279 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-POSTROUTING"
2023-03-09 06:10:10.289 [INFO][65] felix/int_dataplane.go 1605: Set XDP failsafe ports: [{Net: Protocol:tcp Port:22} {Net: Protocol:udp Port:68} {Net: Protocol:tcp Port:179} {Net: Protocol:tcp Port:2379} {Net: Protocol:tcp Port:2380} {Net: Protocol:tcp Port:5473} {Net: Protocol:tcp Port:6443} {Net: Protocol:tcp Port:6666} {Net: Protocol:tcp Port:6667}]
2023-03-09 06:10:10.289 [INFO][65] felix/int_dataplane.go 1327: IPIP enabled, starting thread to keep tunnel configuration in sync.
2023-03-09 06:10:10.289 [INFO][65] felix/daemon.go 422: Connect to the dataplane driver.
2023-03-09 06:10:10.290 [INFO][65] felix/daemon.go 501: using resource updates where applicable
2023-03-09 06:10:10.290 [INFO][65] felix/daemon.go 504: Created Syncer syncer=&watchersyncer.watcherSyncer{status:0x0, watcherCaches:[]*watchersyncer.watcherCache{(*watchersyncer.watcherCache)(0xc0002fae10), (*watchersyncer.watcherCache)(0xc0002faea0), (*watchersyncer.watcherCache)(0xc0002faf30), (*watchersyncer.watcherCache)(0xc0002fafc0), (*watchersyncer.watcherCache)(0xc0002fb050), (*watchersyncer.watcherCache)(0xc0002fb0e0), (*watchersyncer.watcherCache)(0xc0002fb170), (*watchersyncer.watcherCache)(0xc0002fb200), (*watchersyncer.watcherCache)(0xc0002fb290), (*watchersyncer.watcherCache)(0xc0002fb320), (*watchersyncer.watcherCache)(0xc0002fb3b0), (*watchersyncer.watcherCache)(0xc0002fb440), (*watchersyncer.watcherCache)(0xc0002fb4d0), (*watchersyncer.watcherCache)(0xc0002fb560), (*watchersyncer.watcherCache)(0xc0002fb5f0)}, results:(chan interface {})(0xc0003a7680), numSynced:0, callbacks:(*calc.SyncerCallbacksDecoupler)(0xc0000114a0), wgwc:(*sync.WaitGroup)(nil), wgws:(*sync.WaitGroup)(nil), cancel:(context.CancelFunc)(nil)}
2023-03-09 06:10:10.290 [INFO][65] felix/daemon.go 508: Starting the datastore Syncer
2023-03-09 06:10:10.290 [INFO][65] felix/watchersyncer.go 89: Start called
2023-03-09 06:10:10.290 [INFO][65] felix/calc_graph.go 118: Creating calculation graph, filtered to hostname lke96996-146211-640977a3930b
2023-03-09 06:10:10.290 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.WorkloadEndpointKey: (dispatcher.UpdateHandler)(0x198b760)
2023-03-09 06:10:10.290 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.HostEndpointKey: (dispatcher.UpdateHandler)(0x198b760)
2023-03-09 06:10:10.290 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.WorkloadEndpointKey: (dispatcher.UpdateHandler)(0x198b900)
2023-03-09 06:10:10.290 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.HostEndpointKey: (dispatcher.UpdateHandler)(0x198b900)
2023-03-09 06:10:10.290 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.WorkloadEndpointKey: (dispatcher.UpdateHandler)(0x198af20)
2023-03-09 06:10:10.290 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.HostEndpointKey: (dispatcher.UpdateHandler)(0x198af20)
2023-03-09 06:10:10.290 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.PolicyKey: (dispatcher.UpdateHandler)(0x198af20)
2023-03-09 06:10:10.290 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.ProfileRulesKey: (dispatcher.UpdateHandler)(0x198af20)
2023-03-09 06:10:10.290 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.ResourceKey: (dispatcher.UpdateHandler)(0x198af20)
2023-03-09 06:10:10.291 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.ResourceKey: (dispatcher.UpdateHandler)(0x19436a0)
2023-03-09 06:10:10.291 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.ResourceKey: (dispatcher.UpdateHandler)(0x17911a0)
2023-03-09 06:10:10.291 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.WorkloadEndpointKey: (dispatcher.UpdateHandler)(0x17911a0)
2023-03-09 06:10:10.291 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.HostEndpointKey: (dispatcher.UpdateHandler)(0x17911a0)
2023-03-09 06:10:10.291 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.NetworkSetKey: (dispatcher.UpdateHandler)(0x17911a0)
2023-03-09 06:10:10.291 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.PolicyKey: (dispatcher.UpdateHandler)(0x198c3e0)
2023-03-09 06:10:10.291 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.WorkloadEndpointKey: (dispatcher.UpdateHandler)(0x198c3e0)
2023-03-09 06:10:10.291 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.HostEndpointKey: (dispatcher.UpdateHandler)(0x198c3e0)
2023-03-09 06:10:10.292 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.HostIPKey: (dispatcher.UpdateHandler)(0x198bbe0)
2023-03-09 06:10:10.292 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.IPPoolKey: (dispatcher.UpdateHandler)(0x198bbe0)
2023-03-09 06:10:10.292 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.WireguardKey: (dispatcher.UpdateHandler)(0x198bbe0)
2023-03-09 06:10:10.292 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.ResourceKey: (dispatcher.UpdateHandler)(0x198bbe0)
2023-03-09 06:10:10.292 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.GlobalConfigKey: (dispatcher.UpdateHandler)(0x198ba40)
2023-03-09 06:10:10.292 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.HostConfigKey: (dispatcher.UpdateHandler)(0x198ba40)
2023-03-09 06:10:10.292 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.ReadyFlagKey: (dispatcher.UpdateHandler)(0x198ba40)
2023-03-09 06:10:10.293 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.ResourceKey: (dispatcher.UpdateHandler)(0x198b480)
2023-03-09 06:10:10.293 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.IPPoolKey: (dispatcher.UpdateHandler)(0x198b5c0)
2023-03-09 06:10:10.293 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.HostIPKey: (dispatcher.UpdateHandler)(0x198c880)
2023-03-09 06:10:10.293 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.WorkloadEndpointKey: (dispatcher.UpdateHandler)(0x198c880)
2023-03-09 06:10:10.293 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.HostEndpointKey: (dispatcher.UpdateHandler)(0x198c880)
2023-03-09 06:10:10.293 [INFO][65] felix/dispatcher.go 68: Registering listener for type model.HostConfigKey: (dispatcher.UpdateHandler)(0x198c880)
2023-03-09 06:10:10.293 [INFO][65] felix/async_calc_graph.go 255: Starting AsyncCalcGraph
2023-03-09 06:10:10.294 [INFO][65] felix/daemon.go 619: Started the processing graph
2023-03-09 06:10:10.297 [INFO][65] felix/ipip_mgr.go 84: IPIP thread started.
2023-03-09 06:10:10.297 [INFO][65] felix/ipip_mgr.go 105: Failed to get IPIP tunnel device, assuming it isn't present error=Link not found
2023-03-09 06:10:10.301 [INFO][65] felix/int_dataplane.go 1634: Started internal iptables dataplane driver loop
2023-03-09 06:10:10.301 [INFO][65] felix/int_dataplane.go 1644: Will refresh IP sets on timer interval=1m30s
2023-03-09 06:10:10.301 [INFO][65] felix/int_dataplane.go 1654: Will refresh routes on timer interval=1m30s
2023-03-09 06:10:10.301 [INFO][65] felix/int_dataplane.go 1664: Will refresh XDP on timer interval=1m30s
2023-03-09 06:10:10.301 [INFO][65] felix/int_dataplane.go 2110: Started internal status report thread
2023-03-09 06:10:10.301 [INFO][65] felix/int_dataplane.go 2112: Process status reports disabled
2023-03-09 06:10:10.301 [INFO][65] felix/iface_monitor.go 109: Interface monitoring thread started.
2023-03-09 06:10:10.301 [INFO][65] felix/iface_monitor.go 127: Subscribed to netlink updates.
2023-03-09 06:10:10.301 [INFO][65] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=1 ifaceName="lo" state="up"
2023-03-09 06:10:10.301 [INFO][65] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{127.0.0.0,127.0.0.1,::1} ifaceName="lo"
2023-03-09 06:10:10.301 [INFO][65] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=2 ifaceName="eth0" state="up"
2023-03-09 06:10:10.301 [INFO][65] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{139.144.144.82,192.168.129.114,2a01:7e00::f03c:93ff:fe1a:43e2,fe80::f03c:93ff:fe1a:43e2} ifaceName="eth0"
2023-03-09 06:10:10.302 [INFO][65] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=3 ifaceName="docker0" state="down"
2023-03-09 06:10:10.302 [INFO][65] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{172.17.0.1} ifaceName="docker0"
2023-03-09 06:10:10.302 [INFO][65] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=4 ifaceName="wg0" state="up"
2023-03-09 06:10:10.302 [INFO][65] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{172.31.0.1} ifaceName="wg0"
2023-03-09 06:10:10.302 [INFO][65] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"lo", State:"up", Index:1}
2023-03-09 06:10:10.302 [INFO][65] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"eth0", State:"up", Index:2}
2023-03-09 06:10:10.302 [INFO][65] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"docker0", State:"down", Index:3}
2023-03-09 06:10:10.302 [INFO][65] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"wg0", State:"up", Index:4}
2023-03-09 06:10:10.302 [INFO][65] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"lo", Addrs:set.Typed[string]{"127.0.0.0":set.v{}, "127.0.0.1":set.v{}, "::1":set.v{}}}
2023-03-09 06:10:10.302 [INFO][65] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"lo", Addrs:set.Typed[string]{"127.0.0.0":set.v{}, "127.0.0.1":set.v{}, "::1":set.v{}}}
2023-03-09 06:10:10.302 [INFO][65] felix/ipsets.go 130: Queueing IP set for creation family="inet" setID="this-host" setType="hash:ip"
2023-03-09 06:10:10.302 [INFO][65] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"eth0", Addrs:set.Typed[string]{"139.144.144.82":set.v{}, "192.168.129.114":set.v{}, "2a01:7e00::f03c:93ff:fe1a:43e2":set.v{}, "fe80::f03c:93ff:fe1a:43e2":set.v{}}}
2023-03-09 06:10:10.302 [INFO][65] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"eth0", Addrs:set.Typed[string]{"139.144.144.82":set.v{}, "192.168.129.114":set.v{}, "2a01:7e00::f03c:93ff:fe1a:43e2":set.v{}, "fe80::f03c:93ff:fe1a:43e2":set.v{}}}
2023-03-09 06:10:10.302 [INFO][65] felix/ipsets.go 130: Queueing IP set for creation family="inet" setID="this-host" setType="hash:ip"
2023-03-09 06:10:10.302 [INFO][65] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"docker0", Addrs:set.Typed[string]{"172.17.0.1":set.v{}}}
2023-03-09 06:10:10.302 [INFO][65] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"docker0", Addrs:set.Typed[string]{"172.17.0.1":set.v{}}}
2023-03-09 06:10:10.302 [INFO][65] felix/ipsets.go 130: Queueing IP set for creation family="inet" setID="this-host" setType="hash:ip"
2023-03-09 06:10:10.302 [INFO][65] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"wg0", Addrs:set.Typed[string]{"172.31.0.1":set.v{}}}
2023-03-09 06:10:10.302 [INFO][65] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"wg0", Addrs:set.Typed[string]{"172.31.0.1":set.v{}}}
2023-03-09 06:10:10.302 [INFO][65] felix/ipsets.go 130: Queueing IP set for creation family="inet" setID="this-host" setType="hash:ip"
2023-03-09 06:10:10.302 [INFO][65] felix/watchersyncer.go 130: Sending status update Status=wait-for-ready
2023-03-09 06:10:10.303 [INFO][65] felix/watchersyncer.go 149: Starting main event processing loop
2023-03-09 06:10:10.303 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/kubernetesservice"
2023-03-09 06:10:10.303 [INFO][65] felix/async_calc_graph.go 137: AsyncCalcGraph running
2023-03-09 06:10:10.303 [INFO][65] felix/int_dataplane.go 1680: Received *proto.ConfigUpdate update from calculation graph msg=config:<key:"CalicoVersion" value:"v3.24.5" > config:<key:"ClusterGUID" value:"7197fc97702c482f9a5df8fb5c03809a" > config:<key:"ClusterType" value:"k8s,bgp,kubeadm,kdd" > config:<key:"DatastoreType" value:"kubernetes" > config:<key:"DefaultEndpointToHostAction" value:"ACCEPT" > config:<key:"FelixHostname" value:"lke96996-146211-640977a3930b" > config:<key:"FloatingIPs" value:"Disabled" > config:<key:"HealthEnabled" value:"true" > config:<key:"IpInIpMtu" value:"0" > config:<key:"IpInIpTunnelAddr" value:"10.2.0.1" > config:<key:"Ipv6Support" value:"false" > config:<key:"LogFilePath" value:"None" > config:<key:"LogSeverityFile" value:"None" > config:<key:"LogSeverityScreen" value:"Info" > config:<key:"LogSeveritySys" value:"None" > config:<key:"MetadataAddr" value:"None" > config:<key:"ReportingIntervalSecs" value:"0" > config:<key:"VXLANMTU" value:"0" > config:<key:"WireguardMTU" value:"0" > 
2023-03-09 06:10:10.303 [INFO][65] felix/daemon.go 979: Reading from dataplane driver pipe...
2023-03-09 06:10:10.310 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/clusterinformations"
2023-03-09 06:10:10.311 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/felixconfigurations"
2023-03-09 06:10:10.311 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/globalnetworkpolicies"
2023-03-09 06:10:10.311 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/globalnetworksets"
2023-03-09 06:10:10.311 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/ippools"
2023-03-09 06:10:10.311 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/nodes"
2023-03-09 06:10:10.312 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/profiles"
2023-03-09 06:10:10.312 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/workloadendpoints"
2023-03-09 06:10:10.312 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/networkpolicies"
2023-03-09 06:10:10.312 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/networksets"
2023-03-09 06:10:10.312 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/hostendpoints"
2023-03-09 06:10:10.312 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/bgpconfigurations"
2023-03-09 06:10:10.313 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/kubernetesnetworkpolicies"
2023-03-09 06:10:10.313 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/kubernetesendpointslices"
2023-03-09 06:10:10.316 [INFO][65] felix/daemon.go 689: No driver process to monitor
2023-03-09 06:10:10.327 [INFO][65] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/kubernetesservice"
2023-03-09 06:10:10.329 [INFO][65] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/kubernetesendpointslices"
2023-03-09 06:10:10.331 [INFO][65] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/hostendpoints"
2023-03-09 06:10:10.332 [INFO][65] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/networksets"
2023-03-09 06:10:10.332 [INFO][65] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/kubernetesnetworkpolicies"
2023-03-09 06:10:10.333 [INFO][65] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/ippools"
2023-03-09 06:10:10.333 [INFO][65] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/globalnetworksets"
2023-03-09 06:10:10.334 [INFO][65] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/clusterinformations"
2023-03-09 06:10:10.335 [INFO][65] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/bgpconfigurations"
2023-03-09 06:10:10.336 [INFO][65] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/nodes"
2023-03-09 06:10:10.341 [INFO][65] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/globalnetworkpolicies"
2023-03-09 06:10:10.345 [INFO][65] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/felixconfigurations"
2023-03-09 06:10:10.346 [INFO][65] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/networkpolicies"
2023-03-09 06:10:10.348 [INFO][65] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/workloadendpoints"
2023-03-09 06:10:10.348 [INFO][65] felix/watchersyncer.go 130: Sending status update Status=resync
2023-03-09 06:10:10.348 [INFO][65] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:10.348 [INFO][65] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:10.348 [INFO][65] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:10.348 [INFO][65] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:10.348 [INFO][65] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:10.348 [INFO][65] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:10.348 [INFO][65] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:10.348 [INFO][65] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:10.348 [INFO][65] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:10.348 [INFO][65] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:10.348 [INFO][65] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:10.348 [INFO][65] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:10.348 [INFO][65] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:10.348 [INFO][65] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:10.350 [INFO][65] felix/config_batcher.go 74: Global config update: {{GlobalFelixConfig(name=ClusterGUID) 7197fc97702c482f9a5df8fb5c03809a 679 <nil> 0s} 1}
2023-03-09 06:10:10.350 [INFO][65] felix/config_batcher.go 74: Global config update: {{GlobalFelixConfig(name=ClusterType) k8s,bgp,kubeadm,kdd 679 <nil> 0s} 1}
2023-03-09 06:10:10.350 [INFO][65] felix/config_batcher.go 74: Global config update: {{GlobalFelixConfig(name=CalicoVersion) v3.24.5 679 <nil> 0s} 1}
2023-03-09 06:10:10.350 [INFO][65] felix/config_batcher.go 61: Host config update for this host: {{HostConfig(node=lke96996-146211-640977a3930b,name=IpInIpTunnelAddr) 10.2.0.1 681 <nil> 0s} 1}
2023-03-09 06:10:10.350 [INFO][65] felix/config_batcher.go 74: Global config update: {{GlobalFelixConfig(name=LogSeverityScreen) Info 680 <nil> 0s} 1}
2023-03-09 06:10:10.350 [INFO][65] felix/config_batcher.go 74: Global config update: {{GlobalFelixConfig(name=ReportingIntervalSecs) 0 680 <nil> 0s} 1}
2023-03-09 06:10:10.350 [INFO][65] felix/config_batcher.go 74: Global config update: {{GlobalFelixConfig(name=FloatingIPs) Disabled 680 <nil> 0s} 1}
2023-03-09 06:10:10.350 [INFO][65] felix/int_dataplane.go 1680: Received *proto.HostMetadataUpdate update from calculation graph msg=hostname:"lke96996-146211-640977a3930b" ipv4_addr:"192.168.129.114" 
2023-03-09 06:10:10.351 [INFO][65] felix/int_dataplane.go 1680: Received *proto.HostMetadataUpdate update from calculation graph msg=hostname:"lke96996-146211-640977a3e82e" ipv4_addr:"139.144.144.93" 
2023-03-09 06:10:10.351 [INFO][65] felix/int_dataplane.go 1680: Received *proto.IPAMPoolUpdate update from calculation graph msg=id:"10.2.0.0-16" pool:<cidr:"10.2.0.0/16" masquerade:true > 
2023-03-09 06:10:10.351 [INFO][65] felix/int_dataplane.go 1680: Received *proto.ServiceUpdate update from calculation graph msg=name:"kubernetes" namespace:"default" type:"ClusterIP" cluster_ip:"10.128.0.1" 
2023-03-09 06:10:10.351 [INFO][65] felix/int_dataplane.go 1680: Received *proto.ServiceUpdate update from calculation graph msg=name:"kube-dns" namespace:"kube-system" type:"ClusterIP" cluster_ip:"10.128.0.10" 
2023-03-09 06:10:10.371 [WARNING][65] felix/ipip_mgr.go 111: Failed to add IPIP tunnel device error=exit status 1
2023-03-09 06:10:10.371 [WARNING][65] felix/ipip_mgr.go 88: Failed configure IPIP tunnel device, retrying... error=exit status 1
2023-03-09 06:10:10.376 [INFO][65] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/profiles"
2023-03-09 06:10:10.376 [INFO][65] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:10.376 [INFO][65] felix/watchersyncer.go 221: All watchers have sync'd data - sending data and final sync
2023-03-09 06:10:10.376 [INFO][65] felix/watchersyncer.go 130: Sending status update Status=in-sync
2023-03-09 06:10:10.378 [INFO][65] felix/config_batcher.go 102: Datamodel in sync, flushing config update
2023-03-09 06:10:10.378 [INFO][65] felix/config_batcher.go 112: Sending config update global: map[CalicoVersion:v3.24.5 ClusterGUID:7197fc97702c482f9a5df8fb5c03809a ClusterType:k8s,bgp,kubeadm,kdd FloatingIPs:Disabled LogSeverityScreen:Info ReportingIntervalSecs:0], host: map[IpInIpTunnelAddr:10.2.0.1].
2023-03-09 06:10:10.378 [INFO][65] felix/async_calc_graph.go 166: First time we've been in sync
2023-03-09 06:10:10.378 [INFO][65] felix/health.go 137: Health of component changed lastReport=health.HealthReport{Live:true, Ready:false, Detail:""} name="async_calc_graph" newReport=&health.HealthReport{Live:true, Ready:true, Detail:""}
2023-03-09 06:10:10.378 [INFO][65] felix/event_sequencer.go 259: Possible config update. global=map[string]string{"CalicoVersion":"v3.24.5", "ClusterGUID":"7197fc97702c482f9a5df8fb5c03809a", "ClusterType":"k8s,bgp,kubeadm,kdd", "FloatingIPs":"Disabled", "LogSeverityScreen":"Info", "ReportingIntervalSecs":"0"} host=map[string]string{"IpInIpTunnelAddr":"10.2.0.1"}
2023-03-09 06:10:10.378 [INFO][65] felix/config_params.go 435: Merging in config from datastore (global): map[CalicoVersion:v3.24.5 ClusterGUID:7197fc97702c482f9a5df8fb5c03809a ClusterType:k8s,bgp,kubeadm,kdd FloatingIPs:Disabled LogSeverityScreen:Info ReportingIntervalSecs:0]
2023-03-09 06:10:10.378 [INFO][65] felix/config_params.go 542: Parsing value for DefaultEndpointToHostAction: ACCEPT (from environment variable)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 578: Parsed value for DefaultEndpointToHostAction: ACCEPT (from environment variable)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 542: Parsing value for HealthEnabled: true (from environment variable)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 578: Parsed value for HealthEnabled: true (from environment variable)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 542: Parsing value for WireguardMTU: 0 (from environment variable)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 578: Parsed value for WireguardMTU: 0 (from environment variable)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 542: Parsing value for VXLANMTU: 0 (from environment variable)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 578: Parsed value for VXLANMTU: 0 (from environment variable)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 542: Parsing value for FelixHostname: lke96996-146211-640977a3930b (from environment variable)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 578: Parsed value for FelixHostname: lke96996-146211-640977a3930b (from environment variable)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 542: Parsing value for Ipv6Support: false (from environment variable)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 578: Parsed value for Ipv6Support: false (from environment variable)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 542: Parsing value for DatastoreType: kubernetes (from environment variable)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 578: Parsed value for DatastoreType: kubernetes (from environment variable)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 542: Parsing value for IpInIpMtu: 0 (from environment variable)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 578: Parsed value for IpInIpMtu: 0 (from environment variable)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 542: Parsing value for LogSeverityFile: None (from config file)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 559: Value set to 'none', replacing with zero-value: "".
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 578: Parsed value for LogSeverityFile:  (from config file)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 542: Parsing value for LogSeveritySys: None (from config file)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 559: Value set to 'none', replacing with zero-value: "".
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 578: Parsed value for LogSeveritySys:  (from config file)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 542: Parsing value for MetadataAddr: None (from config file)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 559: Value set to 'none', replacing with zero-value: "".
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 578: Parsed value for MetadataAddr:  (from config file)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 542: Parsing value for LogFilePath: None (from config file)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 559: Value set to 'none', replacing with zero-value: "".
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 578: Parsed value for LogFilePath:  (from config file)
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 542: Parsing value for IpInIpTunnelAddr: 10.2.0.1 (from datastore (per-host))
2023-03-09 06:10:10.379 [INFO][65] felix/config_params.go 578: Parsed value for IpInIpTunnelAddr: 10.2.0.1 (from datastore (per-host))
2023-03-09 06:10:10.380 [INFO][65] felix/config_params.go 542: Parsing value for CalicoVersion: v3.24.5 (from datastore (global))
2023-03-09 06:10:10.380 [INFO][65] felix/config_params.go 578: Parsed value for CalicoVersion: v3.24.5 (from datastore (global))
2023-03-09 06:10:10.380 [INFO][65] felix/config_params.go 542: Parsing value for LogSeverityScreen: Info (from datastore (global))
2023-03-09 06:10:10.380 [INFO][65] felix/config_params.go 578: Parsed value for LogSeverityScreen: INFO (from datastore (global))
2023-03-09 06:10:10.380 [INFO][65] felix/config_params.go 542: Parsing value for ReportingIntervalSecs: 0 (from datastore (global))
2023-03-09 06:10:10.380 [INFO][65] felix/config_params.go 578: Parsed value for ReportingIntervalSecs: 0s (from datastore (global))
2023-03-09 06:10:10.380 [INFO][65] felix/config_params.go 542: Parsing value for FloatingIPs: Disabled (from datastore (global))
2023-03-09 06:10:10.380 [INFO][65] felix/config_params.go 578: Parsed value for FloatingIPs: Disabled (from datastore (global))
2023-03-09 06:10:10.380 [INFO][65] felix/config_params.go 542: Parsing value for ClusterGUID: 7197fc97702c482f9a5df8fb5c03809a (from datastore (global))
2023-03-09 06:10:10.380 [INFO][65] felix/config_params.go 578: Parsed value for ClusterGUID: 7197fc97702c482f9a5df8fb5c03809a (from datastore (global))
2023-03-09 06:10:10.380 [INFO][65] felix/config_params.go 542: Parsing value for ClusterType: k8s,bgp,kubeadm,kdd (from datastore (global))
2023-03-09 06:10:10.380 [INFO][65] felix/config_params.go 578: Parsed value for ClusterType: k8s,bgp,kubeadm,kdd (from datastore (global))
2023-03-09 06:10:10.380 [INFO][65] felix/config_params.go 435: Merging in config from datastore (per-host): map[IpInIpTunnelAddr:10.2.0.1]
2023-03-09 06:10:10.380 [INFO][65] felix/config_params.go 542: Parsing value for Ipv6Support: false (from environment variable)
2023-03-09 06:10:10.380 [INFO][65] felix/config_params.go 578: Parsed value for Ipv6Support: false (from environment variable)
2023-03-09 06:10:10.380 [INFO][65] felix/config_params.go 542: Parsing value for DatastoreType: kubernetes (from environment variable)
2023-03-09 06:10:10.380 [INFO][65] felix/config_params.go 578: Parsed value for DatastoreType: kubernetes (from environment variable)
2023-03-09 06:10:10.380 [INFO][65] felix/config_params.go 542: Parsing value for IpInIpMtu: 0 (from environment variable)
2023-03-09 06:10:10.381 [INFO][65] felix/config_params.go 578: Parsed value for IpInIpMtu: 0 (from environment variable)
2023-03-09 06:10:10.381 [INFO][65] felix/config_params.go 542: Parsing value for DefaultEndpointToHostAction: ACCEPT (from environment variable)
2023-03-09 06:10:10.381 [INFO][65] felix/config_params.go 578: Parsed value for DefaultEndpointToHostAction: ACCEPT (from environment variable)
2023-03-09 06:10:10.381 [INFO][65] felix/config_params.go 542: Parsing value for HealthEnabled: true (from environment variable)
2023-03-09 06:10:10.381 [INFO][65] felix/config_params.go 578: Parsed value for HealthEnabled: true (from environment variable)
2023-03-09 06:10:10.381 [INFO][65] felix/config_params.go 542: Parsing value for WireguardMTU: 0 (from environment variable)
2023-03-09 06:10:10.381 [INFO][65] felix/config_params.go 578: Parsed value for WireguardMTU: 0 (from environment variable)
2023-03-09 06:10:10.381 [INFO][65] felix/config_params.go 542: Parsing value for VXLANMTU: 0 (from environment variable)
2023-03-09 06:10:10.381 [INFO][65] felix/config_params.go 578: Parsed value for VXLANMTU: 0 (from environment variable)
2023-03-09 06:10:10.381 [INFO][65] felix/config_params.go 542: Parsing value for FelixHostname: lke96996-146211-640977a3930b (from environment variable)
2023-03-09 06:10:10.381 [INFO][65] felix/config_params.go 578: Parsed value for FelixHostname: lke96996-146211-640977a3930b (from environment variable)
2023-03-09 06:10:10.381 [INFO][65] felix/config_params.go 542: Parsing value for LogSeverityFile: None (from config file)
2023-03-09 06:10:10.381 [INFO][65] felix/config_params.go 559: Value set to 'none', replacing with zero-value: "".
2023-03-09 06:10:10.381 [INFO][65] felix/config_params.go 578: Parsed value for LogSeverityFile:  (from config file)
2023-03-09 06:10:10.381 [INFO][65] felix/config_params.go 542: Parsing value for LogSeveritySys: None (from config file)
2023-03-09 06:10:10.381 [INFO][65] felix/config_params.go 559: Value set to 'none', replacing with zero-value: "".
2023-03-09 06:10:10.381 [INFO][65] felix/config_params.go 578: Parsed value for LogSeveritySys:  (from config file)
2023-03-09 06:10:10.382 [INFO][65] felix/config_params.go 542: Parsing value for MetadataAddr: None (from config file)
2023-03-09 06:10:10.382 [INFO][65] felix/config_params.go 559: Value set to 'none', replacing with zero-value: "".
2023-03-09 06:10:10.382 [INFO][65] felix/config_params.go 578: Parsed value for MetadataAddr:  (from config file)
2023-03-09 06:10:10.382 [INFO][65] felix/config_params.go 542: Parsing value for LogFilePath: None (from config file)
2023-03-09 06:10:10.382 [INFO][65] felix/config_params.go 559: Value set to 'none', replacing with zero-value: "".
2023-03-09 06:10:10.382 [INFO][65] felix/config_params.go 578: Parsed value for LogFilePath:  (from config file)
2023-03-09 06:10:10.382 [INFO][65] felix/config_params.go 542: Parsing value for IpInIpTunnelAddr: 10.2.0.1 (from datastore (per-host))
2023-03-09 06:10:10.382 [INFO][65] felix/config_params.go 578: Parsed value for IpInIpTunnelAddr: 10.2.0.1 (from datastore (per-host))
2023-03-09 06:10:10.382 [INFO][65] felix/config_params.go 542: Parsing value for LogSeverityScreen: Info (from datastore (global))
2023-03-09 06:10:10.382 [INFO][65] felix/config_params.go 578: Parsed value for LogSeverityScreen: INFO (from datastore (global))
2023-03-09 06:10:10.382 [INFO][65] felix/config_params.go 542: Parsing value for ReportingIntervalSecs: 0 (from datastore (global))
2023-03-09 06:10:10.382 [INFO][65] felix/config_params.go 578: Parsed value for ReportingIntervalSecs: 0s (from datastore (global))
2023-03-09 06:10:10.382 [INFO][65] felix/config_params.go 542: Parsing value for FloatingIPs: Disabled (from datastore (global))
2023-03-09 06:10:10.382 [INFO][65] felix/config_params.go 578: Parsed value for FloatingIPs: Disabled (from datastore (global))
2023-03-09 06:10:10.382 [INFO][65] felix/config_params.go 542: Parsing value for ClusterGUID: 7197fc97702c482f9a5df8fb5c03809a (from datastore (global))
2023-03-09 06:10:10.383 [INFO][65] felix/config_params.go 578: Parsed value for ClusterGUID: 7197fc97702c482f9a5df8fb5c03809a (from datastore (global))
2023-03-09 06:10:10.383 [INFO][65] felix/config_params.go 542: Parsing value for ClusterType: k8s,bgp,kubeadm,kdd (from datastore (global))
2023-03-09 06:10:10.383 [INFO][65] felix/config_params.go 578: Parsed value for ClusterType: k8s,bgp,kubeadm,kdd (from datastore (global))
2023-03-09 06:10:10.383 [INFO][65] felix/config_params.go 542: Parsing value for CalicoVersion: v3.24.5 (from datastore (global))
2023-03-09 06:10:10.383 [INFO][65] felix/config_params.go 578: Parsed value for CalicoVersion: v3.24.5 (from datastore (global))
2023-03-09 06:10:10.384 [INFO][65] felix/usagerep.go 91: Waiting before first check-in delay=5m1.957s
2023-03-09 06:10:10.384 [INFO][65] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"disruption-controller" > labels:<key:"projectcalico.org/name" value:"disruption-controller" > 
2023-03-09 06:10:10.385 [INFO][65] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"resourcequota-controller" > labels:<key:"projectcalico.org/name" value:"resourcequota-controller" > 
2023-03-09 06:10:10.385 [INFO][65] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"token-cleaner" > labels:<key:"projectcalico.org/name" value:"token-cleaner" > 
2023-03-09 06:10:10.385 [INFO][65] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"ttl-controller" > labels:<key:"projectcalico.org/name" value:"ttl-controller" > 
2023-03-09 06:10:10.385 [INFO][65] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"endpointslicemirroring-controller" > labels:<key:"projectcalico.org/name" value:"endpointslicemirroring-controller" > 
2023-03-09 06:10:10.385 [INFO][65] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"calico-node" > labels:<key:"projectcalico.org/name" value:"calico-node" > 
2023-03-09 06:10:10.385 [INFO][65] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"clusterrole-aggregation-controller" > labels:<key:"projectcalico.org/name" value:"clusterrole-aggregation-controller" > 
2023-03-09 06:10:10.385 [INFO][65] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"default" > labels:<key:"projectcalico.org/name" value:"default" > 
2023-03-09 06:10:10.385 [INFO][65] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"deployment-controller" > labels:<key:"projectcalico.org/name" value:"deployment-controller" > 
... dropped 42 logs ...
2023-03-09 06:10:10.390 [INFO][65] felix/int_dataplane.go 1680: Received *proto.NamespaceUpdate update from calculation graph msg=id:<name:"kube-system" > labels:<key:"kubernetes.io/metadata.name" value:"kube-system" > labels:<key:"projectcalico.org/name" value:"kube-system" > 
2023-03-09 06:10:10.390 [INFO][65] felix/int_dataplane.go 1680: Received *proto.Encapsulation update from calculation graph msg=ipip_enabled:true 
2023-03-09 06:10:10.390 [INFO][65] felix/int_dataplane.go 1680: Received *proto.InSync update from calculation graph msg=
2023-03-09 06:10:10.390 [INFO][65] felix/int_dataplane.go 1688: Datastore in sync, flushing the dataplane for the first time... timeSinceStart=392.687214ms
2023-03-09 06:10:10.390 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-from-wl-dispatch" ipVersion=0x4 table="filter"
2023-03-09 06:10:10.390 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-to-wl-dispatch" ipVersion=0x4 table="filter"
2023-03-09 06:10:10.390 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-from-host-endpoint" ipVersion=0x4 table="raw"
2023-03-09 06:10:10.390 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-to-host-endpoint" ipVersion=0x4 table="raw"
2023-03-09 06:10:10.390 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-from-host-endpoint" ipVersion=0x4 table="filter"
2023-03-09 06:10:10.390 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-to-host-endpoint" ipVersion=0x4 table="filter"
2023-03-09 06:10:10.390 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-from-hep-forward" ipVersion=0x4 table="filter"
2023-03-09 06:10:10.390 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-to-hep-forward" ipVersion=0x4 table="filter"
2023-03-09 06:10:10.390 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-from-host-endpoint" ipVersion=0x4 table="mangle"
2023-03-09 06:10:10.390 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-to-host-endpoint" ipVersion=0x4 table="mangle"
2023-03-09 06:10:10.390 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-rpf-skip" ipVersion=0x4 table="raw"
2023-03-09 06:10:10.390 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-fip-dnat" ipVersion=0x4 table="nat"
2023-03-09 06:10:10.390 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-fip-snat" ipVersion=0x4 table="nat"
2023-03-09 06:10:10.390 [INFO][65] felix/masq_mgr.go 145: IPAM pools updated, refreshing iptables rule ipVersion=0x4
2023-03-09 06:10:10.390 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-nat-outgoing" ipVersion=0x4 table="nat"
2023-03-09 06:10:10.390 [INFO][65] felix/ipip_mgr.go 221: All-hosts IP set out-of sync, refreshing it.
2023-03-09 06:10:10.390 [INFO][65] felix/ipsets.go 130: Queueing IP set for creation family="inet" setID="all-hosts-net" setType="hash:net"
2023-03-09 06:10:10.390 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-cidr-block" ipVersion=0x4 table="filter"
2023-03-09 06:10:10.442 [INFO][65] felix/wireguard.go 1701: Trying to connect to linkClient ipVersion=0x4
2023-03-09 06:10:10.444 [INFO][65] felix/route_rule.go 189: Trying to connect to netlink
2023-03-09 06:10:10.444 [INFO][65] felix/wireguard.go 632: Public key out of sync or updated ipVersion=0x4 ourPublicKey=AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
2023-03-09 06:10:10.445 [INFO][65] felix/ipsets.go 779: Doing full IP set rewrite family="inet" numMembersInPendingReplace=6 setID="this-host"
2023-03-09 06:10:10.445 [INFO][65] felix/ipsets.go 779: Doing full IP set rewrite family="inet" numMembersInPendingReplace=2 setID="all-hosts-net"
2023-03-09 06:10:10.445 [INFO][65] felix/ipsets.go 779: Doing full IP set rewrite family="inet" numMembersInPendingReplace=1 setID="all-ipam-pools"
2023-03-09 06:10:10.446 [INFO][65] felix/ipsets.go 779: Doing full IP set rewrite family="inet" numMembersInPendingReplace=1 setID="masq-ipam-pools"
2023-03-09 06:10:10.472 [INFO][65] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=5 ifaceName="tunl0" state="down"
2023-03-09 06:10:10.472 [INFO][65] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{} ifaceName="tunl0"
2023-03-09 06:10:10.534 [INFO][65] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=6 ifaceName="calico_tmp_B" state="down"
2023-03-09 06:10:10.535 [INFO][65] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{} ifaceName="calico_tmp_B"
2023-03-09 06:10:10.535 [INFO][65] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=6 ifaceName="calico_tmp_B" state=""
2023-03-09 06:10:10.535 [INFO][65] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=<nil> ifaceName="calico_tmp_B"
2023-03-09 06:10:10.535 [INFO][65] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=7 ifaceName="calico_tmp_A" state="down"
2023-03-09 06:10:10.535 [INFO][65] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{} ifaceName="calico_tmp_A"
2023-03-09 06:10:10.535 [INFO][65] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=7 ifaceName="calico_tmp_A" state=""
2023-03-09 06:10:10.535 [INFO][65] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=<nil> ifaceName="calico_tmp_A"
2023-03-09 06:10:10.542 [INFO][65] felix/int_dataplane.go 1828: Completed first update to dataplane. secsSinceStart=0.545257899
2023-03-09 06:10:10.546 [INFO][65] felix/health.go 137: Health of component changed lastReport=health.HealthReport{Live:true, Ready:false, Detail:""} name="int_dataplane" newReport=&health.HealthReport{Live:true, Ready:true, Detail:""}
2023-03-09 06:10:10.546 [INFO][65] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"tunl0", Addrs:set.Typed[string]{}}
2023-03-09 06:10:10.546 [INFO][65] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"tunl0", Addrs:set.Typed[string]{}}
2023-03-09 06:10:10.546 [INFO][65] felix/ipsets.go 130: Queueing IP set for creation family="inet" setID="this-host" setType="hash:ip"
2023-03-09 06:10:10.546 [INFO][65] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"calico_tmp_B", Addrs:set.Typed[string]{}}
2023-03-09 06:10:10.546 [INFO][65] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"calico_tmp_B", Addrs:set.Typed[string]{}}
2023-03-09 06:10:10.546 [INFO][65] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"calico_tmp_B", Addrs:set.Set[string](nil)}
2023-03-09 06:10:10.546 [INFO][65] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"calico_tmp_B", Addrs:set.Set[string](nil)}
2023-03-09 06:10:10.546 [INFO][65] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"calico_tmp_A", Addrs:set.Typed[string]{}}
2023-03-09 06:10:10.546 [INFO][65] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"calico_tmp_A", Addrs:set.Typed[string]{}}
2023-03-09 06:10:10.547 [INFO][65] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"calico_tmp_A", Addrs:set.Set[string](nil)}
2023-03-09 06:10:10.547 [INFO][65] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"calico_tmp_A", Addrs:set.Set[string](nil)}
2023-03-09 06:10:10.547 [INFO][65] felix/int_dataplane.go 1838: Dataplane updates throttled
2023-03-09 06:10:10.547 [INFO][65] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"tunl0", State:"down", Index:5}
2023-03-09 06:10:10.547 [INFO][65] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"calico_tmp_B", State:"down", Index:6}
2023-03-09 06:10:10.547 [INFO][65] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"calico_tmp_B", State:"", Index:6}
2023-03-09 06:10:10.547 [INFO][65] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"calico_tmp_A", State:"down", Index:7}
2023-03-09 06:10:10.547 [INFO][65] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"calico_tmp_A", State:"", Index:7}
2023-03-09 06:10:10.548 [INFO][65] felix/ipsets.go 779: Doing full IP set rewrite family="inet" numMembersInPendingReplace=6 setID="this-host"
2023-03-09 06:10:10.617 [INFO][66] confd/client.go 995: Recompute BGP peerings: lke96996-146211-640977a3e82e updated
2023-03-09 06:10:10.627 [INFO][65] felix/int_dataplane.go 1680: Received *proto.HostMetadataUpdate update from calculation graph msg=hostname:"lke96996-146211-640977a3e82e" ipv4_addr:"192.168.173.115" 
2023-03-09 06:10:10.634 [INFO][65] felix/ipip_mgr.go 221: All-hosts IP set out-of sync, refreshing it.
2023-03-09 06:10:10.634 [INFO][65] felix/ipsets.go 130: Queueing IP set for creation family="inet" setID="all-hosts-net" setType="hash:net"
2023-03-09 06:10:10.634 [INFO][65] felix/ipsets.go 779: Doing full IP set rewrite family="inet" numMembersInPendingReplace=2 setID="all-hosts-net"
2023-03-09 06:10:11.373 [INFO][65] felix/ipip_mgr.go 132: Tunnel wasn't admin up, enabling it flags=0 mtu=1480 tunnelAddr=10.2.0.1
2023-03-09 06:10:11.373 [INFO][65] felix/ipip_mgr.go 137: Set tunnel admin up mtu=1480 tunnelAddr=10.2.0.1
2023-03-09 06:10:11.373 [INFO][65] felix/ipip_mgr.go 182: Address wasn't present, adding it. addr=10.2.0.1 link="tunl0"
2023-03-09 06:10:11.373 [INFO][65] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=5 ifaceName="tunl0" state="up"
2023-03-09 06:10:11.373 [INFO][65] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{10.2.0.1} ifaceName="tunl0"
2023-03-09 06:10:11.373 [INFO][65] felix/iface_monitor.go 217: Netlink address update for known interface.  addr="10.2.0.1" exists=true ifIndex=5
2023-03-09 06:10:11.374 [INFO][65] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"tunl0", State:"up", Index:5}
2023-03-09 06:10:11.374 [INFO][65] felix/int_dataplane.go 1805: Dataplane updates no longer throttled
2023-03-09 06:10:11.374 [INFO][65] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"tunl0", Addrs:set.Typed[string]{"10.2.0.1":set.v{}}}
2023-03-09 06:10:11.374 [INFO][65] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"tunl0", Addrs:set.Typed[string]{"10.2.0.1":set.v{}}}
2023-03-09 06:10:11.374 [INFO][65] felix/ipsets.go 130: Queueing IP set for creation family="inet" setID="this-host" setType="hash:ip"
2023-03-09 06:10:11.375 [INFO][65] felix/ipsets.go 779: Doing full IP set rewrite family="inet" numMembersInPendingReplace=7 setID="this-host"
bird: bird: device1: Initializing
bird: direct1: Initializing
bird: device1: Starting
bird: device1: Connected to table master
bird: device1: State changed to feed
bird: direct1: Starting
bird: direct1: Connected to table master
bird: direct1: State changed to feed
bird: Graceful restart started
bird: Graceful restart done
bird: Started
bird: device1: State changed to up
bird: direct1: State changed to up
device1: Initializing
bird: direct1: Initializing
bird: device1: Starting
bird: device1: Connected to table master
bird: device1: State changed to feed
bird: direct1: Starting
bird: direct1: Connected to table master
bird: direct1: State changed to feed
bird: Graceful restart started
bird: Graceful restart done
bird: Started
bird: device1: State changed to up
bird: direct1: State changed to up
2023-03-09 06:10:11.741 [INFO][66] confd/client.go 995: Recompute BGP peerings: lke96996-146211-640977a3e82e updated
2023-03-09 06:10:13.017 [INFO][65] felix/calc_graph.go 462: Local endpoint updated id=WorkloadEndpoint(node=lke96996-146211-640977a3930b, orchestrator=k8s, workload=kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl, name=eth0)
2023-03-09 06:10:13.018 [INFO][65] felix/int_dataplane.go 1680: Received *proto.ActiveProfileUpdate update from calculation graph msg=id:<name:"kns.kube-system" > profile:<inbound_rules:<action:"allow" rule_id:"YlN-KJA93B7Fe2ih" > outbound_rules:<action:"allow" rule_id:"GUM3x2IgYhIM5cBY" > > 
2023-03-09 06:10:13.021 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-pri-kns.kube-system" ipVersion=0x4 table="filter"
2023-03-09 06:10:13.021 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-pro-kns.kube-system" ipVersion=0x4 table="filter"
2023-03-09 06:10:13.022 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-pro-kns.kube-system" ipVersion=0x4 table="mangle"
2023-03-09 06:10:13.022 [INFO][65] felix/int_dataplane.go 1680: Received *proto.ActiveProfileUpdate update from calculation graph msg=id:<name:"ksa.kube-system.calico-kube-controllers" > profile:<> 
2023-03-09 06:10:13.022 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-pri-_PTRGc0U-L5Kz7V6ERW" ipVersion=0x4 table="filter"
2023-03-09 06:10:13.022 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-pro-_PTRGc0U-L5Kz7V6ERW" ipVersion=0x4 table="filter"
2023-03-09 06:10:13.023 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-pro-_PTRGc0U-L5Kz7V6ERW" ipVersion=0x4 table="mangle"
2023-03-09 06:10:13.023 [INFO][65] felix/int_dataplane.go 1680: Received *proto.WorkloadEndpointUpdate update from calculation graph msg=id:<orchestrator_id:"k8s" workload_id:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl" endpoint_id:"eth0" > endpoint:<state:"active" name:"cali8f354ab7321" profile_ids:"kns.kube-system" profile_ids:"ksa.kube-system.calico-kube-controllers" ipv4_nets:"10.2.0.2/32" > 
2023-03-09 06:10:13.025 [INFO][65] felix/endpoint_mgr.go 600: Updating per-endpoint chains. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"}
2023-03-09 06:10:13.027 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-tw-cali8f354ab7321" ipVersion=0x4 table="filter"
2023-03-09 06:10:13.027 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-pri-kns.kube-system"
2023-03-09 06:10:13.027 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-pri-_PTRGc0U-L5Kz7V6ERW"
2023-03-09 06:10:13.027 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-fw-cali8f354ab7321" ipVersion=0x4 table="filter"
2023-03-09 06:10:13.027 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-pro-kns.kube-system"
2023-03-09 06:10:13.028 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-pro-_PTRGc0U-L5Kz7V6ERW"
2023-03-09 06:10:13.028 [INFO][65] felix/endpoint_mgr.go 646: Updating endpoint routes. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"}
2023-03-09 06:10:13.039 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-from-wl-dispatch" ipVersion=0x4 table="filter"
2023-03-09 06:10:13.039 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-fw-cali8f354ab7321"
2023-03-09 06:10:13.039 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-to-wl-dispatch" ipVersion=0x4 table="filter"
2023-03-09 06:10:13.039 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-tw-cali8f354ab7321"
2023-03-09 06:10:13.039 [INFO][65] felix/endpoint_mgr.go 1280: Skipping configuration of interface because it is oper down. ifaceName="cali8f354ab7321"
2023-03-09 06:10:13.039 [INFO][65] felix/endpoint_mgr.go 488: Re-evaluated workload endpoint status adminUp=true failed=false known=true operUp=false status="down" workloadEndpointID=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"}
2023-03-09 06:10:13.039 [INFO][65] felix/status_combiner.go 58: Storing endpoint status update ipVersion=0x4 status="down" workload=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"}
2023-03-09 06:10:13.092 [INFO][65] felix/status_combiner.go 78: Endpoint down for at least one IP version id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"} ipVersion=0x4 status="down"
2023-03-09 06:10:13.092 [INFO][65] felix/status_combiner.go 98: Reporting combined status. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"} status="down"
2023-03-09 06:10:13.115 [INFO][65] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=8 ifaceName="cali8f354ab7321" state="down"
2023-03-09 06:10:13.115 [INFO][65] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{} ifaceName="cali8f354ab7321"
2023-03-09 06:10:13.115 [INFO][65] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=8 ifaceName="cali8f354ab7321" state="up"
2023-03-09 06:10:13.115 [INFO][65] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"cali8f354ab7321", State:"down", Index:8}
2023-03-09 06:10:13.115 [INFO][65] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"cali8f354ab7321", State:"up", Index:8}
2023-03-09 06:10:13.115 [INFO][65] felix/endpoint_mgr.go 372: Workload interface came up, marking for reconfiguration. ifaceName="cali8f354ab7321"
2023-03-09 06:10:13.115 [INFO][65] felix/endpoint_mgr.go 429: Workload interface state changed; marking for status update. ifaceName="cali8f354ab7321"
2023-03-09 06:10:13.115 [INFO][65] felix/endpoint_mgr.go 1212: Applying /proc/sys configuration to interface. ifaceName="cali8f354ab7321"
2023-03-09 06:10:13.115 [INFO][65] felix/endpoint_mgr.go 488: Re-evaluated workload endpoint status adminUp=true failed=false known=true operUp=true status="up" workloadEndpointID=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"}
2023-03-09 06:10:13.115 [INFO][65] felix/status_combiner.go 58: Storing endpoint status update ipVersion=0x4 status="up" workload=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"}
2023-03-09 06:10:13.116 [INFO][65] felix/status_combiner.go 81: Endpoint up for at least one IP version id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"} ipVersion=0x4 status="up"
2023-03-09 06:10:13.116 [INFO][65] felix/status_combiner.go 98: Reporting combined status. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"} status="up"
2023-03-09 06:10:13.116 [INFO][65] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"cali8f354ab7321", Addrs:set.Typed[string]{}}
2023-03-09 06:10:13.116 [INFO][65] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"cali8f354ab7321", Addrs:set.Typed[string]{}}
2023-03-09 06:10:13.952 [INFO][65] felix/health.go 242: Overall health status changed newStatus=&health.HealthReport{Live:true, Ready:true, Detail:"+------------------+---------+----------------+-----------------+--------+\n|    COMPONENT     | TIMEOUT |    LIVENESS    |    READINESS    | DETAIL |\n+------------------+---------+----------------+-----------------+--------+\n| async_calc_graph | 20s     | reporting live | reporting ready |        |\n| felix-startup    | 0s      | reporting live | reporting ready |        |\n| int_dataplane    | 1m30s   | reporting live | reporting ready |        |\n+------------------+---------+----------------+-----------------+--------+"}
2023-03-09 06:10:14.975 [INFO][65] felix/iface_monitor.go 217: Netlink address update for known interface.  addr="fe80::ecee:eeff:feee:eeee" exists=true ifIndex=8
2023-03-09 06:10:14.975 [INFO][65] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{fe80::ecee:eeff:feee:eeee} ifaceName="cali8f354ab7321"
2023-03-09 06:10:14.975 [INFO][65] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"cali8f354ab7321", Addrs:set.Typed[string]{"fe80::ecee:eeff:feee:eeee":set.v{}}}
2023-03-09 06:10:14.976 [INFO][65] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"cali8f354ab7321", Addrs:set.Typed[string]{"fe80::ecee:eeff:feee:eeee":set.v{}}}
2023-03-09 06:10:16.314 [INFO][65] felix/calc_graph.go 462: Local endpoint updated id=WorkloadEndpoint(node=lke96996-146211-640977a3930b, orchestrator=k8s, workload=kube-system/coredns-5c64b647bf-gkmxq, name=eth0)
2023-03-09 06:10:16.317 [INFO][65] felix/calc_graph.go 462: Local endpoint updated id=WorkloadEndpoint(node=lke96996-146211-640977a3930b, orchestrator=k8s, workload=kube-system/coredns-5c64b647bf-kxh68, name=eth0)
2023-03-09 06:10:16.319 [INFO][65] felix/int_dataplane.go 1680: Received *proto.ActiveProfileUpdate update from calculation graph msg=id:<name:"ksa.kube-system.coredns" > profile:<> 
2023-03-09 06:10:16.323 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-pri-_u2Tn2rSoAPffvE7JO6" ipVersion=0x4 table="filter"
2023-03-09 06:10:16.323 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-pro-_u2Tn2rSoAPffvE7JO6" ipVersion=0x4 table="filter"
2023-03-09 06:10:16.323 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-pro-_u2Tn2rSoAPffvE7JO6" ipVersion=0x4 table="mangle"
2023-03-09 06:10:16.324 [INFO][65] felix/int_dataplane.go 1680: Received *proto.WorkloadEndpointUpdate update from calculation graph msg=id:<orchestrator_id:"k8s" workload_id:"kube-system/coredns-5c64b647bf-gkmxq" endpoint_id:"eth0" > endpoint:<state:"active" name:"cali83fdaf4c216" profile_ids:"kns.kube-system" profile_ids:"ksa.kube-system.coredns" ipv4_nets:"10.2.0.4/32" > 
2023-03-09 06:10:16.324 [INFO][65] felix/int_dataplane.go 1680: Received *proto.WorkloadEndpointUpdate update from calculation graph msg=id:<orchestrator_id:"k8s" workload_id:"kube-system/coredns-5c64b647bf-kxh68" endpoint_id:"eth0" > endpoint:<state:"active" name:"calif517d2f9787" profile_ids:"kns.kube-system" profile_ids:"ksa.kube-system.coredns" ipv4_nets:"10.2.0.3/32" > 
2023-03-09 06:10:16.324 [INFO][65] felix/endpoint_mgr.go 600: Updating per-endpoint chains. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"}
2023-03-09 06:10:16.326 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-tw-cali83fdaf4c216" ipVersion=0x4 table="filter"
2023-03-09 06:10:16.326 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-pri-_u2Tn2rSoAPffvE7JO6"
2023-03-09 06:10:16.337 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-fw-cali83fdaf4c216" ipVersion=0x4 table="filter"
2023-03-09 06:10:16.337 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-pro-_u2Tn2rSoAPffvE7JO6"
2023-03-09 06:10:16.337 [INFO][65] felix/endpoint_mgr.go 646: Updating endpoint routes. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"}
2023-03-09 06:10:16.337 [INFO][65] felix/endpoint_mgr.go 600: Updating per-endpoint chains. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"}
2023-03-09 06:10:16.337 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-tw-calif517d2f9787" ipVersion=0x4 table="filter"
2023-03-09 06:10:16.337 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-fw-calif517d2f9787" ipVersion=0x4 table="filter"
2023-03-09 06:10:16.337 [INFO][65] felix/endpoint_mgr.go 646: Updating endpoint routes. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"}
2023-03-09 06:10:16.337 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-from-wl-dispatch-8" ipVersion=0x4 table="filter"
2023-03-09 06:10:16.338 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-fw-cali83fdaf4c216"
2023-03-09 06:10:16.338 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-from-wl-dispatch" ipVersion=0x4 table="filter"
2023-03-09 06:10:16.338 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-from-wl-dispatch-8"
2023-03-09 06:10:16.338 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-fw-calif517d2f9787"
2023-03-09 06:10:16.338 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-to-wl-dispatch-8" ipVersion=0x4 table="filter"
2023-03-09 06:10:16.338 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-tw-cali83fdaf4c216"
2023-03-09 06:10:16.338 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-to-wl-dispatch" ipVersion=0x4 table="filter"
2023-03-09 06:10:16.338 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-to-wl-dispatch-8"
2023-03-09 06:10:16.338 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-tw-calif517d2f9787"
2023-03-09 06:10:16.338 [INFO][65] felix/endpoint_mgr.go 1280: Skipping configuration of interface because it is oper down. ifaceName="cali83fdaf4c216"
2023-03-09 06:10:16.338 [INFO][65] felix/endpoint_mgr.go 1280: Skipping configuration of interface because it is oper down. ifaceName="calif517d2f9787"
2023-03-09 06:10:16.338 [INFO][65] felix/endpoint_mgr.go 488: Re-evaluated workload endpoint status adminUp=true failed=false known=true operUp=false status="down" workloadEndpointID=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"}
2023-03-09 06:10:16.338 [INFO][65] felix/status_combiner.go 58: Storing endpoint status update ipVersion=0x4 status="down" workload=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"}
2023-03-09 06:10:16.338 [INFO][65] felix/endpoint_mgr.go 488: Re-evaluated workload endpoint status adminUp=true failed=false known=true operUp=false status="down" workloadEndpointID=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"}
2023-03-09 06:10:16.338 [INFO][65] felix/status_combiner.go 58: Storing endpoint status update ipVersion=0x4 status="down" workload=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"}
2023-03-09 06:10:16.397 [INFO][65] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=9 ifaceName="calif517d2f9787" state="down"
2023-03-09 06:10:16.397 [INFO][65] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{} ifaceName="calif517d2f9787"
2023-03-09 06:10:16.397 [INFO][65] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=9 ifaceName="calif517d2f9787" state="up"
2023-03-09 06:10:16.398 [INFO][65] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=10 ifaceName="cali83fdaf4c216" state="down"
2023-03-09 06:10:16.398 [INFO][65] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{} ifaceName="cali83fdaf4c216"
2023-03-09 06:10:16.398 [INFO][65] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=10 ifaceName="cali83fdaf4c216" state="up"
2023-03-09 06:10:16.404 [INFO][65] felix/calc_graph.go 462: Local endpoint updated id=WorkloadEndpoint(node=lke96996-146211-640977a3930b, orchestrator=k8s, workload=kube-system/csi-linode-controller-0, name=eth0)
2023-03-09 06:10:16.436 [INFO][65] felix/status_combiner.go 78: Endpoint down for at least one IP version id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"} ipVersion=0x4 status="down"
2023-03-09 06:10:16.437 [INFO][65] felix/status_combiner.go 98: Reporting combined status. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"} status="down"
2023-03-09 06:10:16.437 [INFO][65] felix/status_combiner.go 78: Endpoint down for at least one IP version id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"} ipVersion=0x4 status="down"
2023-03-09 06:10:16.437 [INFO][65] felix/status_combiner.go 98: Reporting combined status. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"} status="down"
2023-03-09 06:10:16.437 [INFO][65] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"calif517d2f9787", Addrs:set.Typed[string]{}}
2023-03-09 06:10:16.437 [INFO][65] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"calif517d2f9787", Addrs:set.Typed[string]{}}
2023-03-09 06:10:16.437 [INFO][65] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"cali83fdaf4c216", Addrs:set.Typed[string]{}}
2023-03-09 06:10:16.437 [INFO][65] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"cali83fdaf4c216", Addrs:set.Typed[string]{}}
2023-03-09 06:10:16.437 [INFO][65] felix/int_dataplane.go 1680: Received *proto.ActiveProfileUpdate update from calculation graph msg=id:<name:"ksa.kube-system.csi-controller-sa" > profile:<> 
2023-03-09 06:10:16.437 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-pri-_jv5iCRpeVbyvPPmsd7" ipVersion=0x4 table="filter"
2023-03-09 06:10:16.437 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-pro-_jv5iCRpeVbyvPPmsd7" ipVersion=0x4 table="filter"
2023-03-09 06:10:16.437 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-pro-_jv5iCRpeVbyvPPmsd7" ipVersion=0x4 table="mangle"
2023-03-09 06:10:16.437 [INFO][65] felix/int_dataplane.go 1680: Received *proto.WorkloadEndpointUpdate update from calculation graph msg=id:<orchestrator_id:"k8s" workload_id:"kube-system/csi-linode-controller-0" endpoint_id:"eth0" > endpoint:<state:"active" name:"cali164b735c616" profile_ids:"kns.kube-system" profile_ids:"ksa.kube-system.csi-controller-sa" ipv4_nets:"10.2.0.5/32" > 
2023-03-09 06:10:16.437 [INFO][65] felix/endpoint_mgr.go 600: Updating per-endpoint chains. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"}
2023-03-09 06:10:16.437 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-tw-cali164b735c616" ipVersion=0x4 table="filter"
2023-03-09 06:10:16.437 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-pri-_jv5iCRpeVbyvPPmsd7"
2023-03-09 06:10:16.437 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-fw-cali164b735c616" ipVersion=0x4 table="filter"
2023-03-09 06:10:16.437 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-pro-_jv5iCRpeVbyvPPmsd7"
2023-03-09 06:10:16.437 [INFO][65] felix/endpoint_mgr.go 646: Updating endpoint routes. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"}
2023-03-09 06:10:16.437 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-from-wl-dispatch" ipVersion=0x4 table="filter"
2023-03-09 06:10:16.437 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-fw-cali164b735c616"
2023-03-09 06:10:16.437 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-to-wl-dispatch" ipVersion=0x4 table="filter"
2023-03-09 06:10:16.437 [INFO][65] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-tw-cali164b735c616"
2023-03-09 06:10:16.438 [INFO][65] felix/endpoint_mgr.go 1280: Skipping configuration of interface because it is oper down. ifaceName="cali164b735c616"
2023-03-09 06:10:16.438 [INFO][65] felix/endpoint_mgr.go 488: Re-evaluated workload endpoint status adminUp=true failed=false known=true operUp=false status="down" workloadEndpointID=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"}
2023-03-09 06:10:16.438 [INFO][65] felix/status_combiner.go 58: Storing endpoint status update ipVersion=0x4 status="down" workload=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"}
2023-03-09 06:10:16.503 [INFO][65] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=11 ifaceName="cali164b735c616" state="down"
2023-03-09 06:10:16.504 [INFO][65] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{} ifaceName="cali164b735c616"
2023-03-09 06:10:16.504 [INFO][65] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=11 ifaceName="cali164b735c616" state="up"
2023-03-09 06:10:16.551 [INFO][65] felix/status_combiner.go 78: Endpoint down for at least one IP version id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"} ipVersion=0x4 status="down"
2023-03-09 06:10:16.551 [INFO][65] felix/status_combiner.go 98: Reporting combined status. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"} status="down"
2023-03-09 06:10:16.551 [INFO][65] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"calif517d2f9787", State:"down", Index:9}
2023-03-09 06:10:16.551 [INFO][65] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"calif517d2f9787", State:"up", Index:9}
2023-03-09 06:10:16.551 [INFO][65] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"cali83fdaf4c216", State:"down", Index:10}
2023-03-09 06:10:16.551 [INFO][65] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"cali83fdaf4c216", State:"up", Index:10}
2023-03-09 06:10:16.551 [INFO][65] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"cali164b735c616", State:"down", Index:11}
2023-03-09 06:10:16.551 [INFO][65] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"cali164b735c616", State:"up", Index:11}
2023-03-09 06:10:16.552 [INFO][65] felix/endpoint_mgr.go 372: Workload interface came up, marking for reconfiguration. ifaceName="calif517d2f9787"
2023-03-09 06:10:16.552 [INFO][65] felix/endpoint_mgr.go 429: Workload interface state changed; marking for status update. ifaceName="calif517d2f9787"
2023-03-09 06:10:16.552 [INFO][65] felix/endpoint_mgr.go 372: Workload interface came up, marking for reconfiguration. ifaceName="cali83fdaf4c216"
2023-03-09 06:10:16.552 [INFO][65] felix/endpoint_mgr.go 429: Workload interface state changed; marking for status update. ifaceName="cali83fdaf4c216"
2023-03-09 06:10:16.552 [INFO][65] felix/endpoint_mgr.go 372: Workload interface came up, marking for reconfiguration. ifaceName="cali164b735c616"
2023-03-09 06:10:16.552 [INFO][65] felix/endpoint_mgr.go 429: Workload interface state changed; marking for status update. ifaceName="cali164b735c616"
2023-03-09 06:10:16.552 [INFO][65] felix/endpoint_mgr.go 1212: Applying /proc/sys configuration to interface. ifaceName="calif517d2f9787"
2023-03-09 06:10:16.552 [INFO][65] felix/endpoint_mgr.go 1212: Applying /proc/sys configuration to interface. ifaceName="cali83fdaf4c216"
2023-03-09 06:10:16.552 [INFO][65] felix/endpoint_mgr.go 1212: Applying /proc/sys configuration to interface. ifaceName="cali164b735c616"
2023-03-09 06:10:16.552 [INFO][65] felix/endpoint_mgr.go 488: Re-evaluated workload endpoint status adminUp=true failed=false known=true operUp=true status="up" workloadEndpointID=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"}
2023-03-09 06:10:16.552 [INFO][65] felix/status_combiner.go 58: Storing endpoint status update ipVersion=0x4 status="up" workload=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"}
2023-03-09 06:10:16.552 [INFO][65] felix/endpoint_mgr.go 488: Re-evaluated workload endpoint status adminUp=true failed=false known=true operUp=true status="up" workloadEndpointID=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"}
2023-03-09 06:10:16.552 [INFO][65] felix/status_combiner.go 58: Storing endpoint status update ipVersion=0x4 status="up" workload=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"}
2023-03-09 06:10:16.552 [INFO][65] felix/endpoint_mgr.go 488: Re-evaluated workload endpoint status adminUp=true failed=false known=true operUp=true status="up" workloadEndpointID=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"}
2023-03-09 06:10:16.552 [INFO][65] felix/status_combiner.go 58: Storing endpoint status update ipVersion=0x4 status="up" workload=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"}
2023-03-09 06:10:16.579 [INFO][65] felix/status_combiner.go 81: Endpoint up for at least one IP version id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"} ipVersion=0x4 status="up"
2023-03-09 06:10:16.579 [INFO][65] felix/status_combiner.go 98: Reporting combined status. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"} status="up"
2023-03-09 06:10:16.580 [INFO][65] felix/status_combiner.go 81: Endpoint up for at least one IP version id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"} ipVersion=0x4 status="up"
2023-03-09 06:10:16.583 [INFO][65] felix/status_combiner.go 98: Reporting combined status. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"} status="up"
2023-03-09 06:10:16.583 [INFO][65] felix/status_combiner.go 81: Endpoint up for at least one IP version id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"} ipVersion=0x4 status="up"
2023-03-09 06:10:16.583 [INFO][65] felix/status_combiner.go 98: Reporting combined status. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"} status="up"
2023-03-09 06:10:16.587 [INFO][65] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"cali164b735c616", Addrs:set.Typed[string]{}}
2023-03-09 06:10:16.587 [INFO][65] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"cali164b735c616", Addrs:set.Typed[string]{}}
2023-03-09 06:10:17.144 [INFO][65] felix/calc_graph.go 462: Local endpoint updated id=WorkloadEndpoint(node=lke96996-146211-640977a3930b, orchestrator=k8s, workload=kube-system/csi-linode-controller-0, name=eth0)
2023-03-09 06:10:17.144 [INFO][65] felix/int_dataplane.go 1680: Received *proto.WorkloadEndpointUpdate update from calculation graph msg=id:<orchestrator_id:"k8s" workload_id:"kube-system/csi-linode-controller-0" endpoint_id:"eth0" > endpoint:<state:"active" name:"cali164b735c616" profile_ids:"kns.kube-system" profile_ids:"ksa.kube-system.csi-controller-sa" ipv4_nets:"10.2.0.5/32" > 
2023-03-09 06:10:17.144 [INFO][65] felix/endpoint_mgr.go 600: Updating per-endpoint chains. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"}
2023-03-09 06:10:17.144 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-tw-cali164b735c616" ipVersion=0x4 table="filter"
2023-03-09 06:10:17.145 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-fw-cali164b735c616" ipVersion=0x4 table="filter"
2023-03-09 06:10:17.145 [INFO][65] felix/endpoint_mgr.go 646: Updating endpoint routes. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"}
2023-03-09 06:10:17.145 [INFO][65] felix/endpoint_mgr.go 1212: Applying /proc/sys configuration to interface. ifaceName="cali164b735c616"
2023-03-09 06:10:17.145 [INFO][65] felix/endpoint_mgr.go 488: Re-evaluated workload endpoint status adminUp=true failed=false known=true operUp=true status="up" workloadEndpointID=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"}
2023-03-09 06:10:17.145 [INFO][65] felix/status_combiner.go 58: Storing endpoint status update ipVersion=0x4 status="up" workload=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"}
2023-03-09 06:10:17.187 [INFO][65] felix/calc_graph.go 462: Local endpoint updated id=WorkloadEndpoint(node=lke96996-146211-640977a3930b, orchestrator=k8s, workload=kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl, name=eth0)
2023-03-09 06:10:17.204 [INFO][65] felix/status_combiner.go 81: Endpoint up for at least one IP version id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"} ipVersion=0x4 status="up"
2023-03-09 06:10:17.204 [INFO][65] felix/status_combiner.go 98: Reporting combined status. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"} status="up"
2023-03-09 06:10:17.204 [INFO][65] felix/int_dataplane.go 1680: Received *proto.WorkloadEndpointUpdate update from calculation graph msg=id:<orchestrator_id:"k8s" workload_id:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl" endpoint_id:"eth0" > endpoint:<state:"active" name:"cali8f354ab7321" profile_ids:"kns.kube-system" profile_ids:"ksa.kube-system.calico-kube-controllers" ipv4_nets:"10.2.0.2/32" > 
2023-03-09 06:10:17.204 [INFO][65] felix/endpoint_mgr.go 600: Updating per-endpoint chains. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"}
2023-03-09 06:10:17.204 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-tw-cali8f354ab7321" ipVersion=0x4 table="filter"
2023-03-09 06:10:17.204 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-fw-cali8f354ab7321" ipVersion=0x4 table="filter"
2023-03-09 06:10:17.205 [INFO][65] felix/endpoint_mgr.go 646: Updating endpoint routes. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"}
2023-03-09 06:10:17.205 [INFO][65] felix/endpoint_mgr.go 1212: Applying /proc/sys configuration to interface. ifaceName="cali8f354ab7321"
2023-03-09 06:10:17.205 [INFO][65] felix/endpoint_mgr.go 488: Re-evaluated workload endpoint status adminUp=true failed=false known=true operUp=true status="up" workloadEndpointID=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"}
2023-03-09 06:10:17.205 [INFO][65] felix/status_combiner.go 58: Storing endpoint status update ipVersion=0x4 status="up" workload=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"}
2023-03-09 06:10:17.219 [INFO][65] felix/status_combiner.go 81: Endpoint up for at least one IP version id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"} ipVersion=0x4 status="up"
2023-03-09 06:10:17.219 [INFO][65] felix/status_combiner.go 98: Reporting combined status. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"} status="up"
2023-03-09 06:10:17.474 [INFO][65] felix/iface_monitor.go 217: Netlink address update for known interface.  addr="fe80::ecee:eeff:feee:eeee" exists=true ifIndex=11
2023-03-09 06:10:17.474 [INFO][65] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{fe80::ecee:eeff:feee:eeee} ifaceName="cali164b735c616"
2023-03-09 06:10:17.474 [INFO][65] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"cali164b735c616", Addrs:set.Typed[string]{"fe80::ecee:eeff:feee:eeee":set.v{}}}
2023-03-09 06:10:17.474 [INFO][65] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"cali164b735c616", Addrs:set.Typed[string]{"fe80::ecee:eeff:feee:eeee":set.v{}}}
2023-03-09 06:10:18.314 [INFO][65] felix/iface_monitor.go 217: Netlink address update for known interface.  addr="fe80::ecee:eeff:feee:eeee" exists=true ifIndex=9
2023-03-09 06:10:18.314 [INFO][65] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{fe80::ecee:eeff:feee:eeee} ifaceName="calif517d2f9787"
2023-03-09 06:10:18.314 [INFO][65] felix/iface_monitor.go 217: Netlink address update for known interface.  addr="fe80::ecee:eeff:feee:eeee" exists=true ifIndex=10
2023-03-09 06:10:18.314 [INFO][65] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{fe80::ecee:eeff:feee:eeee} ifaceName="cali83fdaf4c216"
2023-03-09 06:10:18.314 [INFO][65] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"calif517d2f9787", Addrs:set.Typed[string]{"fe80::ecee:eeff:feee:eeee":set.v{}}}
2023-03-09 06:10:18.314 [INFO][65] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"calif517d2f9787", Addrs:set.Typed[string]{"fe80::ecee:eeff:feee:eeee":set.v{}}}
2023-03-09 06:10:18.316 [INFO][65] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"cali83fdaf4c216", Addrs:set.Typed[string]{"fe80::ecee:eeff:feee:eeee":set.v{}}}
2023-03-09 06:10:18.316 [INFO][65] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"cali83fdaf4c216", Addrs:set.Typed[string]{"fe80::ecee:eeff:feee:eeee":set.v{}}}
2023-03-09 06:10:19.171 [INFO][65] felix/calc_graph.go 462: Local endpoint updated id=WorkloadEndpoint(node=lke96996-146211-640977a3930b, orchestrator=k8s, workload=kube-system/coredns-5c64b647bf-kxh68, name=eth0)
2023-03-09 06:10:19.171 [INFO][65] felix/int_dataplane.go 1680: Received *proto.WorkloadEndpointUpdate update from calculation graph msg=id:<orchestrator_id:"k8s" workload_id:"kube-system/coredns-5c64b647bf-kxh68" endpoint_id:"eth0" > endpoint:<state:"active" name:"calif517d2f9787" profile_ids:"kns.kube-system" profile_ids:"ksa.kube-system.coredns" ipv4_nets:"10.2.0.3/32" > 
2023-03-09 06:10:19.172 [INFO][65] felix/endpoint_mgr.go 600: Updating per-endpoint chains. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"}
2023-03-09 06:10:19.172 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-tw-calif517d2f9787" ipVersion=0x4 table="filter"
2023-03-09 06:10:19.172 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-fw-calif517d2f9787" ipVersion=0x4 table="filter"
2023-03-09 06:10:19.172 [INFO][65] felix/endpoint_mgr.go 646: Updating endpoint routes. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"}
2023-03-09 06:10:19.172 [INFO][65] felix/endpoint_mgr.go 1212: Applying /proc/sys configuration to interface. ifaceName="calif517d2f9787"
2023-03-09 06:10:19.173 [INFO][65] felix/endpoint_mgr.go 488: Re-evaluated workload endpoint status adminUp=true failed=false known=true operUp=true status="up" workloadEndpointID=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"}
2023-03-09 06:10:19.173 [INFO][65] felix/status_combiner.go 58: Storing endpoint status update ipVersion=0x4 status="up" workload=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"}
2023-03-09 06:10:19.202 [INFO][65] felix/status_combiner.go 81: Endpoint up for at least one IP version id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"} ipVersion=0x4 status="up"
2023-03-09 06:10:19.202 [INFO][65] felix/status_combiner.go 98: Reporting combined status. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"} status="up"
2023-03-09 06:10:20.164 [INFO][65] felix/calc_graph.go 462: Local endpoint updated id=WorkloadEndpoint(node=lke96996-146211-640977a3930b, orchestrator=k8s, workload=kube-system/coredns-5c64b647bf-gkmxq, name=eth0)
2023-03-09 06:10:20.164 [INFO][65] felix/int_dataplane.go 1680: Received *proto.WorkloadEndpointUpdate update from calculation graph msg=id:<orchestrator_id:"k8s" workload_id:"kube-system/coredns-5c64b647bf-gkmxq" endpoint_id:"eth0" > endpoint:<state:"active" name:"cali83fdaf4c216" profile_ids:"kns.kube-system" profile_ids:"ksa.kube-system.coredns" ipv4_nets:"10.2.0.4/32" > 
2023-03-09 06:10:20.164 [INFO][65] felix/endpoint_mgr.go 600: Updating per-endpoint chains. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"}
2023-03-09 06:10:20.164 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-tw-cali83fdaf4c216" ipVersion=0x4 table="filter"
2023-03-09 06:10:20.164 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-fw-cali83fdaf4c216" ipVersion=0x4 table="filter"
2023-03-09 06:10:20.164 [INFO][65] felix/endpoint_mgr.go 646: Updating endpoint routes. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"}
2023-03-09 06:10:20.165 [INFO][65] felix/endpoint_mgr.go 1212: Applying /proc/sys configuration to interface. ifaceName="cali83fdaf4c216"
2023-03-09 06:10:20.165 [INFO][65] felix/endpoint_mgr.go 488: Re-evaluated workload endpoint status adminUp=true failed=false known=true operUp=true status="up" workloadEndpointID=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"}
2023-03-09 06:10:20.165 [INFO][65] felix/status_combiner.go 58: Storing endpoint status update ipVersion=0x4 status="up" workload=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"}
2023-03-09 06:10:20.188 [INFO][65] felix/calc_graph.go 462: Local endpoint updated id=WorkloadEndpoint(node=lke96996-146211-640977a3930b, orchestrator=k8s, workload=kube-system/coredns-5c64b647bf-gkmxq, name=eth0)
2023-03-09 06:10:20.191 [INFO][65] felix/status_combiner.go 81: Endpoint up for at least one IP version id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"} ipVersion=0x4 status="up"
2023-03-09 06:10:20.191 [INFO][65] felix/status_combiner.go 98: Reporting combined status. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"} status="up"
2023-03-09 06:10:20.191 [INFO][65] felix/int_dataplane.go 1680: Received *proto.WorkloadEndpointUpdate update from calculation graph msg=id:<orchestrator_id:"k8s" workload_id:"kube-system/coredns-5c64b647bf-gkmxq" endpoint_id:"eth0" > endpoint:<state:"active" name:"cali83fdaf4c216" profile_ids:"kns.kube-system" profile_ids:"ksa.kube-system.coredns" ipv4_nets:"10.2.0.4/32" > 
2023-03-09 06:10:20.191 [INFO][65] felix/endpoint_mgr.go 600: Updating per-endpoint chains. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"}
2023-03-09 06:10:20.191 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-tw-cali83fdaf4c216" ipVersion=0x4 table="filter"
2023-03-09 06:10:20.191 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-fw-cali83fdaf4c216" ipVersion=0x4 table="filter"
2023-03-09 06:10:20.191 [INFO][65] felix/endpoint_mgr.go 646: Updating endpoint routes. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"}
2023-03-09 06:10:20.191 [INFO][65] felix/endpoint_mgr.go 1212: Applying /proc/sys configuration to interface. ifaceName="cali83fdaf4c216"
2023-03-09 06:10:20.192 [INFO][65] felix/endpoint_mgr.go 488: Re-evaluated workload endpoint status adminUp=true failed=false known=true operUp=true status="up" workloadEndpointID=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"}
2023-03-09 06:10:20.192 [INFO][65] felix/status_combiner.go 58: Storing endpoint status update ipVersion=0x4 status="up" workload=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"}
2023-03-09 06:10:20.205 [INFO][65] felix/status_combiner.go 81: Endpoint up for at least one IP version id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"} ipVersion=0x4 status="up"
2023-03-09 06:10:20.205 [INFO][65] felix/status_combiner.go 98: Reporting combined status. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-gkmxq", EndpointId:"eth0"} status="up"
2023-03-09 06:10:20.218 [INFO][65] felix/calc_graph.go 462: Local endpoint updated id=WorkloadEndpoint(node=lke96996-146211-640977a3930b, orchestrator=k8s, workload=kube-system/coredns-5c64b647bf-kxh68, name=eth0)
2023-03-09 06:10:20.218 [INFO][65] felix/int_dataplane.go 1680: Received *proto.WorkloadEndpointUpdate update from calculation graph msg=id:<orchestrator_id:"k8s" workload_id:"kube-system/coredns-5c64b647bf-kxh68" endpoint_id:"eth0" > endpoint:<state:"active" name:"calif517d2f9787" profile_ids:"kns.kube-system" profile_ids:"ksa.kube-system.coredns" ipv4_nets:"10.2.0.3/32" > 
2023-03-09 06:10:20.218 [INFO][65] felix/endpoint_mgr.go 600: Updating per-endpoint chains. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"}
2023-03-09 06:10:20.219 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-tw-calif517d2f9787" ipVersion=0x4 table="filter"
2023-03-09 06:10:20.219 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-fw-calif517d2f9787" ipVersion=0x4 table="filter"
2023-03-09 06:10:20.219 [INFO][65] felix/endpoint_mgr.go 646: Updating endpoint routes. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"}
2023-03-09 06:10:20.219 [INFO][65] felix/endpoint_mgr.go 1212: Applying /proc/sys configuration to interface. ifaceName="calif517d2f9787"
2023-03-09 06:10:20.219 [INFO][65] felix/endpoint_mgr.go 488: Re-evaluated workload endpoint status adminUp=true failed=false known=true operUp=true status="up" workloadEndpointID=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"}
2023-03-09 06:10:20.219 [INFO][65] felix/status_combiner.go 58: Storing endpoint status update ipVersion=0x4 status="up" workload=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"}
2023-03-09 06:10:20.228 [INFO][65] felix/status_combiner.go 81: Endpoint up for at least one IP version id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"} ipVersion=0x4 status="up"
2023-03-09 06:10:20.228 [INFO][65] felix/status_combiner.go 98: Reporting combined status. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/coredns-5c64b647bf-kxh68", EndpointId:"eth0"} status="up"
bird: Shutting down
bird: device1: Shutting down
bird: device1: State changed to flush
bird: direct1: Shutting down
bird: direct1: State changed to flush
bird: device1: State changed to down
bird: direct1: State changed to down
bird: Shutdown completed
2023-03-09 06:10:29.889 [INFO][65] felix/calc_graph.go 462: Local endpoint updated id=WorkloadEndpoint(node=lke96996-146211-640977a3930b, orchestrator=k8s, workload=kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl, name=eth0)
2023-03-09 06:10:29.890 [INFO][65] felix/int_dataplane.go 1680: Received *proto.WorkloadEndpointUpdate update from calculation graph msg=id:<orchestrator_id:"k8s" workload_id:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl" endpoint_id:"eth0" > endpoint:<state:"active" name:"cali8f354ab7321" profile_ids:"kns.kube-system" profile_ids:"ksa.kube-system.calico-kube-controllers" ipv4_nets:"10.2.0.2/32" > 
2023-03-09 06:10:29.890 [INFO][65] felix/endpoint_mgr.go 600: Updating per-endpoint chains. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"}
2023-03-09 06:10:29.890 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-tw-cali8f354ab7321" ipVersion=0x4 table="filter"
2023-03-09 06:10:29.891 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-fw-cali8f354ab7321" ipVersion=0x4 table="filter"
2023-03-09 06:10:29.891 [INFO][65] felix/endpoint_mgr.go 646: Updating endpoint routes. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"}
2023-03-09 06:10:29.891 [INFO][65] felix/endpoint_mgr.go 1212: Applying /proc/sys configuration to interface. ifaceName="cali8f354ab7321"
2023-03-09 06:10:29.894 [INFO][65] felix/endpoint_mgr.go 488: Re-evaluated workload endpoint status adminUp=true failed=false known=true operUp=true status="up" workloadEndpointID=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"}
2023-03-09 06:10:29.894 [INFO][65] felix/status_combiner.go 58: Storing endpoint status update ipVersion=0x4 status="up" workload=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"}
2023-03-09 06:10:29.905 [INFO][65] felix/status_combiner.go 81: Endpoint up for at least one IP version id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"} ipVersion=0x4 status="up"
2023-03-09 06:10:29.905 [INFO][65] felix/status_combiner.go 98: Reporting combined status. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/calico-kube-controllers-5dbfbbf9dc-wptvl", EndpointId:"eth0"} status="up"
2023-03-09 06:10:30.244 [INFO][65] felix/calc_graph.go 462: Local endpoint updated id=WorkloadEndpoint(node=lke96996-146211-640977a3930b, orchestrator=k8s, workload=kube-system/csi-linode-controller-0, name=eth0)
2023-03-09 06:10:30.244 [INFO][65] felix/int_dataplane.go 1680: Received *proto.WorkloadEndpointUpdate update from calculation graph msg=id:<orchestrator_id:"k8s" workload_id:"kube-system/csi-linode-controller-0" endpoint_id:"eth0" > endpoint:<state:"active" name:"cali164b735c616" profile_ids:"kns.kube-system" profile_ids:"ksa.kube-system.csi-controller-sa" ipv4_nets:"10.2.0.5/32" > 
2023-03-09 06:10:30.245 [INFO][65] felix/endpoint_mgr.go 600: Updating per-endpoint chains. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"}
2023-03-09 06:10:30.245 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-tw-cali164b735c616" ipVersion=0x4 table="filter"
2023-03-09 06:10:30.245 [INFO][65] felix/table.go 508: Queueing update of chain. chainName="cali-fw-cali164b735c616" ipVersion=0x4 table="filter"
2023-03-09 06:10:30.245 [INFO][65] felix/endpoint_mgr.go 646: Updating endpoint routes. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"}
2023-03-09 06:10:30.245 [INFO][65] felix/endpoint_mgr.go 1212: Applying /proc/sys configuration to interface. ifaceName="cali164b735c616"
2023-03-09 06:10:30.245 [INFO][65] felix/endpoint_mgr.go 488: Re-evaluated workload endpoint status adminUp=true failed=false known=true operUp=true status="up" workloadEndpointID=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"}
2023-03-09 06:10:30.245 [INFO][65] felix/status_combiner.go 58: Storing endpoint status update ipVersion=0x4 status="up" workload=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"}
2023-03-09 06:10:30.253 [INFO][65] felix/status_combiner.go 81: Endpoint up for at least one IP version id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"} ipVersion=0x4 status="up"
2023-03-09 06:10:30.253 [INFO][65] felix/status_combiner.go 98: Reporting combined status. id=proto.WorkloadEndpointID{OrchestratorId:"k8s", WorkloadId:"kube-system/csi-linode-controller-0", EndpointId:"eth0"} status="up"
bird: device1: Initializing
bird: direct1: Initializing
bird: device1: Starting
bird: device1: Connected to table master
bird: device1: State changed to feed
bird: direct1: Starting
bird: direct1: Connected to table master
bird: direct1: State changed to feed
bird: Graceful restart started
bird: Graceful restart done
bird: Started
bird: device1: State changed to up
bird: direct1: State changed to up
2023-03-09 06:10:32.932 [INFO][66] confd/client.go 995: Recompute BGP peerings: lke96996-146211-640977a3930b updated
2023-03-09 06:10:37.153 [INFO][66] confd/client.go 995: Recompute BGP peerings: HostBGPConfig(node=lke96996-146211-640977a3e82e; name=ip_addr_v4) updated; HostBGPConfig(node=lke96996-146211-640977a3e82e; name=network_v4) updated
bird: Reconfiguration requested by SIGHUP
bird: Reconfiguring
bird: device1: Reconfigured
bird: direct1: Reconfigured
bird: Adding protocol Mesh_192_168_173_115
bird: Mesh_192_168_173_115: Initializing
bird: Mesh_192_168_173_115: Starting
bird: Mesh_192_168_173_115: State changed to start
bird: Reconfigured
2023-03-09 06:10:37.170 [INFO][66] confd/resource.go 278: Target config /etc/calico/confd/config/bird.cfg has been updated due to change in key: /calico/bgp/v1/host
2023-03-09 06:10:40.748 [INFO][66] confd/client.go 995: Recompute BGP peerings: lke96996-146211-640977a3e82e updated
bird: Mesh_192_168_173_115: Connected to table master
bird: Mesh_192_168_173_115: State changed to feed
bird: Mesh_192_168_173_115: State changed to up
bird: Mesh_192_168_173_115: Received: Administrative shutdown
bird: Mesh_192_168_173_115: State changed to stop
bird: Mesh_192_168_173_115: State changed to down
bird: Mesh_192_168_173_115: Starting
bird: Mesh_192_168_173_115: State changed to start
bird: Mesh_192_168_173_115: Connected to table master
bird: Mesh_192_168_173_115: State changed to feed
bird: Mesh_192_168_173_115: State changed to up
2023-03-09 06:11:10.196 [INFO][59] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.129.114
2023-03-09 06:11:10.197 [INFO][59] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.129.114/17, detected by connecting to 192.168.128.1
2023-03-09 06:11:13.098 [INFO][65] felix/summary.go 100: Summarising 46 dataplane reconciliation loops over 1m3s: avg=25ms longest=181ms (resync-mangle-v4,resync-nat-v4,resync-raw-v4)
2023-03-09 06:11:36.542 [INFO][65] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/kubernetesnetworkpolicies" error=too old resource version: 1 (897)
2023-03-09 06:11:36.548 [INFO][65] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/workloadendpoints" error=too old resource version: 834 (897)
2023-03-09 06:11:36.548 [INFO][65] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/kubernetesendpointslices" error=too old resource version: 786 (897)
2023-03-09 06:11:36.549 [INFO][65] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/clusterinformations" error=too old resource version: 679 (902)
2023-03-09 06:11:36.549 [INFO][65] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/kubernetesservice" error=too old resource version: 294 (897)
2023-03-09 06:11:36.737 [INFO][65] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/profiles" error=too old resource version: 706 (897)
2023-03-09 06:11:36.985 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/workloadendpoints"
2023-03-09 06:11:36.985 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/kubernetesnetworkpolicies"
2023-03-09 06:11:36.990 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/kubernetesendpointslices"
2023-03-09 06:11:36.990 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/profiles"
2023-03-09 06:11:36.991 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/kubernetesservice"
2023-03-09 06:11:36.991 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/clusterinformations"
2023-03-09 06:11:38.852 [INFO][65] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/bgpconfigurations" error=too old resource version: 492 (909)
2023-03-09 06:11:38.852 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/bgpconfigurations"
2023-03-09 06:11:38.854 [INFO][65] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/ippools" error=too old resource version: 678 (904)
2023-03-09 06:11:38.854 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/ippools"
2023-03-09 06:11:38.855 [INFO][65] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/networkpolicies" error=too old resource version: 445 (910)
2023-03-09 06:11:38.855 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/networkpolicies"
2023-03-09 06:11:38.946 [INFO][65] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/networksets" error=too old resource version: 445 (910)
2023-03-09 06:11:38.946 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/networksets"
2023-03-09 06:11:38.970 [INFO][65] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/hostendpoints" error=too old resource version: 488 (910)
2023-03-09 06:11:38.970 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/hostendpoints"
2023-03-09 06:11:39.062 [INFO][65] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/globalnetworkpolicies" error=too old resource version: 488 (910)
2023-03-09 06:11:39.063 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/globalnetworkpolicies"
2023-03-09 06:11:39.155 [INFO][65] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/globalnetworksets" error=too old resource version: 488 (910)
2023-03-09 06:11:39.155 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/globalnetworksets"
2023-03-09 06:11:39.255 [INFO][66] confd/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/ippools" error=too old resource version: 678 (904)
2023-03-09 06:11:39.256 [INFO][67] status-reporter/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/caliconodestatuses" error=too old resource version: 485 (904)
2023-03-09 06:11:39.256 [INFO][66] confd/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/ippools"
2023-03-09 06:11:39.256 [INFO][66] confd/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/bgppeers" error=too old resource version: 492 (909)
2023-03-09 06:11:39.257 [INFO][67] status-reporter/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/caliconodestatuses"
2023-03-09 06:11:39.257 [INFO][66] confd/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/bgppeers"
2023-03-09 06:11:39.256 [INFO][66] confd/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/bgpconfigurations" error=too old resource version: 492 (909)
2023-03-09 06:11:39.258 [INFO][66] confd/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/bgpconfigurations"
2023-03-09 06:11:39.258 [INFO][65] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/felixconfigurations" error=too old resource version: 680 (910)
2023-03-09 06:11:39.258 [INFO][65] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/felixconfigurations"
2023-03-09 06:12:10.198 [INFO][59] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.129.114
2023-03-09 06:12:10.205 [INFO][59] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.129.114/17, detected by connecting to 192.168.128.1
2023-03-09 06:12:16.609 [INFO][65] felix/summary.go 100: Summarising 11 dataplane reconciliation loops over 1m3.5s: avg=6ms longest=19ms (resync-nat-v4)
2023-03-09 06:13:10.206 [INFO][59] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.129.114
2023-03-09 06:13:10.208 [INFO][59] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.129.114/17, detected by connecting to 192.168.128.1
2023-03-09 06:13:16.740 [INFO][65] felix/summary.go 100: Summarising 8 dataplane reconciliation loops over 1m0.1s: avg=11ms longest=57ms (resync-mangle-v4,resync-nat-v4,resync-raw-v4)
2023-03-09 06:14:10.209 [INFO][59] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.129.114
2023-03-09 06:14:10.211 [INFO][59] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.129.114/17, detected by connecting to 192.168.128.1
2023-03-09 06:14:23.524 [INFO][65] felix/summary.go 100: Summarising 12 dataplane reconciliation loops over 1m6.8s: avg=5ms longest=11ms (resync-filter-v4)
2023-03-09 06:15:10.212 [INFO][59] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.129.114
2023-03-09 06:15:10.213 [INFO][59] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.129.114/17, detected by connecting to 192.168.128.1
2023-03-09 06:15:12.342 [INFO][65] felix/usagerep.go 115: Initial delay complete, doing first report
2023-03-09 06:15:12.342 [INFO][65] felix/usagerep.go 205: Reporting cluster usage/checking for deprecation warnings. alpEnabled=false calicoVersion="v3.24.5" clusterGUID="7197fc97702c482f9a5df8fb5c03809a" clusterType="k8s,bgp,kubeadm,kdd" gitRevision="f1a1611acb98d9187f48bbbe2227301aa69f0499" kubernetesVersion="v1.25.6" stats=calc.StatsUpdate{NumHosts:2, NumWorkloadEndpoints:4, NumHostEndpoints:0, NumPolicies:0, NumProfiles:50, NumALPPolicies:0} version="v3.24.5"
2023-03-09 06:15:13.253 [INFO][65] felix/usagerep.go 117: First report done, starting ticker
2023-03-09 06:15:26.714 [INFO][65] felix/summary.go 100: Summarising 11 dataplane reconciliation loops over 1m3.2s: avg=6ms longest=14ms ()
2023-03-09 06:16:10.224 [INFO][59] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.129.114
2023-03-09 06:16:10.225 [INFO][59] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.129.114/17, detected by connecting to 192.168.128.1
2023-03-09 06:16:26.935 [INFO][65] felix/summary.go 100: Summarising 9 dataplane reconciliation loops over 1m0.2s: avg=6ms longest=20ms (resync-mangle-v4,resync-nat-v4)
2023-03-09 06:17:10.226 [INFO][59] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.129.114
2023-03-09 06:17:10.227 [INFO][59] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.129.114/17, detected by connecting to 192.168.128.1
2023-03-09 06:17:31.120 [INFO][65] felix/summary.go 100: Summarising 11 dataplane reconciliation loops over 1m4.2s: avg=4ms longest=15ms ()
2023-03-09 06:18:10.239 [INFO][59] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.129.114
2023-03-09 06:18:10.240 [INFO][59] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.129.114/17, detected by connecting to 192.168.128.1
2023-03-09 06:18:34.831 [INFO][65] felix/summary.go 100: Summarising 9 dataplane reconciliation loops over 1m3.7s: avg=4ms longest=12ms ()
2023-03-09 06:19:10.241 [INFO][59] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.129.114
2023-03-09 06:19:10.243 [INFO][59] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.129.114/17, detected by connecting to 192.168.128.1
2023-03-09 06:19:35.122 [INFO][65] felix/summary.go 100: Summarising 10 dataplane reconciliation loops over 1m0.3s: avg=6ms longest=13ms (resync-mangle-v4,resync-raw-v4)
2023-03-09 06:20:10.255 [INFO][59] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.129.114
2023-03-09 06:20:10.258 [INFO][59] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.129.114/17, detected by connecting to 192.168.128.1
2023-03-09 06:20:40.360 [INFO][65] felix/summary.go 100: Summarising 10 dataplane reconciliation loops over 1m5.2s: avg=6ms longest=24ms (resync-mangle-v4,resync-raw-v4)
2023-03-09 06:21:10.259 [INFO][59] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.129.114
2023-03-09 06:21:10.261 [INFO][59] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.129.114/17, detected by connecting to 192.168.128.1
2023-03-09 06:21:42.554 [INFO][65] felix/summary.go 100: Summarising 8 dataplane reconciliation loops over 1m2.2s: avg=4ms longest=13ms ()
2023-03-09 06:22:10.272 [INFO][59] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.129.114
2023-03-09 06:22:10.274 [INFO][59] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.129.114/17, detected by connecting to 192.168.128.1
2023-03-09 06:22:44.291 [INFO][65] felix/summary.go 100: Summarising 11 dataplane reconciliation loops over 1m1.7s: avg=6ms longest=16ms (resync-filter-v4)
==== END logs for container calico-node of pod kube-system/calico-node-28hph ====
==== START logs for container upgrade-ipam of pod kube-system/calico-node-hzjbq ====
2023-03-09 06:10:19.640 [INFO][1] ipam_plugin.go 80: migrating from host-local to calico-ipam...
2023-03-09 06:10:19.641 [INFO][1] migrate.go 67: checking host-local IPAM data dir dir existence...
2023-03-09 06:10:19.641 [INFO][1] migrate.go 69: host-local IPAM data dir dir not found; no migration necessary, successfully exiting...
2023-03-09 06:10:19.641 [INFO][1] ipam_plugin.go 110: migration from host-local to calico-ipam complete node="lke96996-146211-640977a3e82e"
==== END logs for container upgrade-ipam of pod kube-system/calico-node-hzjbq ====
==== START logs for container install-cni of pod kube-system/calico-node-hzjbq ====
time="2023-03-09T06:10:20Z" level=info msg="Running as a Kubernetes pod" source="install.go:145"
2023-03-09 06:10:21.435 [INFO][1] cni-installer/<nil> <nil>: Installed /host/opt/cni/bin/bandwidth
2023-03-09 06:10:21.640 [INFO][1] cni-installer/<nil> <nil>: Installed /host/opt/cni/bin/calico
2023-03-09 06:10:21.910 [INFO][1] cni-installer/<nil> <nil>: Installed /host/opt/cni/bin/calico-ipam
2023-03-09 06:10:21.925 [INFO][1] cni-installer/<nil> <nil>: Installed /host/opt/cni/bin/flannel
2023-03-09 06:10:21.937 [INFO][1] cni-installer/<nil> <nil>: Installed /host/opt/cni/bin/host-local
2023-03-09 06:10:22.170 [INFO][1] cni-installer/<nil> <nil>: Installed /host/opt/cni/bin/install
2023-03-09 06:10:22.184 [INFO][1] cni-installer/<nil> <nil>: Installed /host/opt/cni/bin/loopback
2023-03-09 06:10:22.199 [INFO][1] cni-installer/<nil> <nil>: Installed /host/opt/cni/bin/portmap
2023-03-09 06:10:22.215 [INFO][1] cni-installer/<nil> <nil>: Installed /host/opt/cni/bin/tuning
2023-03-09 06:10:22.215 [INFO][1] cni-installer/<nil> <nil>: Wrote Calico CNI binaries to /host/opt/cni/bin

2023-03-09 06:10:22.347 [INFO][1] cni-installer/<nil> <nil>: CNI plugin version: v3.24.5

2023-03-09 06:10:22.347 [INFO][1] cni-installer/<nil> <nil>: /host/secondary-bin-dir is not writeable, skipping
W0309 06:10:22.347827       1 client_config.go:617] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
time="2023-03-09T06:10:22Z" level=info msg="Using CNI config template from CNI_NETWORK_CONFIG environment variable." source="install.go:336"
2023-03-09 06:10:22.367 [INFO][1] cni-installer/<nil> <nil>: Created /host/etc/cni/net.d/10-calico.conflist
2023-03-09 06:10:22.367 [INFO][1] cni-installer/<nil> <nil>: Done configuring CNI.  Sleep= false
{
  "name": "k8s-pod-network",
  "cniVersion": "0.3.1",
  "plugins": [
    {
      "type": "calico",
      "log_level": "info",
      "log_file_path": "/var/log/calico/cni/cni.log",
      "datastore_type": "kubernetes",
      "nodename": "lke96996-146211-640977a3e82e",
      "mtu": 0,
      "ipam": {
          "type": "host-local",
          "subnet": "usePodCidr"
      },
      "policy": {
          "type": "k8s"
      },
      "kubernetes": {
          "kubeconfig": "/etc/cni/net.d/calico-kubeconfig"
      }
    },
    {
      "type": "portmap",
      "snat": true,
      "capabilities": {"portMappings": true}
    },
    {
      "type": "bandwidth",
      "capabilities": {"bandwidth": true}
    }
  ]
}
==== END logs for container install-cni of pod kube-system/calico-node-hzjbq ====
==== START logs for container flexvol-driver of pod kube-system/calico-node-hzjbq ====
==== END logs for container flexvol-driver of pod kube-system/calico-node-hzjbq ====
==== START logs for container calico-node of pod kube-system/calico-node-hzjbq ====
2023-03-09 06:10:37.091 [INFO][9] startup/startup.go 427: Early log level set to info
2023-03-09 06:10:37.092 [INFO][9] startup/utils.go 127: Using NODENAME environment for node name lke96996-146211-640977a3e82e
2023-03-09 06:10:37.092 [INFO][9] startup/utils.go 139: Determined node name: lke96996-146211-640977a3e82e
2023-03-09 06:10:37.092 [INFO][9] startup/startup.go 94: Starting node lke96996-146211-640977a3e82e with version v3.24.5
2023-03-09 06:10:37.093 [INFO][9] startup/startup.go 432: Checking datastore connection
2023-03-09 06:10:37.111 [INFO][9] startup/startup.go 456: Datastore connection verified
2023-03-09 06:10:37.111 [INFO][9] startup/startup.go 104: Datastore is ready
2023-03-09 06:10:37.136 [INFO][9] startup/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.173.115
2023-03-09 06:10:37.136 [INFO][9] startup/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.173.115/17, detected by connecting to 192.168.128.1
2023-03-09 06:10:37.137 [INFO][9] startup/startup.go 561: Node IPv4 changed, will check for conflicts
2023-03-09 06:10:37.140 [INFO][9] startup/startup.go 701: No AS number configured on node resource, using global value
2023-03-09 06:10:37.153 [INFO][9] startup/startup.go 750: found v6= in the kubeadm config map
2023-03-09 06:10:37.159 [INFO][9] startup/startup.go 676: FELIX_IPV6SUPPORT is false through environment variable
2023-03-09 06:10:37.168 [INFO][9] startup/startup.go 218: Using node name: lke96996-146211-640977a3e82e
2023-03-09 06:10:37.168 [INFO][9] startup/utils.go 191: Setting NetworkUnavailable to false
2023-03-09 06:10:37.214 [INFO][15] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "wireguardmtu"="0"
2023-03-09 06:10:37.214 [INFO][15] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "vxlanmtu"="0"
2023-03-09 06:10:37.214 [INFO][15] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "healthenabled"="true"
2023-03-09 06:10:37.214 [INFO][15] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "defaultendpointtohostaction"="ACCEPT"
2023-03-09 06:10:37.214 [INFO][15] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "ipinipmtu"="0"
2023-03-09 06:10:37.214 [INFO][15] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "ipv6support"="false"
2023-03-09 06:10:37.214 [INFO][15] tunnel-ip-allocator/config_params.go 435: Merging in config from environment variable: map[defaultendpointtohostaction:ACCEPT healthenabled:true ipinipmtu:0 ipv6support:false vxlanmtu:0 wireguardmtu:0]
2023-03-09 06:10:37.214 [INFO][15] tunnel-ip-allocator/config_params.go 542: Parsing value for WireguardMTU: 0 (from environment variable)
2023-03-09 06:10:37.215 [INFO][15] tunnel-ip-allocator/config_params.go 578: Parsed value for WireguardMTU: 0 (from environment variable)
2023-03-09 06:10:37.215 [INFO][15] tunnel-ip-allocator/config_params.go 542: Parsing value for VXLANMTU: 0 (from environment variable)
2023-03-09 06:10:37.215 [INFO][15] tunnel-ip-allocator/config_params.go 578: Parsed value for VXLANMTU: 0 (from environment variable)
2023-03-09 06:10:37.215 [INFO][15] tunnel-ip-allocator/config_params.go 542: Parsing value for HealthEnabled: true (from environment variable)
2023-03-09 06:10:37.215 [INFO][15] tunnel-ip-allocator/config_params.go 578: Parsed value for HealthEnabled: true (from environment variable)
2023-03-09 06:10:37.215 [INFO][15] tunnel-ip-allocator/config_params.go 542: Parsing value for DefaultEndpointToHostAction: ACCEPT (from environment variable)
2023-03-09 06:10:37.215 [INFO][15] tunnel-ip-allocator/config_params.go 578: Parsed value for DefaultEndpointToHostAction: ACCEPT (from environment variable)
2023-03-09 06:10:37.215 [INFO][15] tunnel-ip-allocator/config_params.go 542: Parsing value for IpInIpMtu: 0 (from environment variable)
2023-03-09 06:10:37.215 [INFO][15] tunnel-ip-allocator/config_params.go 578: Parsed value for IpInIpMtu: 0 (from environment variable)
2023-03-09 06:10:37.215 [INFO][15] tunnel-ip-allocator/config_params.go 542: Parsing value for Ipv6Support: false (from environment variable)
2023-03-09 06:10:37.215 [INFO][15] tunnel-ip-allocator/config_params.go 578: Parsed value for Ipv6Support: false (from environment variable)
mkdir: cannot create directory '/etc/service/enabled': File exists
Calico node started successfully
bird: bird: Unable to open configuration file /etc/calico/confd/config/bird.cfg: No such file or directory
Unable to open configuration file /etc/calico/confd/config/bird6.cfg: No such file or directory
2023-03-09 06:10:38.493 [INFO][65] status-reporter/startup.go 427: Early log level set to info
2023-03-09 06:10:38.513 [INFO][60] monitor-addresses/startup.go 427: Early log level set to info
2023-03-09 06:10:38.545 [INFO][65] status-reporter/watchersyncer.go 89: Start called
2023-03-09 06:10:38.545 [INFO][65] status-reporter/watchersyncer.go 130: Sending status update Status=wait-for-ready
2023-03-09 06:10:38.546 [INFO][63] confd/config.go 82: Skipping confd config file.
2023-03-09 06:10:38.546 [INFO][63] confd/run.go 18: Starting calico-confd
2023-03-09 06:10:38.552 [INFO][60] monitor-addresses/utils.go 127: Using NODENAME environment for node name lke96996-146211-640977a3e82e
2023-03-09 06:10:38.572 [INFO][60] monitor-addresses/utils.go 139: Determined node name: lke96996-146211-640977a3e82e
2023-03-09 06:10:38.573 [INFO][65] status-reporter/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/caliconodestatuses"
2023-03-09 06:10:38.545 [INFO][65] status-reporter/watchersyncer.go 149: Starting main event processing loop
2023-03-09 06:10:38.579 [INFO][61] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "wireguardmtu"="0"
2023-03-09 06:10:38.584 [INFO][61] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "vxlanmtu"="0"
2023-03-09 06:10:38.585 [INFO][61] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "healthenabled"="true"
2023-03-09 06:10:38.585 [INFO][61] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "defaultendpointtohostaction"="ACCEPT"
2023-03-09 06:10:38.585 [INFO][61] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "ipinipmtu"="0"
2023-03-09 06:10:38.585 [INFO][61] tunnel-ip-allocator/env_var_loader.go 40: Found felix environment variable: "ipv6support"="false"
2023-03-09 06:10:38.585 [INFO][61] tunnel-ip-allocator/config_params.go 435: Merging in config from environment variable: map[defaultendpointtohostaction:ACCEPT healthenabled:true ipinipmtu:0 ipv6support:false vxlanmtu:0 wireguardmtu:0]
2023-03-09 06:10:38.585 [INFO][61] tunnel-ip-allocator/config_params.go 542: Parsing value for WireguardMTU: 0 (from environment variable)
2023-03-09 06:10:38.585 [INFO][61] tunnel-ip-allocator/config_params.go 578: Parsed value for WireguardMTU: 0 (from environment variable)
2023-03-09 06:10:38.585 [INFO][61] tunnel-ip-allocator/config_params.go 542: Parsing value for VXLANMTU: 0 (from environment variable)
2023-03-09 06:10:38.586 [INFO][61] tunnel-ip-allocator/config_params.go 578: Parsed value for VXLANMTU: 0 (from environment variable)
2023-03-09 06:10:38.586 [INFO][61] tunnel-ip-allocator/config_params.go 542: Parsing value for HealthEnabled: true (from environment variable)
2023-03-09 06:10:38.586 [INFO][61] tunnel-ip-allocator/config_params.go 578: Parsed value for HealthEnabled: true (from environment variable)
2023-03-09 06:10:38.586 [INFO][61] tunnel-ip-allocator/config_params.go 542: Parsing value for DefaultEndpointToHostAction: ACCEPT (from environment variable)
2023-03-09 06:10:38.586 [INFO][61] tunnel-ip-allocator/config_params.go 578: Parsed value for DefaultEndpointToHostAction: ACCEPT (from environment variable)
2023-03-09 06:10:38.586 [INFO][61] tunnel-ip-allocator/config_params.go 542: Parsing value for IpInIpMtu: 0 (from environment variable)
2023-03-09 06:10:38.587 [INFO][61] tunnel-ip-allocator/config_params.go 578: Parsed value for IpInIpMtu: 0 (from environment variable)
2023-03-09 06:10:38.587 [INFO][61] tunnel-ip-allocator/config_params.go 542: Parsing value for Ipv6Support: false (from environment variable)
2023-03-09 06:10:38.587 [INFO][61] tunnel-ip-allocator/config_params.go 578: Parsed value for Ipv6Support: false (from environment variable)
2023-03-09 06:10:38.593 [INFO][65] status-reporter/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/caliconodestatuses"
2023-03-09 06:10:38.593 [INFO][65] status-reporter/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:38.593 [INFO][65] status-reporter/watchersyncer.go 130: Sending status update Status=resync
2023-03-09 06:10:38.593 [INFO][65] status-reporter/watchersyncer.go 221: All watchers have sync'd data - sending data and final sync
2023-03-09 06:10:38.593 [INFO][65] status-reporter/watchersyncer.go 130: Sending status update Status=in-sync
W0309 06:10:38.611836      63 client_config.go:617] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
2023-03-09 06:10:38.617 [INFO][63] confd/client.go 1419: Advertise global service ranges from this node
2023-03-09 06:10:38.621 [INFO][63] confd/client.go 1364: Updated with new cluster IP CIDRs: []
2023-03-09 06:10:38.622 [INFO][63] confd/client.go 1419: Advertise global service ranges from this node
2023-03-09 06:10:38.622 [INFO][63] confd/client.go 1355: Updated with new external IP CIDRs: []
2023-03-09 06:10:38.622 [INFO][63] confd/client.go 1419: Advertise global service ranges from this node
2023-03-09 06:10:38.627 [INFO][63] confd/client.go 1374: Updated with new Loadbalancer IP CIDRs: []
2023-03-09 06:10:38.627 [INFO][63] confd/watchersyncer.go 89: Start called
2023-03-09 06:10:38.627 [INFO][63] confd/client.go 422: Source SourceRouteGenerator readiness changed, ready=true
2023-03-09 06:10:38.627 [INFO][63] confd/watchersyncer.go 130: Sending status update Status=wait-for-ready
2023-03-09 06:10:38.627 [INFO][63] confd/watchersyncer.go 149: Starting main event processing loop
2023-03-09 06:10:38.627 [INFO][63] confd/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/bgppeers"
2023-03-09 06:10:38.631 [INFO][63] confd/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/ippools"
2023-03-09 06:10:38.631 [INFO][63] confd/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/bgpconfigurations"
2023-03-09 06:10:38.631 [INFO][63] confd/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/nodes"
2023-03-09 06:10:38.632 [INFO][63] confd/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/bgppeers"
2023-03-09 06:10:38.632 [INFO][63] confd/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:38.632 [INFO][63] confd/watchersyncer.go 130: Sending status update Status=resync
2023-03-09 06:10:38.635 [INFO][63] confd/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/ippools"
2023-03-09 06:10:38.636 [INFO][63] confd/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/nodes"
2023-03-09 06:10:38.636 [INFO][63] confd/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/bgpconfigurations"
2023-03-09 06:10:38.636 [INFO][63] confd/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:38.636 [INFO][63] confd/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:38.636 [INFO][63] confd/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:38.636 [INFO][63] confd/watchersyncer.go 221: All watchers have sync'd data - sending data and final sync
2023-03-09 06:10:38.636 [INFO][63] confd/watchersyncer.go 130: Sending status update Status=in-sync
2023-03-09 06:10:38.636 [INFO][63] confd/client.go 995: Recompute BGP peerings: HostBGPConfig(node=lke96996-146211-640977a3930b; name=ip_addr_v4) updated; HostBGPConfig(node=lke96996-146211-640977a3930b; name=ip_addr_v6) updated; HostBGPConfig(node=lke96996-146211-640977a3930b; name=network_v4) updated; HostBGPConfig(node=lke96996-146211-640977a3930b; name=rr_cluster_id) updated; BlockAffinityKey(cidr=10.2.0.0/24, host=lke96996-146211-640977a3930b) updated; lke96996-146211-640977a3930b updated; HostBGPConfig(node=lke96996-146211-640977a3e82e; name=ip_addr_v4) updated; HostBGPConfig(node=lke96996-146211-640977a3e82e; name=ip_addr_v6) updated; HostBGPConfig(node=lke96996-146211-640977a3e82e; name=network_v4) updated; HostBGPConfig(node=lke96996-146211-640977a3e82e; name=rr_cluster_id) updated; BlockAffinityKey(cidr=10.2.1.0/24, host=lke96996-146211-640977a3e82e) updated; lke96996-146211-640977a3e82e updated
2023-03-09 06:10:38.636 [INFO][63] confd/client.go 422: Source SourceSyncer readiness changed, ready=true
2023-03-09 06:10:38.636 [INFO][63] confd/client.go 442: Data is now syncd, can start rendering templates
W0309 06:10:38.698496      62 client_config.go:617] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
2023-03-09 06:10:38.742 [INFO][70] felix/daemon.go 373: Successfully loaded configuration. GOMAXPROCS=1 builddate="2022-11-08T00:15:45+0000" config=&config.Config{UseInternalDataplaneDriver:true, DataplaneDriver:"calico-iptables-plugin", DataplaneWatchdogTimeout:90000000000, WireguardEnabled:false, WireguardEnabledV6:false, WireguardListeningPort:51820, WireguardListeningPortV6:51821, WireguardRoutingRulePriority:99, WireguardInterfaceName:"wireguard.cali", WireguardInterfaceNameV6:"wg-v6.cali", WireguardMTU:0, WireguardMTUV6:0, WireguardHostEncryptionEnabled:false, WireguardPersistentKeepAlive:0, BPFEnabled:false, BPFDisableUnprivileged:true, BPFLogLevel:"off", BPFDataIfacePattern:(*regexp.Regexp)(0xc0005400a0), BPFConnectTimeLoadBalancingEnabled:true, BPFExternalServiceMode:"tunnel", BPFKubeProxyIptablesCleanupEnabled:true, BPFKubeProxyMinSyncPeriod:1000000000, BPFKubeProxyEndpointSlicesEnabled:true, BPFExtToServiceConnmark:0, BPFPSNATPorts:numorstring.Port{MinPort:0x4e20, MaxPort:0x752f, PortName:""}, BPFMapSizeNATFrontend:65536, BPFMapSizeNATBackend:262144, BPFMapSizeNATAffinity:65536, BPFMapSizeRoute:262144, BPFMapSizeConntrack:512000, BPFMapSizeIPSets:1048576, BPFMapSizeIfState:1000, BPFHostConntrackBypass:true, BPFEnforceRPF:"Strict", BPFPolicyDebugEnabled:true, DebugBPFCgroupV2:"", DebugBPFMapRepinEnabled:false, DatastoreType:"kubernetes", FelixHostname:"lke96996-146211-640977a3e82e", EtcdAddr:"127.0.0.1:2379", EtcdScheme:"http", EtcdKeyFile:"", EtcdCertFile:"", EtcdCaFile:"", EtcdEndpoints:[]string(nil), TyphaAddr:"", TyphaK8sServiceName:"", TyphaK8sNamespace:"kube-system", TyphaReadTimeout:30000000000, TyphaWriteTimeout:10000000000, TyphaKeyFile:"", TyphaCertFile:"", TyphaCAFile:"", TyphaCN:"", TyphaURISAN:"", Ipv6Support:false, BpfIpv6Support:false, IptablesBackend:"auto", RouteRefreshInterval:90000000000, InterfaceRefreshInterval:90000000000, DeviceRouteSourceAddress:net.IP(nil), DeviceRouteSourceAddressIPv6:net.IP(nil), DeviceRouteProtocol:3, RemoveExternalRoutes:true, IptablesRefreshInterval:90000000000, IptablesPostWriteCheckIntervalSecs:1000000000, IptablesLockFilePath:"/run/xtables.lock", IptablesLockTimeoutSecs:0, IptablesLockProbeIntervalMillis:50000000, FeatureDetectOverride:map[string]string(nil), IpsetsRefreshInterval:10000000000, MaxIpsetSize:1048576, XDPRefreshInterval:90000000000, PolicySyncPathPrefix:"", NetlinkTimeoutSecs:10000000000, MetadataAddr:"", MetadataPort:8775, OpenstackRegion:"", InterfacePrefix:"cali", InterfaceExclude:[]*regexp.Regexp{(*regexp.Regexp)(0xc0005401e0)}, ChainInsertMode:"insert", DefaultEndpointToHostAction:"ACCEPT", IptablesFilterAllowAction:"ACCEPT", IptablesMangleAllowAction:"ACCEPT", LogPrefix:"calico-packet", LogFilePath:"", LogSeverityFile:"", LogSeverityScreen:"INFO", LogSeveritySys:"", LogDebugFilenameRegex:(*regexp.Regexp)(nil), VXLANEnabled:(*bool)(nil), VXLANPort:4789, VXLANVNI:4096, VXLANMTU:0, VXLANMTUV6:0, IPv4VXLANTunnelAddr:net.IP(nil), IPv6VXLANTunnelAddr:net.IP(nil), VXLANTunnelMACAddr:"", VXLANTunnelMACAddrV6:"", IpInIpEnabled:(*bool)(nil), IpInIpMtu:0, IpInIpTunnelAddr:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xa, 0x2, 0x1, 0x1}, FloatingIPs:"Disabled", AllowVXLANPacketsFromWorkloads:false, AllowIPIPPacketsFromWorkloads:false, AWSSrcDstCheck:"DoNothing", ServiceLoopPrevention:"Drop", WorkloadSourceSpoofing:"Disabled", ReportingIntervalSecs:0, ReportingTTLSecs:90000000000, EndpointReportingEnabled:false, EndpointReportingDelaySecs:1000000000, IptablesMarkMask:0xffff0000, DisableConntrackInvalidCheck:false, HealthEnabled:true, HealthPort:9099, HealthHost:"localhost", PrometheusMetricsEnabled:false, PrometheusMetricsHost:"", PrometheusMetricsPort:9091, PrometheusGoMetricsEnabled:true, PrometheusProcessMetricsEnabled:true, PrometheusWireGuardMetricsEnabled:true, FailsafeInboundHostPorts:[]config.ProtoPort{config.ProtoPort{Net:"", Protocol:"tcp", Port:0x16}, config.ProtoPort{Net:"", Protocol:"udp", Port:0x44}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0xb3}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94c}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1561}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x192b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0a}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0b}}, FailsafeOutboundHostPorts:[]config.ProtoPort{config.ProtoPort{Net:"", Protocol:"udp", Port:0x35}, config.ProtoPort{Net:"", Protocol:"udp", Port:0x43}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0xb3}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94c}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1561}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x192b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0a}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0b}}, KubeNodePortRanges:[]numorstring.Port{numorstring.Port{MinPort:0x7530, MaxPort:0x7fff, PortName:""}}, NATPortRange:numorstring.Port{MinPort:0x0, MaxPort:0x0, PortName:""}, NATOutgoingAddress:net.IP(nil), UsageReportingEnabled:true, UsageReportingInitialDelaySecs:300000000000, UsageReportingIntervalSecs:86400000000000, ClusterGUID:"7197fc97702c482f9a5df8fb5c03809a", ClusterType:"k8s,bgp,kubeadm,kdd", CalicoVersion:"v3.24.5", ExternalNodesCIDRList:[]string(nil), DebugMemoryProfilePath:"", DebugCPUProfilePath:"/tmp/felix-cpu-<timestamp>.pprof", DebugDisableLogDropping:false, DebugSimulateCalcGraphHangAfter:0, DebugSimulateDataplaneHangAfter:0, DebugPanicAfter:0, DebugSimulateDataRace:false, RouteSource:"CalicoIPAM", RouteTableRange:idalloc.IndexRange{Min:0, Max:0}, RouteTableRanges:[]idalloc.IndexRange(nil), RouteSyncDisabled:false, IptablesNATOutgoingInterfaceFilter:"", SidecarAccelerationEnabled:false, XDPEnabled:true, GenericXDPEnabled:false, Variant:"Calico", MTUIfacePattern:(*regexp.Regexp)(0xc000540460), Encapsulation:config.Encapsulation{IPIPEnabled:true, VXLANEnabled:false, VXLANEnabledV6:false}, internalOverrides:map[string]string{}, sourceToRawConfig:map[config.Source]map[string]string{0x1:map[string]string{"CalicoVersion":"v3.24.5", "ClusterGUID":"7197fc97702c482f9a5df8fb5c03809a", "ClusterType":"k8s,bgp,kubeadm,kdd", "FloatingIPs":"Disabled", "LogSeverityScreen":"Info", "ReportingIntervalSecs":"0"}, 0x2:map[string]string{"IpInIpTunnelAddr":"10.2.1.1"}, 0x3:map[string]string{"LogFilePath":"None", "LogSeverityFile":"None", "LogSeveritySys":"None", "MetadataAddr":"None"}, 0x4:map[string]string{"datastoretype":"kubernetes", "defaultendpointtohostaction":"ACCEPT", "felixhostname":"lke96996-146211-640977a3e82e", "healthenabled":"true", "ipinipmtu":"0", "ipv6support":"false", "vxlanmtu":"0", "wireguardmtu":"0"}}, rawValues:map[string]string{"CalicoVersion":"v3.24.5", "ClusterGUID":"7197fc97702c482f9a5df8fb5c03809a", "ClusterType":"k8s,bgp,kubeadm,kdd", "DatastoreType":"kubernetes", "DefaultEndpointToHostAction":"ACCEPT", "FelixHostname":"lke96996-146211-640977a3e82e", "FloatingIPs":"Disabled", "HealthEnabled":"true", "IpInIpMtu":"0", "IpInIpTunnelAddr":"10.2.1.1", "Ipv6Support":"false", "LogFilePath":"None", "LogSeverityFile":"None", "LogSeverityScreen":"Info", "LogSeveritySys":"None", "MetadataAddr":"None", "ReportingIntervalSecs":"0", "VXLANMTU":"0", "WireguardMTU":"0"}, Err:error(nil), loadClientConfigFromEnvironment:(func() (*apiconfig.CalicoAPIConfig, error))(0x144f0c0), useNodeResourceUpdates:false} gitcommit="f1a1611acb98d9187f48bbbe2227301aa69f0499" version="v3.24.5"
2023-03-09 06:10:38.770 [INFO][62] cni-config-monitor/token_watch.go 225: Update of CNI kubeconfig triggered based on elapsed time.
2023-03-09 06:10:38.791 [ERROR][63] confd/resource.go 306: Error from checkcmd "bird6 -p -c /etc/calico/confd/config/.bird6.cfg806902688": "bird: /etc/calico/confd/config/.bird6.cfg806902688:6:1 Unable to open included file /etc/calico/confd/config/bird6_aggr.cfg: No such file or directory\n"
2023-03-09 06:10:38.791 [INFO][63] confd/resource.go 240: Check failed, but file does not yet exist - create anyway
2023-03-09 06:10:38.792 [INFO][62] cni-config-monitor/token_watch.go 279: Wrote updated CNI kubeconfig file. path="/host/etc/cni/net.d/calico-kubeconfig"
2023-03-09 06:10:38.777 [INFO][70] felix/bootstrap.go 209: Wireguard is not enabled - ensure no wireguard config iface="wireguard.cali" ipVersion=0x4 nodeName="lke96996-146211-640977a3e82e"
2023-03-09 06:10:38.795 [INFO][70] felix/bootstrap.go 624: Wireguard public key not set in datastore ipVersion=0x4 nodeName="lke96996-146211-640977a3e82e"
2023-03-09 06:10:38.796 [INFO][63] confd/resource.go 278: Target config /etc/calico/confd/config/bird6.cfg has been updated
2023-03-09 06:10:38.796 [INFO][63] confd/resource.go 278: Target config /etc/calico/confd/config/bird6_aggr.cfg has been updated
2023-03-09 06:10:38.796 [INFO][63] confd/resource.go 278: Target config /etc/calico/confd/config/bird_ipam.cfg has been updated
2023-03-09 06:10:38.796 [INFO][63] confd/resource.go 278: Target config /etc/calico/confd/config/bird_aggr.cfg has been updated
2023-03-09 06:10:38.796 [INFO][63] confd/resource.go 278: Target config /etc/calico/confd/config/bird6_ipam.cfg has been updated
2023-03-09 06:10:38.797 [ERROR][63] confd/resource.go 306: Error from checkcmd "bird -p -c /etc/calico/confd/config/.bird.cfg2899201397": "bird: /etc/calico/confd/config/.bird.cfg2899201397:6:1 Unable to open included file /etc/calico/confd/config/bird_aggr.cfg: No such file or directory\n"
2023-03-09 06:10:38.797 [INFO][63] confd/resource.go 240: Check failed, but file does not yet exist - create anyway
2023-03-09 06:10:38.795 [INFO][70] felix/bootstrap.go 209: Wireguard is not enabled - ensure no wireguard config iface="wg-v6.cali" ipVersion=0x6 nodeName="lke96996-146211-640977a3e82e"
2023-03-09 06:10:38.798 [INFO][63] confd/resource.go 278: Target config /etc/calico/confd/config/bird.cfg has been updated
2023-03-09 06:10:38.800 [INFO][70] felix/bootstrap.go 624: Wireguard public key not set in datastore ipVersion=0x6 nodeName="lke96996-146211-640977a3e82e"
2023-03-09 06:10:38.800 [INFO][70] felix/driver.go 72: Using internal (linux) dataplane driver.
2023-03-09 06:10:38.800 [INFO][70] felix/driver.go 157: Calculated iptables mark bits acceptMark=0x10000 endpointMark=0xfff00000 endpointMarkNonCali=0x0 passMark=0x20000 scratch0Mark=0x40000 scratch1Mark=0x80000
2023-03-09 06:10:38.800 [INFO][70] felix/int_dataplane.go 336: Creating internal dataplane driver. config=intdataplane.Config{Hostname:"lke96996-146211-640977a3e82e", IPv6Enabled:false, RuleRendererOverride:rules.RuleRenderer(nil), IPIPMTU:0, VXLANMTU:0, VXLANMTUV6:0, VXLANPort:4789, MaxIPSetSize:1048576, RouteSyncDisabled:false, IptablesBackend:"auto", IPSetsRefreshInterval:10000000000, RouteRefreshInterval:90000000000, DeviceRouteSourceAddress:net.IP(nil), DeviceRouteSourceAddressIPv6:net.IP(nil), DeviceRouteProtocol:3, RemoveExternalRoutes:true, IptablesRefreshInterval:90000000000, IptablesPostWriteCheckInterval:1000000000, IptablesInsertMode:"insert", IptablesLockFilePath:"/run/xtables.lock", IptablesLockTimeout:0, IptablesLockProbeInterval:50000000, XDPRefreshInterval:90000000000, FloatingIPsEnabled:false, Wireguard:wireguard.Config{Enabled:false, EnabledV6:false, ListeningPort:51820, ListeningPortV6:51821, FirewallMark:0, RoutingRulePriority:99, RoutingTableIndex:1, RoutingTableIndexV6:2, InterfaceName:"wireguard.cali", InterfaceNameV6:"wg-v6.cali", MTU:0, MTUV6:0, RouteSource:"CalicoIPAM", EncryptHostTraffic:false, PersistentKeepAlive:0, RouteSyncDisabled:false}, NetlinkTimeout:10000000000, RulesConfig:rules.Config{IPSetConfigV4:(*ipsets.IPVersionConfig)(0xc0002a7310), IPSetConfigV6:(*ipsets.IPVersionConfig)(0xc0002a7400), WorkloadIfacePrefixes:[]string{"cali"}, IptablesMarkAccept:0x10000, IptablesMarkPass:0x20000, IptablesMarkScratch0:0x40000, IptablesMarkScratch1:0x80000, IptablesMarkEndpoint:0xfff00000, IptablesMarkNonCaliEndpoint:0x0, KubeNodePortRanges:[]numorstring.Port{numorstring.Port{MinPort:0x7530, MaxPort:0x7fff, PortName:""}}, KubeIPVSSupportEnabled:false, OpenStackMetadataIP:net.IP(nil), OpenStackMetadataPort:0x2247, OpenStackSpecialCasesEnabled:false, VXLANEnabled:false, VXLANEnabledV6:false, VXLANPort:4789, VXLANVNI:4096, IPIPEnabled:true, FelixConfigIPIPEnabled:(*bool)(nil), IPIPTunnelAddress:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xa, 0x2, 0x1, 0x1}, VXLANTunnelAddress:net.IP(nil), VXLANTunnelAddressV6:net.IP(nil), AllowVXLANPacketsFromWorkloads:false, AllowIPIPPacketsFromWorkloads:false, WireguardEnabled:false, WireguardEnabledV6:false, WireguardInterfaceName:"wireguard.cali", WireguardInterfaceNameV6:"wg-v6.cali", WireguardIptablesMark:0x0, WireguardListeningPort:51820, WireguardListeningPortV6:51821, WireguardEncryptHostTraffic:false, RouteSource:"CalicoIPAM", IptablesLogPrefix:"calico-packet", EndpointToHostAction:"ACCEPT", IptablesFilterAllowAction:"ACCEPT", IptablesMangleAllowAction:"ACCEPT", FailsafeInboundHostPorts:[]config.ProtoPort{config.ProtoPort{Net:"", Protocol:"tcp", Port:0x16}, config.ProtoPort{Net:"", Protocol:"udp", Port:0x44}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0xb3}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94c}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1561}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x192b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0a}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0b}}, FailsafeOutboundHostPorts:[]config.ProtoPort{config.ProtoPort{Net:"", Protocol:"udp", Port:0x35}, config.ProtoPort{Net:"", Protocol:"udp", Port:0x43}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0xb3}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94c}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1561}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x192b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0a}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0b}}, DisableConntrackInvalid:false, NATPortRange:numorstring.Port{MinPort:0x0, MaxPort:0x0, PortName:""}, IptablesNATOutgoingInterfaceFilter:"", NATOutgoingAddress:net.IP(nil), BPFEnabled:false, ServiceLoopPrevention:"Drop"}, IfaceMonitorConfig:ifacemonitor.Config{InterfaceExcludes:[]*regexp.Regexp{(*regexp.Regexp)(0xc0005401e0)}, ResyncInterval:90000000000}, StatusReportingInterval:0, ConfigChangedRestartCallback:(func())(0x259e3a0), FatalErrorRestartCallback:(func(error))(0x259e280), PostInSyncCallback:(func())(0x258bf40), HealthAggregator:(*health.HealthAggregator)(0xc000604780), WatchdogTimeout:90000000000, RouteTableManager:(*idalloc.IndexAllocator)(0xc00021e3e0), DebugSimulateDataplaneHangAfter:0, ExternalNodesCidrs:[]string(nil), BPFEnabled:false, BPFPolicyDebugEnabled:true, BPFDisableUnprivileged:true, BPFKubeProxyIptablesCleanupEnabled:true, BPFLogLevel:"off", BPFExtToServiceConnmark:0, BPFDataIfacePattern:(*regexp.Regexp)(0xc0005400a0), XDPEnabled:true, XDPAllowGeneric:false, BPFConntrackTimeouts:conntrack.Timeouts{CreationGracePeriod:10000000000, TCPPreEstablished:20000000000, TCPEstablished:3600000000000, TCPFinsSeen:30000000000, TCPResetSeen:40000000000, UDPLastSeen:60000000000, GenericIPLastSeen:600000000000, ICMPLastSeen:5000000000}, BPFCgroupV2:"", BPFConnTimeLBEnabled:true, BPFMapRepin:false, BPFNodePortDSREnabled:false, BPFPSNATPorts:numorstring.Port{MinPort:0x4e20, MaxPort:0x752f, PortName:""}, BPFMapSizeRoute:262144, BPFMapSizeConntrack:512000, BPFMapSizeNATFrontend:65536, BPFMapSizeNATBackend:262144, BPFMapSizeNATAffinity:65536, BPFMapSizeIPSets:1048576, BPFMapSizeIfState:1000, BPFIpv6Enabled:false, BPFHostConntrackBypass:true, BPFEnforceRPF:"Strict", KubeProxyMinSyncPeriod:1000000000, SidecarAccelerationEnabled:false, LookPathOverride:(func(string) (string, error))(nil), KubeClientSet:(*kubernetes.Clientset)(0xc0000e5200), FeatureDetectOverrides:map[string]string(nil), hostMTU:0, MTUIfacePattern:(*regexp.Regexp)(0xc000540460), RouteSource:"CalicoIPAM", KubernetesProvider:0x0}
2023-03-09 06:10:38.801 [INFO][70] felix/rule_defs.go 373: Creating rule renderer. config=rules.Config{IPSetConfigV4:(*ipsets.IPVersionConfig)(0xc0002a7310), IPSetConfigV6:(*ipsets.IPVersionConfig)(0xc0002a7400), WorkloadIfacePrefixes:[]string{"cali"}, IptablesMarkAccept:0x10000, IptablesMarkPass:0x20000, IptablesMarkScratch0:0x40000, IptablesMarkScratch1:0x80000, IptablesMarkEndpoint:0xfff00000, IptablesMarkNonCaliEndpoint:0x0, KubeNodePortRanges:[]numorstring.Port{numorstring.Port{MinPort:0x7530, MaxPort:0x7fff, PortName:""}}, KubeIPVSSupportEnabled:false, OpenStackMetadataIP:net.IP(nil), OpenStackMetadataPort:0x2247, OpenStackSpecialCasesEnabled:false, VXLANEnabled:false, VXLANEnabledV6:false, VXLANPort:4789, VXLANVNI:4096, IPIPEnabled:true, FelixConfigIPIPEnabled:(*bool)(nil), IPIPTunnelAddress:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xa, 0x2, 0x1, 0x1}, VXLANTunnelAddress:net.IP(nil), VXLANTunnelAddressV6:net.IP(nil), AllowVXLANPacketsFromWorkloads:false, AllowIPIPPacketsFromWorkloads:false, WireguardEnabled:false, WireguardEnabledV6:false, WireguardInterfaceName:"wireguard.cali", WireguardInterfaceNameV6:"wg-v6.cali", WireguardIptablesMark:0x0, WireguardListeningPort:51820, WireguardListeningPortV6:51821, WireguardEncryptHostTraffic:false, RouteSource:"CalicoIPAM", IptablesLogPrefix:"calico-packet", EndpointToHostAction:"ACCEPT", IptablesFilterAllowAction:"ACCEPT", IptablesMangleAllowAction:"ACCEPT", FailsafeInboundHostPorts:[]config.ProtoPort{config.ProtoPort{Net:"", Protocol:"tcp", Port:0x16}, config.ProtoPort{Net:"", Protocol:"udp", Port:0x44}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0xb3}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94c}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1561}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x192b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0a}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0b}}, FailsafeOutboundHostPorts:[]config.ProtoPort{config.ProtoPort{Net:"", Protocol:"udp", Port:0x35}, config.ProtoPort{Net:"", Protocol:"udp", Port:0x43}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0xb3}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x94c}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1561}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x192b}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0a}, config.ProtoPort{Net:"", Protocol:"tcp", Port:0x1a0b}}, DisableConntrackInvalid:false, NATPortRange:numorstring.Port{MinPort:0x0, MaxPort:0x0, PortName:""}, IptablesNATOutgoingInterfaceFilter:"", NATOutgoingAddress:net.IP(nil), BPFEnabled:false, ServiceLoopPrevention:"Drop"}
2023-03-09 06:10:38.801 [INFO][70] felix/rule_defs.go 383: Workload to host packets will be accepted.
2023-03-09 06:10:38.801 [INFO][70] felix/rule_defs.go 397: filter table allowed packets will be accepted immediately.
2023-03-09 06:10:38.801 [INFO][70] felix/rule_defs.go 405: mangle table allowed packets will be accepted immediately.
2023-03-09 06:10:38.801 [INFO][70] felix/rule_defs.go 413: Packets to unknown service IPs will be dropped
2023-03-09 06:10:38.801 [INFO][70] felix/int_dataplane.go 1020: Determined pod MTU mtu=1480
2023-03-09 06:10:38.801 [INFO][70] felix/iface_monitor.go 84: configured to periodically rescan interfaces. interval=1m30s
2023-03-09 06:10:38.801 [INFO][70] felix/feature_detect.go 354: Looked up iptables command backendMode="legacy" candidates=[]string{"ip6tables-legacy-save", "ip6tables-save"} command="ip6tables-legacy-save" ipVersion=0x6 saveOrRestore="save"
2023-03-09 06:10:38.801 [INFO][70] felix/feature_detect.go 354: Looked up iptables command backendMode="legacy" candidates=[]string{"iptables-legacy-save", "iptables-save"} command="iptables-legacy-save" ipVersion=0x4 saveOrRestore="save"
2023-03-09 06:10:38.804 [INFO][70] felix/feature_detect.go 354: Looked up iptables command backendMode="nft" candidates=[]string{"ip6tables-nft-save", "ip6tables-save"} command="ip6tables-nft-save" ipVersion=0x6 saveOrRestore="save"
2023-03-09 06:10:38.804 [INFO][70] felix/feature_detect.go 354: Looked up iptables command backendMode="nft" candidates=[]string{"iptables-nft-save", "iptables-save"} command="iptables-nft-save" ipVersion=0x4 saveOrRestore="save"
2023-03-09 06:10:38.843 [INFO][70] felix/feature_detect.go 163: Updating detected iptables features features=environment.Features{SNATFullyRandom:true, MASQFullyRandom:true, RestoreSupportsLock:true, ChecksumOffloadBroken:false, IPIPDeviceIsL3:false} iptablesVersion=1.8.4 kernelVersion=5.10.0-19
2023-03-09 06:10:38.843 [INFO][70] felix/table.go 336: Calculated old-insert detection regex. pattern="(?:-j|--jump) cali-|(?:-j|--jump) califw-|(?:-j|--jump) calitw-|(?:-j|--jump) califh-|(?:-j|--jump) calith-|(?:-j|--jump) calipi-|(?:-j|--jump) calipo-|(?:-j|--jump) felix-"
2023-03-09 06:10:38.844 [INFO][70] felix/table.go 449: Enabling iptables-in-nftables-mode workarounds.
2023-03-09 06:10:38.845 [INFO][70] felix/feature_detect.go 354: Looked up iptables command backendMode="nft" candidates=[]string{"iptables-nft-restore", "iptables-restore"} command="iptables-nft-restore" ipVersion=0x4 saveOrRestore="restore"
2023-03-09 06:10:38.850 [INFO][70] felix/feature_detect.go 354: Looked up iptables command backendMode="nft" candidates=[]string{"iptables-nft-save", "iptables-save"} command="iptables-nft-save" ipVersion=0x4 saveOrRestore="save"
2023-03-09 06:10:38.852 [INFO][70] felix/table.go 336: Calculated old-insert detection regex. pattern="(?:-j|--jump) cali-|(?:-j|--jump) califw-|(?:-j|--jump) calitw-|(?:-j|--jump) califh-|(?:-j|--jump) calith-|(?:-j|--jump) calipi-|(?:-j|--jump) calipo-|(?:-j|--jump) felix-|-A POSTROUTING .* felix-masq-ipam-pools .*|-A POSTROUTING -o tunl0 -m addrtype ! --src-type LOCAL --limit-iface-out -m addrtype --src-type LOCAL -j MASQUERADE"
2023-03-09 06:10:38.853 [INFO][70] felix/table.go 449: Enabling iptables-in-nftables-mode workarounds.
2023-03-09 06:10:38.854 [INFO][70] felix/feature_detect.go 354: Looked up iptables command backendMode="nft" candidates=[]string{"iptables-nft-restore", "iptables-restore"} command="iptables-nft-restore" ipVersion=0x4 saveOrRestore="restore"
2023-03-09 06:10:38.854 [INFO][70] felix/feature_detect.go 354: Looked up iptables command backendMode="nft" candidates=[]string{"iptables-nft-save", "iptables-save"} command="iptables-nft-save" ipVersion=0x4 saveOrRestore="save"
2023-03-09 06:10:38.854 [INFO][70] felix/table.go 336: Calculated old-insert detection regex. pattern="(?:-j|--jump) cali-|(?:-j|--jump) califw-|(?:-j|--jump) calitw-|(?:-j|--jump) califh-|(?:-j|--jump) calith-|(?:-j|--jump) calipi-|(?:-j|--jump) calipo-|(?:-j|--jump) felix-"
2023-03-09 06:10:38.854 [INFO][70] felix/table.go 449: Enabling iptables-in-nftables-mode workarounds.
2023-03-09 06:10:38.854 [INFO][70] felix/feature_detect.go 354: Looked up iptables command backendMode="nft" candidates=[]string{"iptables-nft-restore", "iptables-restore"} command="iptables-nft-restore" ipVersion=0x4 saveOrRestore="restore"
2023-03-09 06:10:38.854 [INFO][70] felix/feature_detect.go 354: Looked up iptables command backendMode="nft" candidates=[]string{"iptables-nft-save", "iptables-save"} command="iptables-nft-save" ipVersion=0x4 saveOrRestore="save"
2023-03-09 06:10:38.854 [INFO][70] felix/table.go 336: Calculated old-insert detection regex. pattern="(?:-j|--jump) cali-|(?:-j|--jump) califw-|(?:-j|--jump) calitw-|(?:-j|--jump) califh-|(?:-j|--jump) calith-|(?:-j|--jump) calipi-|(?:-j|--jump) calipo-|(?:-j|--jump) felix-"
2023-03-09 06:10:38.854 [INFO][70] felix/table.go 449: Enabling iptables-in-nftables-mode workarounds.
2023-03-09 06:10:38.854 [INFO][70] felix/feature_detect.go 354: Looked up iptables command backendMode="nft" candidates=[]string{"iptables-nft-restore", "iptables-restore"} command="iptables-nft-restore" ipVersion=0x4 saveOrRestore="restore"
2023-03-09 06:10:38.854 [INFO][70] felix/feature_detect.go 354: Looked up iptables command backendMode="nft" candidates=[]string{"iptables-nft-save", "iptables-save"} command="iptables-nft-save" ipVersion=0x4 saveOrRestore="save"
2023-03-09 06:10:38.866 [INFO][70] felix/int_dataplane.go 514: XDP acceleration enabled.
2023-03-09 06:10:38.874 [INFO][70] felix/connecttime.go 54: Running bpftool to look up programs attached to cgroup args=[]string{"bpftool", "-j", "-p", "cgroup", "show", "/run/calico/cgroup"}
2023-03-09 06:10:38.880 [INFO][70] felix/route_table.go 317: Calculated interface name regexp ifaceRegex="^cali.*" ipVersion=0x4 tableIndex=0
2023-03-09 06:10:38.880 [INFO][70] felix/ipsets.go 130: Queueing IP set for creation family="inet" setID="all-ipam-pools" setType="hash:net"
2023-03-09 06:10:38.880 [INFO][70] felix/ipsets.go 130: Queueing IP set for creation family="inet" setID="masq-ipam-pools" setType="hash:net"
2023-03-09 06:10:38.880 [INFO][70] felix/route_table.go 317: Calculated interface name regexp ifaceRegex="^wireguard.cali$" ipVersion=0x4 tableIndex=1
2023-03-09 06:10:38.880 [INFO][70] felix/int_dataplane.go 918: Registering to report health.
2023-03-09 06:10:38.882 [INFO][70] felix/int_dataplane.go 1856: attempted to modprobe nf_conntrack_proto_sctp error=exit status 1 output=""
2023-03-09 06:10:38.882 [INFO][70] felix/int_dataplane.go 1858: Making sure IPv4 forwarding is enabled.
2023-03-09 06:10:38.882 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-failsafe-in" ipVersion=0x4 table="raw"
2023-03-09 06:10:38.882 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-failsafe-out" ipVersion=0x4 table="raw"
2023-03-09 06:10:38.882 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-PREROUTING" ipVersion=0x4 table="raw"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-rpf-skip"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-from-host-endpoint"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-wireguard-incoming-mark" ipVersion=0x4 table="raw"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-OUTPUT" ipVersion=0x4 table="raw"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-to-host-endpoint"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-PREROUTING"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-OUTPUT"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-FORWARD" ipVersion=0x4 table="filter"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-from-hep-forward"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-from-wl-dispatch"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-to-wl-dispatch"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-to-hep-forward"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-cidr-block"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-INPUT" ipVersion=0x4 table="filter"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-wl-to-host"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-from-host-endpoint"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-wl-to-host" ipVersion=0x4 table="filter"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-failsafe-in" ipVersion=0x4 table="filter"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-OUTPUT" ipVersion=0x4 table="filter"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-to-host-endpoint"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-failsafe-out" ipVersion=0x4 table="filter"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-FORWARD"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-INPUT"
2023-03-09 06:10:38.883 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-OUTPUT"
2023-03-09 06:10:38.884 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-PREROUTING" ipVersion=0x4 table="nat"
2023-03-09 06:10:38.884 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-fip-dnat"
2023-03-09 06:10:38.884 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-POSTROUTING" ipVersion=0x4 table="nat"
2023-03-09 06:10:38.884 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-fip-snat"
2023-03-09 06:10:38.884 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-nat-outgoing"
2023-03-09 06:10:38.884 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-OUTPUT" ipVersion=0x4 table="nat"
2023-03-09 06:10:38.884 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-PREROUTING"
2023-03-09 06:10:38.884 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-POSTROUTING"
2023-03-09 06:10:38.884 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-OUTPUT"
2023-03-09 06:10:38.884 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-failsafe-in" ipVersion=0x4 table="mangle"
2023-03-09 06:10:38.884 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-failsafe-out" ipVersion=0x4 table="mangle"
2023-03-09 06:10:38.884 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-PREROUTING" ipVersion=0x4 table="mangle"
2023-03-09 06:10:38.884 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-from-host-endpoint"
2023-03-09 06:10:38.884 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-POSTROUTING" ipVersion=0x4 table="mangle"
2023-03-09 06:10:38.884 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-to-host-endpoint"
2023-03-09 06:10:38.884 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-PREROUTING"
2023-03-09 06:10:38.884 [INFO][70] felix/table.go 582: Chain became referenced, marking it for programming chainName="cali-POSTROUTING"
2023-03-09 06:10:38.899 [INFO][70] felix/int_dataplane.go 1605: Set XDP failsafe ports: [{Net: Protocol:tcp Port:22} {Net: Protocol:udp Port:68} {Net: Protocol:tcp Port:179} {Net: Protocol:tcp Port:2379} {Net: Protocol:tcp Port:2380} {Net: Protocol:tcp Port:5473} {Net: Protocol:tcp Port:6443} {Net: Protocol:tcp Port:6666} {Net: Protocol:tcp Port:6667}]
2023-03-09 06:10:38.899 [INFO][70] felix/int_dataplane.go 1327: IPIP enabled, starting thread to keep tunnel configuration in sync.
2023-03-09 06:10:38.899 [INFO][70] felix/daemon.go 422: Connect to the dataplane driver.
2023-03-09 06:10:38.900 [INFO][70] felix/daemon.go 501: using resource updates where applicable
2023-03-09 06:10:38.900 [INFO][70] felix/daemon.go 504: Created Syncer syncer=&watchersyncer.watcherSyncer{status:0x0, watcherCaches:[]*watchersyncer.watcherCache{(*watchersyncer.watcherCache)(0xc000680480), (*watchersyncer.watcherCache)(0xc0006805a0), (*watchersyncer.watcherCache)(0xc0006806c0), (*watchersyncer.watcherCache)(0xc000680750), (*watchersyncer.watcherCache)(0xc000680a20), (*watchersyncer.watcherCache)(0xc000680bd0), (*watchersyncer.watcherCache)(0xc000680c60), (*watchersyncer.watcherCache)(0xc000680cf0), (*watchersyncer.watcherCache)(0xc000680d80), (*watchersyncer.watcherCache)(0xc000680e10), (*watchersyncer.watcherCache)(0xc000680ea0), (*watchersyncer.watcherCache)(0xc000680f30), (*watchersyncer.watcherCache)(0xc000680fc0), (*watchersyncer.watcherCache)(0xc000681050), (*watchersyncer.watcherCache)(0xc0006810e0)}, results:(chan interface {})(0xc0003a7020), numSynced:0, callbacks:(*calc.SyncerCallbacksDecoupler)(0xc000010ca0), wgwc:(*sync.WaitGroup)(nil), wgws:(*sync.WaitGroup)(nil), cancel:(context.CancelFunc)(nil)}
2023-03-09 06:10:38.900 [INFO][70] felix/daemon.go 508: Starting the datastore Syncer
2023-03-09 06:10:38.900 [INFO][70] felix/watchersyncer.go 89: Start called
2023-03-09 06:10:38.900 [INFO][70] felix/calc_graph.go 118: Creating calculation graph, filtered to hostname lke96996-146211-640977a3e82e
2023-03-09 06:10:38.900 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.WorkloadEndpointKey: (dispatcher.UpdateHandler)(0x198b760)
2023-03-09 06:10:38.900 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.HostEndpointKey: (dispatcher.UpdateHandler)(0x198b760)
2023-03-09 06:10:38.900 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.WorkloadEndpointKey: (dispatcher.UpdateHandler)(0x198b900)
2023-03-09 06:10:38.900 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.HostEndpointKey: (dispatcher.UpdateHandler)(0x198b900)
2023-03-09 06:10:38.900 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.WorkloadEndpointKey: (dispatcher.UpdateHandler)(0x198af20)
2023-03-09 06:10:38.900 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.HostEndpointKey: (dispatcher.UpdateHandler)(0x198af20)
2023-03-09 06:10:38.900 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.PolicyKey: (dispatcher.UpdateHandler)(0x198af20)
2023-03-09 06:10:38.900 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.ProfileRulesKey: (dispatcher.UpdateHandler)(0x198af20)
2023-03-09 06:10:38.900 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.ResourceKey: (dispatcher.UpdateHandler)(0x198af20)
2023-03-09 06:10:38.900 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.ResourceKey: (dispatcher.UpdateHandler)(0x19436a0)
2023-03-09 06:10:38.900 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.ResourceKey: (dispatcher.UpdateHandler)(0x17911a0)
2023-03-09 06:10:38.900 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.WorkloadEndpointKey: (dispatcher.UpdateHandler)(0x17911a0)
2023-03-09 06:10:38.901 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.HostEndpointKey: (dispatcher.UpdateHandler)(0x17911a0)
2023-03-09 06:10:38.901 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.NetworkSetKey: (dispatcher.UpdateHandler)(0x17911a0)
2023-03-09 06:10:38.901 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.PolicyKey: (dispatcher.UpdateHandler)(0x198c3e0)
2023-03-09 06:10:38.901 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.WorkloadEndpointKey: (dispatcher.UpdateHandler)(0x198c3e0)
2023-03-09 06:10:38.901 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.HostEndpointKey: (dispatcher.UpdateHandler)(0x198c3e0)
2023-03-09 06:10:38.901 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.HostIPKey: (dispatcher.UpdateHandler)(0x198bbe0)
2023-03-09 06:10:38.901 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.IPPoolKey: (dispatcher.UpdateHandler)(0x198bbe0)
2023-03-09 06:10:38.901 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.WireguardKey: (dispatcher.UpdateHandler)(0x198bbe0)
2023-03-09 06:10:38.901 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.ResourceKey: (dispatcher.UpdateHandler)(0x198bbe0)
2023-03-09 06:10:38.901 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.GlobalConfigKey: (dispatcher.UpdateHandler)(0x198ba40)
2023-03-09 06:10:38.901 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.HostConfigKey: (dispatcher.UpdateHandler)(0x198ba40)
2023-03-09 06:10:38.901 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.ReadyFlagKey: (dispatcher.UpdateHandler)(0x198ba40)
2023-03-09 06:10:38.901 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.ResourceKey: (dispatcher.UpdateHandler)(0x198b480)
2023-03-09 06:10:38.901 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.IPPoolKey: (dispatcher.UpdateHandler)(0x198b5c0)
2023-03-09 06:10:38.901 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.HostIPKey: (dispatcher.UpdateHandler)(0x198c880)
2023-03-09 06:10:38.901 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.WorkloadEndpointKey: (dispatcher.UpdateHandler)(0x198c880)
2023-03-09 06:10:38.901 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.HostEndpointKey: (dispatcher.UpdateHandler)(0x198c880)
2023-03-09 06:10:38.901 [INFO][70] felix/dispatcher.go 68: Registering listener for type model.HostConfigKey: (dispatcher.UpdateHandler)(0x198c880)
2023-03-09 06:10:38.901 [INFO][70] felix/async_calc_graph.go 255: Starting AsyncCalcGraph
2023-03-09 06:10:38.907 [INFO][70] felix/daemon.go 619: Started the processing graph
2023-03-09 06:10:38.907 [INFO][70] felix/ipip_mgr.go 84: IPIP thread started.
2023-03-09 06:10:38.907 [INFO][70] felix/ipip_mgr.go 105: Failed to get IPIP tunnel device, assuming it isn't present error=Link not found
2023-03-09 06:10:38.911 [INFO][70] felix/int_dataplane.go 1634: Started internal iptables dataplane driver loop
2023-03-09 06:10:38.911 [INFO][70] felix/int_dataplane.go 1644: Will refresh IP sets on timer interval=1m30s
2023-03-09 06:10:38.911 [INFO][70] felix/int_dataplane.go 1654: Will refresh routes on timer interval=1m30s
2023-03-09 06:10:38.911 [INFO][70] felix/int_dataplane.go 1664: Will refresh XDP on timer interval=1m30s
2023-03-09 06:10:38.911 [INFO][70] felix/int_dataplane.go 2110: Started internal status report thread
2023-03-09 06:10:38.911 [INFO][70] felix/int_dataplane.go 2112: Process status reports disabled
2023-03-09 06:10:38.911 [INFO][70] felix/iface_monitor.go 109: Interface monitoring thread started.
2023-03-09 06:10:38.912 [INFO][70] felix/iface_monitor.go 127: Subscribed to netlink updates.
2023-03-09 06:10:38.912 [INFO][70] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=1 ifaceName="lo" state="up"
2023-03-09 06:10:38.912 [INFO][70] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{127.0.0.1,::1,127.0.0.0} ifaceName="lo"
2023-03-09 06:10:38.912 [INFO][70] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=2 ifaceName="eth0" state="up"
2023-03-09 06:10:38.912 [INFO][70] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{2a01:7e00::f03c:93ff:fe1a:4310,fe80::f03c:93ff:fe1a:4310,139.144.144.93,192.168.173.115} ifaceName="eth0"
2023-03-09 06:10:38.912 [INFO][70] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=3 ifaceName="docker0" state="down"
2023-03-09 06:10:38.912 [INFO][70] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{172.17.0.1} ifaceName="docker0"
2023-03-09 06:10:38.912 [INFO][70] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=4 ifaceName="wg0" state="up"
2023-03-09 06:10:38.912 [INFO][70] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{172.31.1.1} ifaceName="wg0"
2023-03-09 06:10:38.912 [INFO][70] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"lo", State:"up", Index:1}
2023-03-09 06:10:38.912 [INFO][70] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"eth0", State:"up", Index:2}
2023-03-09 06:10:38.912 [INFO][70] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"docker0", State:"down", Index:3}
2023-03-09 06:10:38.912 [INFO][70] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"wg0", State:"up", Index:4}
2023-03-09 06:10:38.912 [INFO][70] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"lo", Addrs:set.Typed[string]{"127.0.0.0":set.v{}, "127.0.0.1":set.v{}, "::1":set.v{}}}
2023-03-09 06:10:38.912 [INFO][70] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"lo", Addrs:set.Typed[string]{"127.0.0.0":set.v{}, "127.0.0.1":set.v{}, "::1":set.v{}}}
2023-03-09 06:10:38.913 [INFO][70] felix/ipsets.go 130: Queueing IP set for creation family="inet" setID="this-host" setType="hash:ip"
2023-03-09 06:10:38.913 [INFO][70] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"eth0", Addrs:set.Typed[string]{"139.144.144.93":set.v{}, "192.168.173.115":set.v{}, "2a01:7e00::f03c:93ff:fe1a:4310":set.v{}, "fe80::f03c:93ff:fe1a:4310":set.v{}}}
2023-03-09 06:10:38.913 [INFO][70] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"eth0", Addrs:set.Typed[string]{"139.144.144.93":set.v{}, "192.168.173.115":set.v{}, "2a01:7e00::f03c:93ff:fe1a:4310":set.v{}, "fe80::f03c:93ff:fe1a:4310":set.v{}}}
2023-03-09 06:10:38.913 [INFO][70] felix/ipsets.go 130: Queueing IP set for creation family="inet" setID="this-host" setType="hash:ip"
2023-03-09 06:10:38.913 [INFO][70] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"docker0", Addrs:set.Typed[string]{"172.17.0.1":set.v{}}}
2023-03-09 06:10:38.913 [INFO][70] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"docker0", Addrs:set.Typed[string]{"172.17.0.1":set.v{}}}
2023-03-09 06:10:38.913 [INFO][70] felix/ipsets.go 130: Queueing IP set for creation family="inet" setID="this-host" setType="hash:ip"
2023-03-09 06:10:38.913 [INFO][70] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"wg0", Addrs:set.Typed[string]{"172.31.1.1":set.v{}}}
2023-03-09 06:10:38.913 [INFO][70] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"wg0", Addrs:set.Typed[string]{"172.31.1.1":set.v{}}}
2023-03-09 06:10:38.913 [INFO][70] felix/ipsets.go 130: Queueing IP set for creation family="inet" setID="this-host" setType="hash:ip"
2023-03-09 06:10:38.913 [INFO][70] felix/watchersyncer.go 130: Sending status update Status=wait-for-ready
2023-03-09 06:10:38.913 [INFO][70] felix/watchersyncer.go 149: Starting main event processing loop
2023-03-09 06:10:38.913 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/kubernetesservice"
2023-03-09 06:10:38.913 [INFO][70] felix/async_calc_graph.go 137: AsyncCalcGraph running
2023-03-09 06:10:38.913 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ConfigUpdate update from calculation graph msg=config:<key:"CalicoVersion" value:"v3.24.5" > config:<key:"ClusterGUID" value:"7197fc97702c482f9a5df8fb5c03809a" > config:<key:"ClusterType" value:"k8s,bgp,kubeadm,kdd" > config:<key:"DatastoreType" value:"kubernetes" > config:<key:"DefaultEndpointToHostAction" value:"ACCEPT" > config:<key:"FelixHostname" value:"lke96996-146211-640977a3e82e" > config:<key:"FloatingIPs" value:"Disabled" > config:<key:"HealthEnabled" value:"true" > config:<key:"IpInIpMtu" value:"0" > config:<key:"IpInIpTunnelAddr" value:"10.2.1.1" > config:<key:"Ipv6Support" value:"false" > config:<key:"LogFilePath" value:"None" > config:<key:"LogSeverityFile" value:"None" > config:<key:"LogSeverityScreen" value:"Info" > config:<key:"LogSeveritySys" value:"None" > config:<key:"MetadataAddr" value:"None" > config:<key:"ReportingIntervalSecs" value:"0" > config:<key:"VXLANMTU" value:"0" > config:<key:"WireguardMTU" value:"0" > 
2023-03-09 06:10:38.913 [INFO][70] felix/daemon.go 979: Reading from dataplane driver pipe...
2023-03-09 06:10:38.919 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/clusterinformations"
2023-03-09 06:10:38.919 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/felixconfigurations"
2023-03-09 06:10:38.919 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/globalnetworkpolicies"
2023-03-09 06:10:38.920 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/globalnetworksets"
2023-03-09 06:10:38.920 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/ippools"
2023-03-09 06:10:38.920 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/nodes"
2023-03-09 06:10:38.920 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/profiles"
2023-03-09 06:10:38.920 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/workloadendpoints"
2023-03-09 06:10:38.920 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/networkpolicies"
2023-03-09 06:10:38.920 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/networksets"
2023-03-09 06:10:38.920 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/hostendpoints"
2023-03-09 06:10:38.920 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/bgpconfigurations"
2023-03-09 06:10:38.921 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/kubernetesnetworkpolicies"
2023-03-09 06:10:38.922 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/kubernetesendpointslices"
2023-03-09 06:10:38.955 [INFO][70] felix/daemon.go 689: No driver process to monitor
2023-03-09 06:10:38.959 [INFO][70] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/kubernetesservice"
2023-03-09 06:10:38.964 [INFO][70] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/hostendpoints"
2023-03-09 06:10:38.965 [INFO][70] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/networksets"
2023-03-09 06:10:38.965 [INFO][70] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/clusterinformations"
2023-03-09 06:10:38.972 [INFO][70] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/felixconfigurations"
2023-03-09 06:10:38.972 [INFO][70] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/networkpolicies"
2023-03-09 06:10:38.973 [INFO][70] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/kubernetesnetworkpolicies"
2023-03-09 06:10:38.973 [INFO][70] felix/watchersyncer.go 130: Sending status update Status=resync
2023-03-09 06:10:38.973 [INFO][70] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:38.973 [INFO][70] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:38.973 [INFO][70] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:38.973 [INFO][70] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:38.973 [INFO][70] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:38.974 [INFO][70] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:38.974 [INFO][70] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:38.990 [INFO][70] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/workloadendpoints"
2023-03-09 06:10:38.990 [INFO][70] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/kubernetesendpointslices"
2023-03-09 06:10:38.990 [INFO][70] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/ippools"
2023-03-09 06:10:38.990 [INFO][70] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/globalnetworkpolicies"
2023-03-09 06:10:38.991 [INFO][70] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/nodes"
2023-03-09 06:10:39.002 [INFO][70] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/globalnetworksets"
2023-03-09 06:10:39.005 [INFO][70] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/bgpconfigurations"
2023-03-09 06:10:39.006 [INFO][70] felix/config_batcher.go 74: Global config update: {{GlobalFelixConfig(name=ClusterGUID) 7197fc97702c482f9a5df8fb5c03809a 679 <nil> 0s} 1}
2023-03-09 06:10:39.006 [INFO][70] felix/config_batcher.go 74: Global config update: {{GlobalFelixConfig(name=ClusterType) k8s,bgp,kubeadm,kdd 679 <nil> 0s} 1}
2023-03-09 06:10:39.006 [INFO][70] felix/config_batcher.go 74: Global config update: {{GlobalFelixConfig(name=CalicoVersion) v3.24.5 679 <nil> 0s} 1}
2023-03-09 06:10:39.006 [INFO][70] felix/config_batcher.go 74: Global config update: {{GlobalFelixConfig(name=LogSeverityScreen) Info 680 <nil> 0s} 1}
2023-03-09 06:10:39.006 [INFO][70] felix/config_batcher.go 74: Global config update: {{GlobalFelixConfig(name=ReportingIntervalSecs) 0 680 <nil> 0s} 1}
2023-03-09 06:10:39.006 [INFO][70] felix/config_batcher.go 74: Global config update: {{GlobalFelixConfig(name=FloatingIPs) Disabled 680 <nil> 0s} 1}
2023-03-09 06:10:39.006 [INFO][70] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:39.006 [INFO][70] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:39.006 [INFO][70] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:39.006 [INFO][70] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:39.007 [INFO][70] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:39.007 [INFO][70] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:39.011 [INFO][70] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:39.017 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceUpdate update from calculation graph msg=name:"kubernetes" namespace:"default" type:"ClusterIP" cluster_ip:"10.128.0.1" 
2023-03-09 06:10:39.017 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceUpdate update from calculation graph msg=name:"kube-dns" namespace:"kube-system" type:"ClusterIP" cluster_ip:"10.128.0.10" 
2023-03-09 06:10:39.017 [INFO][70] felix/config_batcher.go 61: Host config update for this host: {{HostConfig(node=lke96996-146211-640977a3e82e,name=IpInIpTunnelAddr) 10.2.1.1 865 <nil> 0s} 1}
2023-03-09 06:10:39.017 [INFO][70] felix/int_dataplane.go 1680: Received *proto.HostMetadataUpdate update from calculation graph msg=hostname:"lke96996-146211-640977a3930b" ipv4_addr:"192.168.129.114" 
2023-03-09 06:10:39.018 [INFO][70] felix/int_dataplane.go 1680: Received *proto.HostMetadataUpdate update from calculation graph msg=hostname:"lke96996-146211-640977a3e82e" ipv4_addr:"192.168.173.115" 
2023-03-09 06:10:39.018 [INFO][70] felix/int_dataplane.go 1680: Received *proto.IPAMPoolUpdate update from calculation graph msg=id:"10.2.0.0-16" pool:<cidr:"10.2.0.0/16" masquerade:true > 
2023-03-09 06:10:39.043 [WARNING][70] felix/ipip_mgr.go 111: Failed to add IPIP tunnel device error=exit status 1
2023-03-09 06:10:39.043 [WARNING][70] felix/ipip_mgr.go 88: Failed configure IPIP tunnel device, retrying... error=exit status 1
2023-03-09 06:10:39.063 [INFO][70] felix/watchercache.go 294: Sending synced update ListRoot="/calico/resources/v3/projectcalico.org/profiles"
2023-03-09 06:10:39.063 [INFO][70] felix/watchersyncer.go 209: Received InSync event from one of the watcher caches
2023-03-09 06:10:39.063 [INFO][70] felix/watchersyncer.go 221: All watchers have sync'd data - sending data and final sync
2023-03-09 06:10:39.063 [INFO][70] felix/watchersyncer.go 130: Sending status update Status=in-sync
2023-03-09 06:10:39.068 [INFO][70] felix/config_batcher.go 102: Datamodel in sync, flushing config update
2023-03-09 06:10:39.068 [INFO][70] felix/config_batcher.go 112: Sending config update global: map[CalicoVersion:v3.24.5 ClusterGUID:7197fc97702c482f9a5df8fb5c03809a ClusterType:k8s,bgp,kubeadm,kdd FloatingIPs:Disabled LogSeverityScreen:Info ReportingIntervalSecs:0], host: map[IpInIpTunnelAddr:10.2.1.1].
2023-03-09 06:10:39.068 [INFO][70] felix/async_calc_graph.go 166: First time we've been in sync
2023-03-09 06:10:39.068 [INFO][70] felix/health.go 137: Health of component changed lastReport=health.HealthReport{Live:true, Ready:false, Detail:""} name="async_calc_graph" newReport=&health.HealthReport{Live:true, Ready:true, Detail:""}
2023-03-09 06:10:39.068 [INFO][70] felix/event_sequencer.go 259: Possible config update. global=map[string]string{"CalicoVersion":"v3.24.5", "ClusterGUID":"7197fc97702c482f9a5df8fb5c03809a", "ClusterType":"k8s,bgp,kubeadm,kdd", "FloatingIPs":"Disabled", "LogSeverityScreen":"Info", "ReportingIntervalSecs":"0"} host=map[string]string{"IpInIpTunnelAddr":"10.2.1.1"}
2023-03-09 06:10:39.068 [INFO][70] felix/config_params.go 435: Merging in config from datastore (global): map[CalicoVersion:v3.24.5 ClusterGUID:7197fc97702c482f9a5df8fb5c03809a ClusterType:k8s,bgp,kubeadm,kdd FloatingIPs:Disabled LogSeverityScreen:Info ReportingIntervalSecs:0]
2023-03-09 06:10:39.068 [INFO][70] felix/config_params.go 542: Parsing value for IpInIpMtu: 0 (from environment variable)
2023-03-09 06:10:39.068 [INFO][70] felix/config_params.go 578: Parsed value for IpInIpMtu: 0 (from environment variable)
2023-03-09 06:10:39.068 [INFO][70] felix/config_params.go 542: Parsing value for WireguardMTU: 0 (from environment variable)
2023-03-09 06:10:39.068 [INFO][70] felix/config_params.go 578: Parsed value for WireguardMTU: 0 (from environment variable)
2023-03-09 06:10:39.068 [INFO][70] felix/config_params.go 542: Parsing value for VXLANMTU: 0 (from environment variable)
2023-03-09 06:10:39.074 [INFO][70] felix/config_params.go 578: Parsed value for VXLANMTU: 0 (from environment variable)
2023-03-09 06:10:39.074 [INFO][70] felix/config_params.go 542: Parsing value for DefaultEndpointToHostAction: ACCEPT (from environment variable)
2023-03-09 06:10:39.074 [INFO][70] felix/config_params.go 578: Parsed value for DefaultEndpointToHostAction: ACCEPT (from environment variable)
2023-03-09 06:10:39.074 [INFO][70] felix/config_params.go 542: Parsing value for DatastoreType: kubernetes (from environment variable)
2023-03-09 06:10:39.074 [INFO][70] felix/config_params.go 578: Parsed value for DatastoreType: kubernetes (from environment variable)
2023-03-09 06:10:39.074 [INFO][70] felix/config_params.go 542: Parsing value for HealthEnabled: true (from environment variable)
2023-03-09 06:10:39.074 [INFO][70] felix/config_params.go 578: Parsed value for HealthEnabled: true (from environment variable)
2023-03-09 06:10:39.075 [INFO][70] felix/config_params.go 542: Parsing value for FelixHostname: lke96996-146211-640977a3e82e (from environment variable)
2023-03-09 06:10:39.075 [INFO][70] felix/config_params.go 578: Parsed value for FelixHostname: lke96996-146211-640977a3e82e (from environment variable)
2023-03-09 06:10:39.075 [INFO][70] felix/config_params.go 542: Parsing value for Ipv6Support: false (from environment variable)
2023-03-09 06:10:39.092 [INFO][70] felix/usagerep.go 91: Waiting before first check-in delay=5m1.282s
2023-03-09 06:10:39.092 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"daemon-set-controller" > labels:<key:"projectcalico.org/name" value:"daemon-set-controller" > 
2023-03-09 06:10:39.092 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"job-controller" > labels:<key:"projectcalico.org/name" value:"job-controller" > 
2023-03-09 06:10:39.092 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"persistent-volume-binder" > labels:<key:"projectcalico.org/name" value:"persistent-volume-binder" > 
2023-03-09 06:10:39.092 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"ttl-after-finished-controller" > labels:<key:"projectcalico.org/name" value:"ttl-after-finished-controller" > 
2023-03-09 06:10:39.092 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"cluster-autoscaler-user" > labels:<key:"k8s-addon" value:"cluster-autoscaler.addons.k8s.io" > labels:<key:"k8s-app" value:"cluster-autoscaler" > labels:<key:"projectcalico.org/name" value:"cluster-autoscaler-user" > 
2023-03-09 06:10:39.092 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"attachdetach-controller" > labels:<key:"projectcalico.org/name" value:"attachdetach-controller" > 
2023-03-09 06:10:39.092 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"clusterrole-aggregation-controller" > labels:<key:"projectcalico.org/name" value:"clusterrole-aggregation-controller" > 
2023-03-09 06:10:39.092 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"cronjob-controller" > labels:<key:"projectcalico.org/name" value:"cronjob-controller" > 
2023-03-09 06:10:39.092 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"csi-controller-sa" > labels:<key:"projectcalico.org/name" value:"csi-controller-sa" > 
2023-03-09 06:10:39.093 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"deployment-controller" > labels:<key:"projectcalico.org/name" value:"deployment-controller" > 
2023-03-09 06:10:39.093 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"endpoint-controller" > labels:<key:"projectcalico.org/name" value:"endpoint-controller" > 
2023-03-09 06:10:39.093 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"service-account-controller" > labels:<key:"projectcalico.org/name" value:"service-account-controller" > 
2023-03-09 06:10:39.093 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"default" name:"default" > labels:<key:"projectcalico.org/name" value:"default" > 
2023-03-09 06:10:39.093 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"expand-controller" > labels:<key:"projectcalico.org/name" value:"expand-controller" > 
2023-03-09 06:10:39.093 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"pvc-protection-controller" > labels:<key:"projectcalico.org/name" value:"pvc-protection-controller" > 
2023-03-09 06:10:39.093 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"replicaset-controller" > labels:<key:"projectcalico.org/name" value:"replicaset-controller" > 
2023-03-09 06:10:39.093 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"endpointslicemirroring-controller" > labels:<key:"projectcalico.org/name" value:"endpointslicemirroring-controller" > 
2023-03-09 06:10:39.093 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"node-controller" > labels:<key:"projectcalico.org/name" value:"node-controller" > 
2023-03-09 06:10:39.093 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"pod-garbage-collector" > labels:<key:"projectcalico.org/name" value:"pod-garbage-collector" > 
2023-03-09 06:10:39.093 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"pv-protection-controller" > labels:<key:"projectcalico.org/name" value:"pv-protection-controller" > 
2023-03-09 06:10:39.093 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"statefulset-controller" > labels:<key:"projectcalico.org/name" value:"statefulset-controller" > 
2023-03-09 06:10:39.094 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"token-cleaner" > labels:<key:"projectcalico.org/name" value:"token-cleaner" > 
2023-03-09 06:10:39.094 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"coredns" > labels:<key:"projectcalico.org/name" value:"coredns" > 
2023-03-09 06:10:39.094 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"disruption-controller" > labels:<key:"projectcalico.org/name" value:"disruption-controller" > 
2023-03-09 06:10:39.094 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"resourcequota-controller" > labels:<key:"projectcalico.org/name" value:"resourcequota-controller" > 
2023-03-09 06:10:39.094 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"bootstrap-signer" > labels:<key:"projectcalico.org/name" value:"bootstrap-signer" > 
2023-03-09 06:10:39.094 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-public" name:"default" > labels:<key:"projectcalico.org/name" value:"default" > 
2023-03-09 06:10:39.094 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"csi-node-sa" > labels:<key:"projectcalico.org/name" value:"csi-node-sa" > 
2023-03-09 06:10:39.094 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"endpointslice-controller" > labels:<key:"projectcalico.org/name" value:"endpointslice-controller" > 
2023-03-09 06:10:39.094 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"ephemeral-volume-controller" > labels:<key:"projectcalico.org/name" value:"ephemeral-volume-controller" > 
2023-03-09 06:10:39.095 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"kubernetes-dashboard-lke-user" > labels:<key:"app" value:"kubernetes-dashboard-lke" > labels:<key:"projectcalico.org/name" value:"kubernetes-dashboard-lke-user" > 
2023-03-09 06:10:39.075 [INFO][70] felix/config_params.go 578: Parsed value for Ipv6Support: false (from environment variable)
2023-03-09 06:10:39.121 [INFO][70] felix/config_params.go 542: Parsing value for MetadataAddr: None (from config file)
2023-03-09 06:10:39.121 [INFO][70] felix/config_params.go 559: Value set to 'none', replacing with zero-value: "".
2023-03-09 06:10:39.121 [INFO][70] felix/config_params.go 578: Parsed value for MetadataAddr:  (from config file)
2023-03-09 06:10:39.121 [INFO][70] felix/config_params.go 542: Parsing value for LogFilePath: None (from config file)
2023-03-09 06:10:39.121 [INFO][70] felix/config_params.go 559: Value set to 'none', replacing with zero-value: "".
2023-03-09 06:10:39.121 [INFO][70] felix/config_params.go 578: Parsed value for LogFilePath:  (from config file)
2023-03-09 06:10:39.121 [INFO][70] felix/config_params.go 542: Parsing value for LogSeverityFile: None (from config file)
2023-03-09 06:10:39.121 [INFO][70] felix/config_params.go 559: Value set to 'none', replacing with zero-value: "".
2023-03-09 06:10:39.121 [INFO][70] felix/config_params.go 578: Parsed value for LogSeverityFile:  (from config file)
2023-03-09 06:10:39.121 [INFO][70] felix/config_params.go 542: Parsing value for LogSeveritySys: None (from config file)
2023-03-09 06:10:39.121 [INFO][70] felix/config_params.go 559: Value set to 'none', replacing with zero-value: "".
2023-03-09 06:10:39.121 [INFO][70] felix/config_params.go 578: Parsed value for LogSeveritySys:  (from config file)
2023-03-09 06:10:39.121 [INFO][70] felix/config_params.go 542: Parsing value for IpInIpTunnelAddr: 10.2.1.1 (from datastore (per-host))
2023-03-09 06:10:39.121 [INFO][70] felix/config_params.go 578: Parsed value for IpInIpTunnelAddr: 10.2.1.1 (from datastore (per-host))
2023-03-09 06:10:39.121 [INFO][70] felix/config_params.go 542: Parsing value for LogSeverityScreen: Info (from datastore (global))
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 578: Parsed value for LogSeverityScreen: INFO (from datastore (global))
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 542: Parsing value for ReportingIntervalSecs: 0 (from datastore (global))
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 578: Parsed value for ReportingIntervalSecs: 0s (from datastore (global))
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 542: Parsing value for FloatingIPs: Disabled (from datastore (global))
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 578: Parsed value for FloatingIPs: Disabled (from datastore (global))
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 542: Parsing value for ClusterGUID: 7197fc97702c482f9a5df8fb5c03809a (from datastore (global))
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 578: Parsed value for ClusterGUID: 7197fc97702c482f9a5df8fb5c03809a (from datastore (global))
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 542: Parsing value for ClusterType: k8s,bgp,kubeadm,kdd (from datastore (global))
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 578: Parsed value for ClusterType: k8s,bgp,kubeadm,kdd (from datastore (global))
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 542: Parsing value for CalicoVersion: v3.24.5 (from datastore (global))
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 578: Parsed value for CalicoVersion: v3.24.5 (from datastore (global))
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 435: Merging in config from datastore (per-host): map[IpInIpTunnelAddr:10.2.1.1]
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 542: Parsing value for Ipv6Support: false (from environment variable)
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 578: Parsed value for Ipv6Support: false (from environment variable)
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 542: Parsing value for IpInIpMtu: 0 (from environment variable)
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 578: Parsed value for IpInIpMtu: 0 (from environment variable)
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 542: Parsing value for WireguardMTU: 0 (from environment variable)
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 578: Parsed value for WireguardMTU: 0 (from environment variable)
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 542: Parsing value for VXLANMTU: 0 (from environment variable)
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 578: Parsed value for VXLANMTU: 0 (from environment variable)
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 542: Parsing value for DefaultEndpointToHostAction: ACCEPT (from environment variable)
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 578: Parsed value for DefaultEndpointToHostAction: ACCEPT (from environment variable)
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 542: Parsing value for DatastoreType: kubernetes (from environment variable)
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 578: Parsed value for DatastoreType: kubernetes (from environment variable)
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 542: Parsing value for HealthEnabled: true (from environment variable)
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 578: Parsed value for HealthEnabled: true (from environment variable)
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 542: Parsing value for FelixHostname: lke96996-146211-640977a3e82e (from environment variable)
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 578: Parsed value for FelixHostname: lke96996-146211-640977a3e82e (from environment variable)
2023-03-09 06:10:39.122 [INFO][70] felix/config_params.go 542: Parsing value for MetadataAddr: None (from config file)
2023-03-09 06:10:39.123 [INFO][70] felix/config_params.go 559: Value set to 'none', replacing with zero-value: "".
2023-03-09 06:10:39.123 [INFO][70] felix/config_params.go 578: Parsed value for MetadataAddr:  (from config file)
2023-03-09 06:10:39.123 [INFO][70] felix/config_params.go 542: Parsing value for LogFilePath: None (from config file)
2023-03-09 06:10:39.123 [INFO][70] felix/config_params.go 559: Value set to 'none', replacing with zero-value: "".
2023-03-09 06:10:39.123 [INFO][70] felix/config_params.go 578: Parsed value for LogFilePath:  (from config file)
2023-03-09 06:10:39.135 [INFO][70] felix/config_params.go 542: Parsing value for LogSeverityFile: None (from config file)
2023-03-09 06:10:39.135 [INFO][70] felix/config_params.go 559: Value set to 'none', replacing with zero-value: "".
2023-03-09 06:10:39.135 [INFO][70] felix/config_params.go 578: Parsed value for LogSeverityFile:  (from config file)
2023-03-09 06:10:39.135 [INFO][70] felix/config_params.go 542: Parsing value for LogSeveritySys: None (from config file)
2023-03-09 06:10:39.135 [INFO][70] felix/config_params.go 559: Value set to 'none', replacing with zero-value: "".
2023-03-09 06:10:39.135 [INFO][70] felix/config_params.go 578: Parsed value for LogSeveritySys:  (from config file)
2023-03-09 06:10:39.135 [INFO][70] felix/config_params.go 542: Parsing value for IpInIpTunnelAddr: 10.2.1.1 (from datastore (per-host))
2023-03-09 06:10:39.135 [INFO][70] felix/config_params.go 578: Parsed value for IpInIpTunnelAddr: 10.2.1.1 (from datastore (per-host))
2023-03-09 06:10:39.135 [INFO][70] felix/config_params.go 542: Parsing value for ClusterGUID: 7197fc97702c482f9a5df8fb5c03809a (from datastore (global))
2023-03-09 06:10:39.135 [INFO][70] felix/config_params.go 578: Parsed value for ClusterGUID: 7197fc97702c482f9a5df8fb5c03809a (from datastore (global))
2023-03-09 06:10:39.135 [INFO][70] felix/config_params.go 542: Parsing value for ClusterType: k8s,bgp,kubeadm,kdd (from datastore (global))
2023-03-09 06:10:39.136 [INFO][70] felix/config_params.go 578: Parsed value for ClusterType: k8s,bgp,kubeadm,kdd (from datastore (global))
2023-03-09 06:10:39.136 [INFO][70] felix/config_params.go 542: Parsing value for CalicoVersion: v3.24.5 (from datastore (global))
2023-03-09 06:10:39.136 [INFO][70] felix/config_params.go 578: Parsed value for CalicoVersion: v3.24.5 (from datastore (global))
2023-03-09 06:10:39.136 [INFO][70] felix/config_params.go 542: Parsing value for LogSeverityScreen: Info (from datastore (global))
2023-03-09 06:10:39.136 [INFO][70] felix/config_params.go 578: Parsed value for LogSeverityScreen: INFO (from datastore (global))
2023-03-09 06:10:39.136 [INFO][70] felix/config_params.go 542: Parsing value for ReportingIntervalSecs: 0 (from datastore (global))
2023-03-09 06:10:39.136 [INFO][70] felix/config_params.go 578: Parsed value for ReportingIntervalSecs: 0s (from datastore (global))
2023-03-09 06:10:39.136 [INFO][70] felix/config_params.go 542: Parsing value for FloatingIPs: Disabled (from datastore (global))
2023-03-09 06:10:39.136 [INFO][70] felix/config_params.go 578: Parsed value for FloatingIPs: Disabled (from datastore (global))
2023-03-09 06:10:39.136 [INFO][70] felix/async_calc_graph.go 220: First flush after becoming in sync, sending InSync message.
2023-03-09 06:10:39.136 [INFO][70] felix/daemon.go 1153: Datastore now in sync.
2023-03-09 06:10:39.136 [INFO][70] felix/daemon.go 1155: Datastore in sync for first time, sending message to status reporter.
2023-03-09 06:10:39.095 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"lke-admin" > labels:<key:"projectcalico.org/name" value:"lke-admin" > 
2023-03-09 06:10:39.136 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"replication-controller" > labels:<key:"projectcalico.org/name" value:"replication-controller" > 
2023-03-09 06:10:39.136 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-node-lease" name:"default" > labels:<key:"projectcalico.org/name" value:"default" > 
2023-03-09 06:10:39.136 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"generic-garbage-collector" > labels:<key:"projectcalico.org/name" value:"generic-garbage-collector" > 
2023-03-09 06:10:39.136 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"horizontal-pod-autoscaler" > labels:<key:"projectcalico.org/name" value:"horizontal-pod-autoscaler" > 
2023-03-09 06:10:39.136 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"kube-proxy" > labels:<key:"projectcalico.org/name" value:"kube-proxy" > 
2023-03-09 06:10:39.136 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"root-ca-cert-publisher" > labels:<key:"projectcalico.org/name" value:"root-ca-cert-publisher" > 
2023-03-09 06:10:39.137 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"calico-node" > labels:<key:"projectcalico.org/name" value:"calico-node" > 
2023-03-09 06:10:39.137 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"ccm-user" > labels:<key:"projectcalico.org/name" value:"ccm-user" > 
2023-03-09 06:10:39.137 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"certificate-controller" > labels:<key:"projectcalico.org/name" value:"certificate-controller" > 
2023-03-09 06:10:39.137 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"default" > labels:<key:"projectcalico.org/name" value:"default" > 
2023-03-09 06:10:39.137 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"namespace-controller" > labels:<key:"projectcalico.org/name" value:"namespace-controller" > 
2023-03-09 06:10:39.137 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"ttl-controller" > labels:<key:"projectcalico.org/name" value:"ttl-controller" > 
2023-03-09 06:10:39.137 [INFO][70] felix/int_dataplane.go 1680: Received *proto.ServiceAccountUpdate update from calculation graph msg=id:<namespace:"kube-system" name:"calico-kube-controllers" > labels:<key:"projectcalico.org/name" value:"calico-kube-controllers" > 
2023-03-09 06:10:39.137 [INFO][70] felix/int_dataplane.go 1680: Received *proto.NamespaceUpdate update from calculation graph msg=id:<name:"default" > labels:<key:"kubernetes.io/metadata.name" value:"default" > labels:<key:"projectcalico.org/name" value:"default" > 
2023-03-09 06:10:39.137 [INFO][70] felix/int_dataplane.go 1680: Received *proto.NamespaceUpdate update from calculation graph msg=id:<name:"kube-node-lease" > labels:<key:"kubernetes.io/metadata.name" value:"kube-node-lease" > labels:<key:"projectcalico.org/name" value:"kube-node-lease" > 
2023-03-09 06:10:39.137 [INFO][70] felix/int_dataplane.go 1680: Received *proto.NamespaceUpdate update from calculation graph msg=id:<name:"kube-public" > labels:<key:"kubernetes.io/metadata.name" value:"kube-public" > labels:<key:"projectcalico.org/name" value:"kube-public" > 
2023-03-09 06:10:39.137 [INFO][70] felix/int_dataplane.go 1680: Received *proto.NamespaceUpdate update from calculation graph msg=id:<name:"kube-system" > labels:<key:"kubernetes.io/metadata.name" value:"kube-system" > labels:<key:"projectcalico.org/name" value:"kube-system" > 
2023-03-09 06:10:39.137 [INFO][70] felix/int_dataplane.go 1680: Received *proto.Encapsulation update from calculation graph msg=ipip_enabled:true 
2023-03-09 06:10:39.137 [INFO][70] felix/int_dataplane.go 1680: Received *proto.InSync update from calculation graph msg=
2023-03-09 06:10:39.137 [INFO][70] felix/int_dataplane.go 1688: Datastore in sync, flushing the dataplane for the first time... timeSinceStart=557.610391ms
2023-03-09 06:10:39.137 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-from-wl-dispatch" ipVersion=0x4 table="filter"
2023-03-09 06:10:39.137 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-to-wl-dispatch" ipVersion=0x4 table="filter"
2023-03-09 06:10:39.137 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-from-host-endpoint" ipVersion=0x4 table="raw"
2023-03-09 06:10:39.138 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-to-host-endpoint" ipVersion=0x4 table="raw"
2023-03-09 06:10:39.138 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-from-host-endpoint" ipVersion=0x4 table="filter"
2023-03-09 06:10:39.138 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-to-host-endpoint" ipVersion=0x4 table="filter"
2023-03-09 06:10:39.138 [INFO][70] felix/table.go 508: Queueing update of chain. chainName="cali-from-hep-forward" ipVersion=0x4 table="filter"
... dropped 13 logs ...
2023-03-09 06:10:39.295 [INFO][70] felix/wireguard.go 1701: Trying to connect to linkClient ipVersion=0x4
2023-03-09 06:10:39.297 [INFO][70] felix/route_rule.go 189: Trying to connect to netlink
2023-03-09 06:10:39.297 [INFO][70] felix/wireguard.go 632: Public key out of sync or updated ipVersion=0x4 ourPublicKey=AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
2023-03-09 06:10:39.304 [INFO][70] felix/ipsets.go 779: Doing full IP set rewrite family="inet" numMembersInPendingReplace=1 setID="all-ipam-pools"
2023-03-09 06:10:39.304 [INFO][70] felix/ipsets.go 779: Doing full IP set rewrite family="inet" numMembersInPendingReplace=1 setID="masq-ipam-pools"
2023-03-09 06:10:39.304 [INFO][70] felix/ipsets.go 779: Doing full IP set rewrite family="inet" numMembersInPendingReplace=6 setID="this-host"
2023-03-09 06:10:39.304 [INFO][70] felix/ipsets.go 779: Doing full IP set rewrite family="inet" numMembersInPendingReplace=2 setID="all-hosts-net"
2023-03-09 06:10:39.335 [INFO][70] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=6 ifaceName="calico_tmp_B" state="down"
2023-03-09 06:10:39.335 [INFO][70] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{} ifaceName="calico_tmp_B"
2023-03-09 06:10:39.335 [INFO][70] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=7 ifaceName="calico_tmp_A" state="down"
2023-03-09 06:10:39.335 [INFO][70] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{} ifaceName="calico_tmp_A"
2023-03-09 06:10:39.389 [INFO][70] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=7 ifaceName="calico_tmp_A" state=""
2023-03-09 06:10:39.389 [INFO][70] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=<nil> ifaceName="calico_tmp_A"
2023-03-09 06:10:39.389 [INFO][70] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=6 ifaceName="calico_tmp_B" state=""
2023-03-09 06:10:39.389 [INFO][70] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=<nil> ifaceName="calico_tmp_B"
2023-03-09 06:10:39.390 [INFO][70] felix/int_dataplane.go 1828: Completed first update to dataplane. secsSinceStart=0.809960511
2023-03-09 06:10:39.395 [INFO][70] felix/health.go 137: Health of component changed lastReport=health.HealthReport{Live:true, Ready:false, Detail:""} name="int_dataplane" newReport=&health.HealthReport{Live:true, Ready:true, Detail:""}
2023-03-09 06:10:39.395 [INFO][70] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"tunl0", Addrs:set.Typed[string]{}}
2023-03-09 06:10:39.395 [INFO][70] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"tunl0", Addrs:set.Typed[string]{}}
2023-03-09 06:10:39.395 [INFO][70] felix/ipsets.go 130: Queueing IP set for creation family="inet" setID="this-host" setType="hash:ip"
2023-03-09 06:10:39.395 [INFO][70] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"calico_tmp_B", Addrs:set.Typed[string]{}}
2023-03-09 06:10:39.395 [INFO][70] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"calico_tmp_B", Addrs:set.Typed[string]{}}
2023-03-09 06:10:39.395 [INFO][70] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"calico_tmp_A", Addrs:set.Typed[string]{}}
2023-03-09 06:10:39.395 [INFO][70] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"calico_tmp_A", Addrs:set.Typed[string]{}}
2023-03-09 06:10:39.395 [INFO][70] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"calico_tmp_A", Addrs:set.Set[string](nil)}
2023-03-09 06:10:39.395 [INFO][70] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"calico_tmp_A", Addrs:set.Set[string](nil)}
2023-03-09 06:10:39.395 [INFO][70] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"calico_tmp_B", Addrs:set.Set[string](nil)}
2023-03-09 06:10:39.396 [INFO][70] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"calico_tmp_B", Addrs:set.Set[string](nil)}
2023-03-09 06:10:39.397 [INFO][70] felix/ipsets.go 779: Doing full IP set rewrite family="inet" numMembersInPendingReplace=6 setID="this-host"
2023-03-09 06:10:39.407 [INFO][70] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"tunl0", State:"down", Index:5}
2023-03-09 06:10:39.407 [INFO][70] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"calico_tmp_B", State:"down", Index:6}
2023-03-09 06:10:39.407 [INFO][70] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"calico_tmp_A", State:"down", Index:7}
2023-03-09 06:10:39.407 [INFO][70] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"calico_tmp_A", State:"", Index:7}
2023-03-09 06:10:39.407 [INFO][70] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"calico_tmp_B", State:"", Index:6}
2023-03-09 06:10:39.407 [INFO][70] felix/int_dataplane.go 1838: Dataplane updates throttled
2023-03-09 06:10:40.044 [INFO][70] felix/ipip_mgr.go 132: Tunnel wasn't admin up, enabling it flags=0 mtu=1480 tunnelAddr=10.2.1.1
2023-03-09 06:10:40.045 [INFO][70] felix/ipip_mgr.go 137: Set tunnel admin up mtu=1480 tunnelAddr=10.2.1.1
2023-03-09 06:10:40.045 [INFO][70] felix/ipip_mgr.go 182: Address wasn't present, adding it. addr=10.2.1.1 link="tunl0"
2023-03-09 06:10:40.046 [INFO][70] felix/int_dataplane.go 1241: Linux interface state changed. ifIndex=5 ifaceName="tunl0" state="up"
2023-03-09 06:10:40.046 [INFO][70] felix/int_dataplane.go 1277: Linux interface addrs changed. addrs=set.Set{10.2.1.1} ifaceName="tunl0"
2023-03-09 06:10:40.046 [INFO][70] felix/int_dataplane.go 1695: Received interface update msg=&intdataplane.ifaceUpdate{Name:"tunl0", State:"up", Index:5}
2023-03-09 06:10:40.046 [INFO][70] felix/int_dataplane.go 1805: Dataplane updates no longer throttled
2023-03-09 06:10:40.047 [INFO][70] felix/iface_monitor.go 217: Netlink address update for known interface.  addr="10.2.1.1" exists=true ifIndex=5
2023-03-09 06:10:40.047 [INFO][70] felix/int_dataplane.go 1713: Received interface addresses update msg=&intdataplane.ifaceAddrsUpdate{Name:"tunl0", Addrs:set.Typed[string]{"10.2.1.1":set.v{}}}
2023-03-09 06:10:40.047 [INFO][70] felix/hostip_mgr.go 84: Interface addrs changed. update=&intdataplane.ifaceAddrsUpdate{Name:"tunl0", Addrs:set.Typed[string]{"10.2.1.1":set.v{}}}
2023-03-09 06:10:40.047 [INFO][70] felix/ipsets.go 130: Queueing IP set for creation family="inet" setID="this-host" setType="hash:ip"
2023-03-09 06:10:40.048 [INFO][70] felix/ipsets.go 779: Doing full IP set rewrite family="inet" numMembersInPendingReplace=7 setID="this-host"
bird: device1: Initializing
bird: direct1: Initializing
bird: Mesh_192_168_129_114: Initializing
bird: device1: Starting
bird: device1: Connected to table master
bird: device1: State changed to feed
bird: direct1: Starting
bird: direct1: Connected to table master
bird: direct1: State changed to feed
bird: Mesh_192_168_129_114: Starting
bird: Mesh_192_168_129_114: State changed to start
bird: Graceful restart started
bird: Started
bird: device1: State changed to up
bird: direct1: State changed to up
bird: device1: Initializing
bird: direct1: Initializing
bird: device1: Starting
bird: device1: Connected to table master
bird: device1: State changed to feed
bird: direct1: Starting
bird: direct1: Connected to table master
bird: direct1: State changed to feed
bird: Graceful restart started
bird: Graceful restart done
bird: Started
bird: device1: State changed to up
bird: direct1: State changed to up
2023-03-09 06:10:40.748 [INFO][63] confd/client.go 995: Recompute BGP peerings: lke96996-146211-640977a3e82e updated
bird: Mesh_192_168_129_114: Connected to table master
bird: Mesh_192_168_129_114: State changed to wait
bird: Graceful restart done
bird: Mesh_192_168_129_114: State changed to feed
bird: Mesh_192_168_129_114: State changed to up
2023-03-09 06:10:47.074 [INFO][70] felix/health.go 242: Overall health status changed newStatus=&health.HealthReport{Live:true, Ready:true, Detail:"+------------------+---------+----------------+-----------------+--------+\n|    COMPONENT     | TIMEOUT |    LIVENESS    |    READINESS    | DETAIL |\n+------------------+---------+----------------+-----------------+--------+\n| async_calc_graph | 20s     | reporting live | reporting ready |        |\n| felix-startup    | 0s      | reporting live | reporting ready |        |\n| int_dataplane    | 1m30s   | reporting live | reporting ready |        |\n+------------------+---------+----------------+-----------------+--------+"}
bird: Shutting down
bird: device1: Shutting down
bird: device1: State changed to flush
bird: direct1: Shutting down
bird: direct1: State changed to flush
bird: Mesh_192_168_129_114: Shutting down
bird: Mesh_192_168_129_114: State changed to stop
bird: device1: State changed to down
bird: direct1: State changed to down
bird: Mesh_192_168_129_114: State changed to down
bird: Shutdown completed
bird: device1: Initializing
bird: direct1: Initializing
bird: Mesh_192_168_129_114: Initializing
bird: device1: Starting
bird: device1: Connected to table master
bird: device1: State changed to feed
bird: direct1: Starting
bird: direct1: Connected to table master
bird: direct1: State changed to feed
bird: Mesh_192_168_129_114: Starting
bird: Mesh_192_168_129_114: State changed to start
bird: Graceful restart started
bird: Started
bird: device1: State changed to up
bird: direct1: State changed to up
bird: Mesh_192_168_129_114: Connected to table master
bird: Mesh_192_168_129_114: State changed to wait
bird: Graceful restart done
bird: Mesh_192_168_129_114: State changed to feed
bird: Mesh_192_168_129_114: State changed to up
2023-03-09 06:11:36.940 [INFO][70] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/kubernetesendpointslices" error=too old resource version: 786 (897)
2023-03-09 06:11:36.940 [INFO][70] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/kubernetesservice" error=too old resource version: 294 (897)
2023-03-09 06:11:36.940 [INFO][70] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/clusterinformations" error=too old resource version: 679 (902)
2023-03-09 06:11:36.940 [INFO][70] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/kubernetesnetworkpolicies" error=too old resource version: 1 (897)
2023-03-09 06:11:36.940 [INFO][70] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/workloadendpoints" error=too old resource version: 867 (897)
2023-03-09 06:11:36.982 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/kubernetesendpointslices"
2023-03-09 06:11:36.982 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/kubernetesservice"
2023-03-09 06:11:36.984 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/workloadendpoints"
2023-03-09 06:11:36.984 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/clusterinformations"
2023-03-09 06:11:36.984 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/kubernetesnetworkpolicies"
2023-03-09 06:11:37.136 [INFO][70] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/profiles" error=too old resource version: 868 (897)
2023-03-09 06:11:37.136 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/profiles"
2023-03-09 06:11:37.362 [INFO][65] status-reporter/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/caliconodestatuses" error=too old resource version: 485 (904)
2023-03-09 06:11:37.363 [INFO][65] status-reporter/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/caliconodestatuses"
2023-03-09 06:11:38.452 [INFO][63] confd/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/bgpconfigurations" error=too old resource version: 492 (909)
2023-03-09 06:11:38.452 [INFO][63] confd/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/bgpconfigurations"
2023-03-09 06:11:38.623 [INFO][60] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.173.115
2023-03-09 06:11:38.623 [INFO][60] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.173.115/17, detected by connecting to 192.168.128.1
2023-03-09 06:11:38.654 [INFO][63] confd/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/ippools" error=too old resource version: 678 (904)
2023-03-09 06:11:38.655 [INFO][63] confd/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/ippools"
2023-03-09 06:11:38.655 [INFO][63] confd/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/bgppeers" error=too old resource version: 492 (909)
2023-03-09 06:11:38.655 [INFO][63] confd/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/bgppeers"
2023-03-09 06:11:39.255 [INFO][70] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/globalnetworkpolicies" error=too old resource version: 488 (910)
2023-03-09 06:11:39.255 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/globalnetworkpolicies"
2023-03-09 06:11:39.255 [INFO][70] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/networksets" error=too old resource version: 445 (910)
2023-03-09 06:11:39.255 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/networksets"
2023-03-09 06:11:39.255 [INFO][70] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/ippools" error=too old resource version: 678 (904)
2023-03-09 06:11:39.255 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/ippools"
2023-03-09 06:11:39.255 [INFO][70] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/globalnetworksets" error=too old resource version: 488 (910)
2023-03-09 06:11:39.255 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/globalnetworksets"
2023-03-09 06:11:39.256 [INFO][70] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/networkpolicies" error=too old resource version: 445 (910)
2023-03-09 06:11:39.256 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/networkpolicies"
2023-03-09 06:11:39.256 [INFO][70] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/bgpconfigurations" error=too old resource version: 492 (909)
2023-03-09 06:11:39.256 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/bgpconfigurations"
2023-03-09 06:11:39.257 [INFO][70] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/hostendpoints" error=too old resource version: 488 (910)
2023-03-09 06:11:39.257 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/hostendpoints"
2023-03-09 06:11:39.258 [INFO][70] felix/watchercache.go 125: Watch error received from Upstream ListRoot="/calico/resources/v3/projectcalico.org/felixconfigurations" error=too old resource version: 680 (910)
2023-03-09 06:11:39.258 [INFO][70] felix/watchercache.go 181: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/felixconfigurations"
2023-03-09 06:11:41.643 [INFO][70] felix/summary.go 100: Summarising 20 dataplane reconciliation loops over 1m2.8s: avg=21ms longest=252ms (resync-filter-v4,resync-ipsets-v4,resync-mangle-v4,resync-nat-v4,resync-raw-v4,resync-routes-v4,resync-routes-v4,resync-rules-v4,update-filter-v4,update-ipsets-4,update-mangle-v4,update-nat-v4,update-raw-v4)
2023-03-09 06:12:38.624 [INFO][60] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.173.115
2023-03-09 06:12:38.624 [INFO][60] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.173.115/17, detected by connecting to 192.168.128.1
2023-03-09 06:12:44.001 [INFO][70] felix/summary.go 100: Summarising 10 dataplane reconciliation loops over 1m2.4s: avg=4ms longest=11ms (resync-mangle-v4,resync-raw-v4)
2023-03-09 06:13:38.624 [INFO][60] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.173.115
2023-03-09 06:13:38.625 [INFO][60] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.173.115/17, detected by connecting to 192.168.128.1
2023-03-09 06:13:45.775 [INFO][70] felix/summary.go 100: Summarising 9 dataplane reconciliation loops over 1m1.8s: avg=5ms longest=18ms (resync-ipsets-v4)
2023-03-09 06:14:38.635 [INFO][60] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.173.115
2023-03-09 06:14:38.636 [INFO][60] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.173.115/17, detected by connecting to 192.168.128.1
2023-03-09 06:14:49.564 [INFO][70] felix/summary.go 100: Summarising 11 dataplane reconciliation loops over 1m3.8s: avg=6ms longest=35ms ()
2023-03-09 06:15:38.636 [INFO][60] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.173.115
2023-03-09 06:15:38.637 [INFO][60] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.173.115/17, detected by connecting to 192.168.128.1
2023-03-09 06:15:40.375 [INFO][70] felix/usagerep.go 115: Initial delay complete, doing first report
2023-03-09 06:15:40.375 [INFO][70] felix/usagerep.go 205: Reporting cluster usage/checking for deprecation warnings. alpEnabled=false calicoVersion="v3.24.5" clusterGUID="7197fc97702c482f9a5df8fb5c03809a" clusterType="k8s,bgp,kubeadm,kdd" gitRevision="f1a1611acb98d9187f48bbbe2227301aa69f0499" kubernetesVersion="v1.25.6" stats=calc.StatsUpdate{NumHosts:2, NumWorkloadEndpoints:4, NumHostEndpoints:0, NumPolicies:0, NumProfiles:50, NumALPPolicies:0} version="v3.24.5"
2023-03-09 06:15:41.305 [INFO][70] felix/usagerep.go 117: First report done, starting ticker
2023-03-09 06:15:53.237 [INFO][70] felix/summary.go 100: Summarising 10 dataplane reconciliation loops over 1m3.7s: avg=4ms longest=11ms (resync-filter-v4,resync-nat-v4)
2023-03-09 06:16:38.647 [INFO][60] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.173.115
2023-03-09 06:16:38.648 [INFO][60] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.173.115/17, detected by connecting to 192.168.128.1
2023-03-09 06:16:53.341 [INFO][70] felix/summary.go 100: Summarising 8 dataplane reconciliation loops over 1m0.1s: avg=5ms longest=15ms (resync-filter-v4,resync-nat-v4)
2023-03-09 06:17:38.648 [INFO][60] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.173.115
2023-03-09 06:17:38.649 [INFO][60] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.173.115/17, detected by connecting to 192.168.128.1
2023-03-09 06:17:55.387 [INFO][70] felix/summary.go 100: Summarising 8 dataplane reconciliation loops over 1m2s: avg=3ms longest=6ms ()
2023-03-09 06:18:38.660 [INFO][60] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.173.115
2023-03-09 06:18:38.660 [INFO][60] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.173.115/17, detected by connecting to 192.168.128.1
2023-03-09 06:19:02.808 [INFO][70] felix/summary.go 100: Summarising 12 dataplane reconciliation loops over 1m7.4s: avg=3ms longest=11ms (resync-nat-v4)
2023-03-09 06:19:38.661 [INFO][60] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.173.115
2023-03-09 06:19:38.661 [INFO][60] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.173.115/17, detected by connecting to 192.168.128.1
2023-03-09 06:20:06.467 [INFO][70] felix/summary.go 100: Summarising 9 dataplane reconciliation loops over 1m3.7s: avg=5ms longest=15ms (resync-filter-v4,resync-nat-v4)
2023-03-09 06:20:38.672 [INFO][60] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.173.115
2023-03-09 06:20:38.672 [INFO][60] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.173.115/17, detected by connecting to 192.168.128.1
2023-03-09 06:21:09.341 [INFO][70] felix/summary.go 100: Summarising 10 dataplane reconciliation loops over 1m2.9s: avg=4ms longest=13ms (resync-filter-v4,resync-nat-v4)
2023-03-09 06:21:38.673 [INFO][60] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.173.115
2023-03-09 06:21:38.674 [INFO][60] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.173.115/17, detected by connecting to 192.168.128.1
2023-03-09 06:22:11.388 [INFO][70] felix/summary.go 100: Summarising 8 dataplane reconciliation loops over 1m2s: avg=3ms longest=6ms (resync-ipsets-v4)
2023-03-09 06:22:38.684 [INFO][60] monitor-addresses/reachaddr.go 47: Auto-detected address by connecting to remote Destination="192.168.128.1" IP=192.168.173.115
2023-03-09 06:22:38.685 [INFO][60] monitor-addresses/autodetection_methods.go 143: Using autodetected IPv4 address 192.168.173.115/17, detected by connecting to 192.168.128.1
==== END logs for container calico-node of pod kube-system/calico-node-hzjbq ====
==== START logs for container coredns of pod kube-system/coredns-5c64b647bf-gkmxq ====
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
==== END logs for container coredns of pod kube-system/coredns-5c64b647bf-gkmxq ====
==== START logs for container coredns of pod kube-system/coredns-5c64b647bf-kxh68 ====
[INFO] plugin/ready: Still waiting on: "kubernetes"
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
==== END logs for container coredns of pod kube-system/coredns-5c64b647bf-kxh68 ====
==== START logs for container init of pod kube-system/csi-linode-controller-0 ====
linode://43589322
==== END logs for container init of pod kube-system/csi-linode-controller-0 ====
==== START logs for container csi-provisioner of pod kube-system/csi-linode-controller-0 ====
I0309 06:10:22.188216       1 feature_gate.go:245] feature gates: &{map[]}
I0309 06:10:22.188674       1 csi-provisioner.go:138] Version: v3.0.0
I0309 06:10:22.188747       1 csi-provisioner.go:161] Building kube configs for running in cluster...
I0309 06:10:30.363006       1 common.go:111] Probing CSI driver for readiness
I0309 06:10:30.369898       1 csi-provisioner.go:205] Detected CSI driver linodebs.csi.linode.com
I0309 06:10:30.374459       1 csi-provisioner.go:274] CSI driver supports PUBLISH_UNPUBLISH_VOLUME, watching VolumeAttachments
I0309 06:10:30.375684       1 controller.go:731] Using saving PVs to API server in background
I0309 06:10:30.476063       1 controller.go:810] Starting provisioner controller linodebs.csi.linode.com_csi-linode-controller-0_4065bf75-d560-4e06-9c4b-49b6da83cddc!
I0309 06:10:30.476266       1 volume_store.go:97] Starting save volume queue
I0309 06:10:30.576724       1 controller.go:859] Started provisioner controller linodebs.csi.linode.com_csi-linode-controller-0_4065bf75-d560-4e06-9c4b-49b6da83cddc!
==== END logs for container csi-provisioner of pod kube-system/csi-linode-controller-0 ====
==== START logs for container csi-attacher of pod kube-system/csi-linode-controller-0 ====
I0309 06:10:24.626017       1 main.go:99] Version: v3.3.0
I0309 06:10:30.310420       1 common.go:111] Probing CSI driver for readiness
I0309 06:10:30.313877       1 main.go:155] CSI driver name: "linodebs.csi.linode.com"
I0309 06:10:30.315408       1 main.go:230] CSI driver supports ControllerPublishUnpublish, using real CSI handler
I0309 06:10:30.315762       1 controller.go:128] Starting CSI attacher
==== END logs for container csi-attacher of pod kube-system/csi-linode-controller-0 ====
==== START logs for container csi-resizer of pod kube-system/csi-linode-controller-0 ====
I0309 06:10:27.165020       1 main.go:93] Version : v1.3.0
I0309 06:10:27.165260       1 feature_gate.go:245] feature gates: &{map[]}
I0309 06:10:30.328247       1 common.go:111] Probing CSI driver for readiness
I0309 06:10:30.330194       1 main.go:141] CSI driver name: "linodebs.csi.linode.com"
I0309 06:10:30.331459       1 controller.go:120] Register Pod informer for resizer linodebs.csi.linode.com
I0309 06:10:30.331645       1 controller.go:251] Starting external resizer linodebs.csi.linode.com
==== END logs for container csi-resizer of pod kube-system/csi-linode-controller-0 ====
==== START logs for container linode-csi-plugin of pod kube-system/csi-linode-controller-0 ====
==== END logs for container linode-csi-plugin of pod kube-system/csi-linode-controller-0 ====
==== START logs for container init of pod kube-system/csi-linode-node-bwnv6 ====
linode://43589322
==== END logs for container init of pod kube-system/csi-linode-node-bwnv6 ====
==== START logs for container csi-node-driver-registrar of pod kube-system/csi-linode-node-bwnv6 ====
I0309 06:10:31.358608       1 main.go:110] Version: v1.3.0-0-g6e9fff3e
I0309 06:10:31.359424       1 main.go:120] Attempting to open a gRPC connection with: "/csi/csi.sock"
I0309 06:10:31.359444       1 connection.go:151] Connecting to unix:///csi/csi.sock
I0309 06:10:32.360058       1 main.go:127] Calling CSI driver to discover driver name
I0309 06:10:32.361699       1 main.go:137] CSI driver name: "linodebs.csi.linode.com"
I0309 06:10:32.362678       1 node_register.go:51] Starting Registration Server at: /registration/linodebs.csi.linode.com-reg.sock
I0309 06:10:32.363068       1 node_register.go:60] Registration Server started at: /registration/linodebs.csi.linode.com-reg.sock
I0309 06:10:32.905939       1 main.go:77] Received GetInfo call: &InfoRequest{}
I0309 06:10:32.944535       1 main.go:87] Received NotifyRegistrationStatus call: &RegistrationStatus{PluginRegistered:true,Error:,}
==== END logs for container csi-node-driver-registrar of pod kube-system/csi-linode-node-bwnv6 ====
==== START logs for container csi-linode-plugin of pod kube-system/csi-linode-node-bwnv6 ====
==== END logs for container csi-linode-plugin of pod kube-system/csi-linode-node-bwnv6 ====
==== START logs for container init of pod kube-system/csi-linode-node-pttdn ====
linode://43589324
==== END logs for container init of pod kube-system/csi-linode-node-pttdn ====
==== START logs for container csi-node-driver-registrar of pod kube-system/csi-linode-node-pttdn ====
I0309 06:10:32.377664       1 main.go:110] Version: v1.3.0-0-g6e9fff3e
I0309 06:10:32.379830       1 main.go:120] Attempting to open a gRPC connection with: "/csi/csi.sock"
I0309 06:10:32.379937       1 connection.go:151] Connecting to unix:///csi/csi.sock
I0309 06:10:40.564720       1 main.go:127] Calling CSI driver to discover driver name
I0309 06:10:40.566920       1 main.go:137] CSI driver name: "linodebs.csi.linode.com"
I0309 06:10:40.567732       1 node_register.go:51] Starting Registration Server at: /registration/linodebs.csi.linode.com-reg.sock
I0309 06:10:40.568054       1 node_register.go:60] Registration Server started at: /registration/linodebs.csi.linode.com-reg.sock
I0309 06:10:40.725028       1 main.go:77] Received GetInfo call: &InfoRequest{}
I0309 06:10:40.759754       1 main.go:87] Received NotifyRegistrationStatus call: &RegistrationStatus{PluginRegistered:true,Error:,}
==== END logs for container csi-node-driver-registrar of pod kube-system/csi-linode-node-pttdn ====
==== START logs for container csi-linode-plugin of pod kube-system/csi-linode-node-pttdn ====
==== END logs for container csi-linode-plugin of pod kube-system/csi-linode-node-pttdn ====
==== START logs for container kube-proxy of pod kube-system/kube-proxy-dz8bx ====
I0309 06:10:22.702419       1 node.go:163] Successfully retrieved node IP: 192.168.173.115
I0309 06:10:22.702635       1 server_others.go:138] "Detected node IP" address="192.168.173.115"
I0309 06:10:22.702700       1 server_others.go:589] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I0309 06:10:22.713702       1 server_others.go:206] "Using iptables Proxier"
I0309 06:10:22.713834       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0309 06:10:22.713890       1 server_others.go:214] "Creating dualStackProxier for iptables"
I0309 06:10:22.713943       1 server_others.go:512] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0309 06:10:22.713994       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0309 06:10:22.714146       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0309 06:10:22.714352       1 server.go:661] "Version info" version="v1.25.6"
I0309 06:10:22.714403       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0309 06:10:22.715602       1 conntrack.go:100] "Set sysctl" entry="net/netfilter/nf_conntrack_max" value=131072
I0309 06:10:22.715681       1 conntrack.go:52] "Setting nf_conntrack_max" nf_conntrack_max=131072
I0309 06:10:22.716188       1 conntrack.go:83] "Setting conntrack hashsize" conntrack hashsize=32768
I0309 06:10:22.716437       1 conntrack.go:100] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_close_wait" value=3600
I0309 06:10:22.719562       1 config.go:317] "Starting service config controller"
I0309 06:10:22.719643       1 shared_informer.go:255] Waiting for caches to sync for service config
I0309 06:10:22.719698       1 config.go:226] "Starting endpoint slice config controller"
I0309 06:10:22.719742       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I0309 06:10:22.723164       1 config.go:444] "Starting node config controller"
I0309 06:10:22.723174       1 shared_informer.go:255] Waiting for caches to sync for node config
I0309 06:10:22.820367       1 shared_informer.go:262] Caches are synced for endpoint slice config
I0309 06:10:22.820447       1 shared_informer.go:262] Caches are synced for service config
I0309 06:10:22.826198       1 shared_informer.go:262] Caches are synced for node config
==== END logs for container kube-proxy of pod kube-system/kube-proxy-dz8bx ====
==== START logs for container kube-proxy of pod kube-system/kube-proxy-n4rms ====
I0309 06:10:00.406401       1 node.go:163] Successfully retrieved node IP: 192.168.129.114
I0309 06:10:00.406665       1 server_others.go:138] "Detected node IP" address="192.168.129.114"
I0309 06:10:00.406783       1 server_others.go:589] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I0309 06:10:00.423922       1 server_others.go:206] "Using iptables Proxier"
I0309 06:10:00.424189       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0309 06:10:00.424250       1 server_others.go:214] "Creating dualStackProxier for iptables"
I0309 06:10:00.424316       1 server_others.go:512] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0309 06:10:00.424428       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0309 06:10:00.424633       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0309 06:10:00.424953       1 server.go:661] "Version info" version="v1.25.6"
I0309 06:10:00.425023       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0309 06:10:00.426128       1 conntrack.go:100] "Set sysctl" entry="net/netfilter/nf_conntrack_max" value=131072
I0309 06:10:00.426247       1 conntrack.go:52] "Setting nf_conntrack_max" nf_conntrack_max=131072
I0309 06:10:00.426670       1 conntrack.go:83] "Setting conntrack hashsize" conntrack hashsize=32768
I0309 06:10:00.428684       1 conntrack.go:100] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_close_wait" value=3600
I0309 06:10:00.430034       1 config.go:317] "Starting service config controller"
I0309 06:10:00.430159       1 shared_informer.go:255] Waiting for caches to sync for service config
I0309 06:10:00.430252       1 config.go:226] "Starting endpoint slice config controller"
I0309 06:10:00.430295       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I0309 06:10:00.430986       1 config.go:444] "Starting node config controller"
I0309 06:10:00.431057       1 shared_informer.go:255] Waiting for caches to sync for node config
I0309 06:10:00.531233       1 shared_informer.go:262] Caches are synced for node config
I0309 06:10:00.531426       1 shared_informer.go:262] Caches are synced for service config
I0309 06:10:00.531500       1 shared_informer.go:262] Caches are synced for endpoint slice config
==== END logs for container kube-proxy of pod kube-system/kube-proxy-n4rms ====
---
apiVersion: v1
items:
- count: 8
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:27Z"
  involvedObject:
    kind: Node
    name: lke96996-146211-640977a3930b
    uid: lke96996-146211-640977a3930b
  lastTimestamp: "2023-03-09T06:09:39Z"
  message: 'Node lke96996-146211-640977a3930b status is now: NodeHasSufficientMemory'
  metadata:
    creationTimestamp: "2023-03-09T06:09:40Z"
    name: lke96996-146211-640977a3930b.174aab4f0a6ae908
    namespace: default
    resourceVersion: "529"
    uid: d79eb47c-7038-4eb7-8ec3-c105743f4742
  reason: NodeHasSufficientMemory
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:42Z"
  involvedObject:
    apiVersion: v1
    kind: Node
    name: lke96996-146211-640977a3930b
    resourceVersion: "530"
    uid: 3acd7392-09b7-4c8a-aa2b-738940017be5
  lastTimestamp: "2023-03-09T06:09:42Z"
  message: Node synced successfully
  metadata:
    creationTimestamp: "2023-03-09T06:09:42Z"
    name: lke96996-146211-640977a3930b.174aab52a44acbb3
    namespace: default
    resourceVersion: "562"
    uid: 792a13ac-cb13-4cd2-8353-e952b692c308
  reason: Synced
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: cloud-node-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:42Z"
  involvedObject:
    kind: Node
    name: lke96996-146211-640977a3930b
    uid: lke96996-146211-640977a3930b
  lastTimestamp: "2023-03-09T06:09:42Z"
  message: Starting kubelet.
  metadata:
    creationTimestamp: "2023-03-09T06:09:43Z"
    name: lke96996-146211-640977a3930b.174aab52ad02818f
    namespace: default
    resourceVersion: "565"
    uid: c45003b4-3e28-4bad-bdf2-a50d4fdf9d3e
  reason: Starting
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:42Z"
  involvedObject:
    kind: Node
    name: lke96996-146211-640977a3930b
    uid: lke96996-146211-640977a3930b
  lastTimestamp: "2023-03-09T06:09:42Z"
  message: invalid capacity 0 on image filesystem
  metadata:
    creationTimestamp: "2023-03-09T06:09:43Z"
    name: lke96996-146211-640977a3930b.174aab52b05170a4
    namespace: default
    resourceVersion: "566"
    uid: 73f6c80b-0ab4-48b9-89f4-9c955f70ea9b
  reason: InvalidDiskCapacity
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Warning
- count: 2
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:42Z"
  involvedObject:
    kind: Node
    name: lke96996-146211-640977a3930b
    uid: lke96996-146211-640977a3930b
  lastTimestamp: "2023-03-09T06:09:42Z"
  message: 'Node lke96996-146211-640977a3930b status is now: NodeHasSufficientMemory'
  metadata:
    creationTimestamp: "2023-03-09T06:09:43Z"
    name: lke96996-146211-640977a3930b.174aab52b40e49b6
    namespace: default
    resourceVersion: "571"
    uid: 0fc87f74-7086-4bae-acfe-9f1a4868d4c7
  reason: NodeHasSufficientMemory
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 2
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:42Z"
  involvedObject:
    kind: Node
    name: lke96996-146211-640977a3930b
    uid: lke96996-146211-640977a3930b
  lastTimestamp: "2023-03-09T06:09:42Z"
  message: 'Node lke96996-146211-640977a3930b status is now: NodeHasNoDiskPressure'
  metadata:
    creationTimestamp: "2023-03-09T06:09:43Z"
    name: lke96996-146211-640977a3930b.174aab52b40e5e88
    namespace: default
    resourceVersion: "572"
    uid: 237b1911-a7c8-4fb4-bcbb-c288aaffca57
  reason: NodeHasNoDiskPressure
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 2
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:42Z"
  involvedObject:
    kind: Node
    name: lke96996-146211-640977a3930b
    uid: lke96996-146211-640977a3930b
  lastTimestamp: "2023-03-09T06:09:42Z"
  message: 'Node lke96996-146211-640977a3930b status is now: NodeHasSufficientPID'
  metadata:
    creationTimestamp: "2023-03-09T06:09:43Z"
    name: lke96996-146211-640977a3930b.174aab52b40e7044
    namespace: default
    resourceVersion: "574"
    uid: 97d041e5-495b-4f6e-b6fe-6ce94f200eff
  reason: NodeHasSufficientPID
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:42Z"
  involvedObject:
    kind: Node
    name: lke96996-146211-640977a3930b
    uid: lke96996-146211-640977a3930b
  lastTimestamp: "2023-03-09T06:09:42Z"
  message: Updated Node Allocatable limit across pods
  metadata:
    creationTimestamp: "2023-03-09T06:09:43Z"
    name: lke96996-146211-640977a3930b.174aab52b469b8b3
    namespace: default
    resourceVersion: "575"
    uid: d765d7c5-c40c-4d5c-a2db-d0e9db774fa8
  reason: NodeAllocatableEnforced
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:44Z"
  involvedObject:
    apiVersion: v1
    kind: Node
    name: lke96996-146211-640977a3930b
    uid: 3acd7392-09b7-4c8a-aa2b-738940017be5
  lastTimestamp: "2023-03-09T06:09:44Z"
  message: 'Node lke96996-146211-640977a3930b event: Registered Node lke96996-146211-640977a3930b
    in Controller'
  metadata:
    creationTimestamp: "2023-03-09T06:09:44Z"
    name: lke96996-146211-640977a3930b.174aab53059a2714
    namespace: default
    resourceVersion: "576"
    uid: 5928e953-c370-4348-8137-1a23e1d63e1f
  reason: RegisteredNode
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: node-controller
  type: Normal
- action: StartKubeProxy
  eventTime: "2023-03-09T06:10:00.428970Z"
  firstTimestamp: null
  involvedObject:
    kind: Node
    name: lke96996-146211-640977a3930b
    uid: lke96996-146211-640977a3930b
  lastTimestamp: null
  metadata:
    creationTimestamp: "2023-03-09T06:10:00Z"
    name: lke96996-146211-640977a3930b.174aab56cc1dd102
    namespace: default
    resourceVersion: "658"
    uid: a500b739-dd3e-42b6-9431-a1489c90f90a
  reason: Starting
  reportingComponent: kube-proxy
  reportingInstance: kube-proxy-lke96996-146211-640977a3930b
  source: {}
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:03Z"
  involvedObject:
    kind: Node
    name: lke96996-146211-640977a3930b
    uid: lke96996-146211-640977a3930b
  lastTimestamp: "2023-03-09T06:10:03Z"
  message: 'Node lke96996-146211-640977a3930b status is now: NodeReady'
  metadata:
    creationTimestamp: "2023-03-09T06:10:03Z"
    name: lke96996-146211-640977a3930b.174aab5768d56362
    namespace: default
    resourceVersion: "669"
    uid: f274b070-70f4-47f6-ab77-16bd0db013b4
  reason: NodeReady
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3930b
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:11:51Z"
  involvedObject:
    apiVersion: v1
    kind: Node
    name: lke96996-146211-640977a3930b
    uid: 3acd7392-09b7-4c8a-aa2b-738940017be5
  lastTimestamp: "2023-03-09T06:11:51Z"
  message: 'Node lke96996-146211-640977a3930b event: Registered Node lke96996-146211-640977a3930b
    in Controller'
  metadata:
    creationTimestamp: "2023-03-09T06:11:51Z"
    name: lke96996-146211-640977a3930b.174aab7093976851
    namespace: default
    resourceVersion: "918"
    uid: e3257494-7ab9-4ac4-aa52-c60d5783723c
  reason: RegisteredNode
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: node-controller
  type: Normal
- count: 8
  eventTime: null
  firstTimestamp: "2023-03-09T06:09:56Z"
  involvedObject:
    kind: Node
    name: lke96996-146211-640977a3e82e
    uid: lke96996-146211-640977a3e82e
  lastTimestamp: "2023-03-09T06:10:09Z"
  message: 'Node lke96996-146211-640977a3e82e status is now: NodeHasSufficientMemory'
  metadata:
    creationTimestamp: "2023-03-09T06:10:09Z"
    name: lke96996-146211-640977a3e82e.174aab55dad46321
    namespace: default
    resourceVersion: "684"
    uid: 9e4ed6c8-f682-45c4-a33c-fdecd1348ba3
  reason: NodeHasSufficientMemory
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:10Z"
  involvedObject:
    apiVersion: v1
    kind: Node
    name: lke96996-146211-640977a3e82e
    resourceVersion: "685"
    uid: c97e244d-76b3-4227-985e-e625f0e8fdcf
  lastTimestamp: "2023-03-09T06:10:10Z"
  message: Node synced successfully
  metadata:
    creationTimestamp: "2023-03-09T06:10:10Z"
    name: lke96996-146211-640977a3e82e.174aab592c0a1017
    namespace: default
    resourceVersion: "710"
    uid: 11b8b52a-7899-4327-aae5-7a879bf1385f
  reason: Synced
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: cloud-node-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:11Z"
  involvedObject:
    kind: Node
    name: lke96996-146211-640977a3e82e
    uid: lke96996-146211-640977a3e82e
  lastTimestamp: "2023-03-09T06:10:11Z"
  message: Starting kubelet.
  metadata:
    creationTimestamp: "2023-03-09T06:10:12Z"
    name: lke96996-146211-640977a3e82e.174aab596466ed7e
    namespace: default
    resourceVersion: "713"
    uid: 243a9cb9-0c79-4e58-9179-0990a8ff45e1
  reason: Starting
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:11Z"
  involvedObject:
    kind: Node
    name: lke96996-146211-640977a3e82e
    uid: lke96996-146211-640977a3e82e
  lastTimestamp: "2023-03-09T06:10:11Z"
  message: invalid capacity 0 on image filesystem
  metadata:
    creationTimestamp: "2023-03-09T06:10:12Z"
    name: lke96996-146211-640977a3e82e.174aab596642bc23
    namespace: default
    resourceVersion: "714"
    uid: 065e7cbc-eff7-4c38-82fc-4616363bcfce
  reason: InvalidDiskCapacity
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Warning
- count: 2
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:11Z"
  involvedObject:
    kind: Node
    name: lke96996-146211-640977a3e82e
    uid: lke96996-146211-640977a3e82e
  lastTimestamp: "2023-03-09T06:10:11Z"
  message: 'Node lke96996-146211-640977a3e82e status is now: NodeHasSufficientMemory'
  metadata:
    creationTimestamp: "2023-03-09T06:10:12Z"
    name: lke96996-146211-640977a3e82e.174aab596a489002
    namespace: default
    resourceVersion: "720"
    uid: 11f7f2ff-a81e-4066-b192-5019e04e7e2a
  reason: NodeHasSufficientMemory
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 2
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:11Z"
  involvedObject:
    kind: Node
    name: lke96996-146211-640977a3e82e
    uid: lke96996-146211-640977a3e82e
  lastTimestamp: "2023-03-09T06:10:11Z"
  message: 'Node lke96996-146211-640977a3e82e status is now: NodeHasNoDiskPressure'
  metadata:
    creationTimestamp: "2023-03-09T06:10:12Z"
    name: lke96996-146211-640977a3e82e.174aab596a48a506
    namespace: default
    resourceVersion: "722"
    uid: 6fad1964-f865-4bb5-9c34-86a5a7929b0e
  reason: NodeHasNoDiskPressure
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 2
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:11Z"
  involvedObject:
    kind: Node
    name: lke96996-146211-640977a3e82e
    uid: lke96996-146211-640977a3e82e
  lastTimestamp: "2023-03-09T06:10:11Z"
  message: 'Node lke96996-146211-640977a3e82e status is now: NodeHasSufficientPID'
  metadata:
    creationTimestamp: "2023-03-09T06:10:12Z"
    name: lke96996-146211-640977a3e82e.174aab596a48aec0
    namespace: default
    resourceVersion: "723"
    uid: 431befa6-8a21-41e5-88fa-13b3f97ee90e
  reason: NodeHasSufficientPID
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:11Z"
  involvedObject:
    kind: Node
    name: lke96996-146211-640977a3e82e
    uid: lke96996-146211-640977a3e82e
  lastTimestamp: "2023-03-09T06:10:11Z"
  message: Updated Node Allocatable limit across pods
  metadata:
    creationTimestamp: "2023-03-09T06:10:12Z"
    name: lke96996-146211-640977a3e82e.174aab596a84466b
    namespace: default
    resourceVersion: "719"
    uid: df97ac33-b27d-4983-af6f-8eeac327dc34
  reason: NodeAllocatableEnforced
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:14Z"
  involvedObject:
    apiVersion: v1
    kind: Node
    name: lke96996-146211-640977a3e82e
    uid: c97e244d-76b3-4227-985e-e625f0e8fdcf
  lastTimestamp: "2023-03-09T06:10:14Z"
  message: 'Node lke96996-146211-640977a3e82e event: Registered Node lke96996-146211-640977a3e82e
    in Controller'
  metadata:
    creationTimestamp: "2023-03-09T06:10:14Z"
    name: lke96996-146211-640977a3e82e.174aab5a020862d7
    namespace: default
    resourceVersion: "734"
    uid: dcb2fe3a-5428-4ddc-bea8-9daebeb604d6
  reason: RegisteredNode
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: node-controller
  type: Normal
- action: StartKubeProxy
  eventTime: "2023-03-09T06:10:22.716664Z"
  firstTimestamp: null
  involvedObject:
    kind: Node
    name: lke96996-146211-640977a3e82e
    uid: lke96996-146211-640977a3e82e
  lastTimestamp: null
  metadata:
    creationTimestamp: "2023-03-09T06:10:22Z"
    name: lke96996-146211-640977a3e82e.174aab5bfc90fd17
    namespace: default
    resourceVersion: "806"
    uid: 09ecc3db-57e9-4819-9e89-a11152ea9d4f
  reason: Starting
  reportingComponent: kube-proxy
  reportingInstance: kube-proxy-lke96996-146211-640977a3e82e
  source: {}
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:10:32Z"
  involvedObject:
    kind: Node
    name: lke96996-146211-640977a3e82e
    uid: lke96996-146211-640977a3e82e
  lastTimestamp: "2023-03-09T06:10:32Z"
  message: 'Node lke96996-146211-640977a3e82e status is now: NodeReady'
  metadata:
    creationTimestamp: "2023-03-09T06:10:32Z"
    name: lke96996-146211-640977a3e82e.174aab5e2b13f554
    namespace: default
    resourceVersion: "846"
    uid: 256a67de-a91c-4a33-af39-af2c3fed9c62
  reason: NodeReady
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: kubelet
    host: lke96996-146211-640977a3e82e
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2023-03-09T06:11:51Z"
  involvedObject:
    apiVersion: v1
    kind: Node
    name: lke96996-146211-640977a3e82e
    uid: c97e244d-76b3-4227-985e-e625f0e8fdcf
  lastTimestamp: "2023-03-09T06:11:51Z"
  message: 'Node lke96996-146211-640977a3e82e event: Registered Node lke96996-146211-640977a3e82e
    in Controller'
  metadata:
    creationTimestamp: "2023-03-09T06:11:51Z"
    name: lke96996-146211-640977a3e82e.174aab7093b6ce60
    namespace: default
    resourceVersion: "919"
    uid: e7daf5a9-8851-40c0-af13-fd6d8c09e804
  reason: RegisteredNode
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: node-controller
  type: Normal
kind: EventList
metadata:
  resourceVersion: "1308"
---
apiVersion: v1
items: []
kind: ReplicationControllerList
metadata:
  resourceVersion: "1308"
---
apiVersion: v1
items:
- metadata:
    creationTimestamp: "2023-03-09T06:08:17Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "190"
    uid: 9463e43b-ada7-4acb-a5c3-32e0f474ca70
  spec:
    clusterIP: 10.128.0.1
    clusterIPs:
    - 10.128.0.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
kind: ServiceList
metadata:
  resourceVersion: "1308"
---
apiVersion: apps/v1
items: []
kind: DaemonSetList
metadata:
  resourceVersion: "1308"
---
apiVersion: apps/v1
items: []
kind: DeploymentList
metadata:
  resourceVersion: "1308"
---
apiVersion: apps/v1
items: []
kind: ReplicaSetList
metadata:
  resourceVersion: "1308"
---
apiVersion: v1
items: []
kind: PodList
metadata:
  resourceVersion: "1308"
